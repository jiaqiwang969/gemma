"""
Gemini API å…¼å®¹æœåŠ¡å™¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å°†æœ¬åœ° Gemma 3n æ¨¡å‹æš´éœ²ä¸º Google Gemini 3 Pro Preview API æ ¼å¼ã€‚

æ ¸å¿ƒç‰¹æ€§:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ thoughtSignature - AI çš„"å·¥ä½œè®°å¿†"ï¼Œå®ç°çœŸæ­£çš„æœ‰çŠ¶æ€æ¨ç†
â€¢ KV Cache æŒä¹…åŒ– - é€šè¿‡ llama.cpp slot-save ä¿å­˜æ¨¡å‹å†…éƒ¨çŠ¶æ€
â€¢ å¤šè½®å¯¹è¯è¿ç»­æ€§ - è·¨è¯·æ±‚ä¿æŒé€»è¾‘é“¾æ¡

æ”¯æŒçš„ API:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ generateContent - æ–‡æœ¬ç”Ÿæˆ (æ ¸å¿ƒ)
â€¢ thinkingConfig - æ€è€ƒç­‰çº§ (minimal/low/medium/high)
â€¢ systemInstruction - ç³»ç»ŸæŒ‡ä»¤/äººè®¾
â€¢ responseMimeType + responseSchema - JSON ç»“æ„åŒ–è¾“å‡º
â€¢ safetySettings - å®‰å…¨è®¾ç½®
â€¢ functionDeclarations + functionCall - å·¥å…·è°ƒç”¨
â€¢ thoughtSignature - æ€ç»´ç­¾å (çŠ¶æ€æŒä¹…åŒ–)

è®¾è®¡å“²å­¦:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä¼ ç»Ÿ LLM æ˜¯"æ— çŠ¶æ€"çš„ï¼Œåƒæ‚£æœ‰çŸ­æœŸè®°å¿†ä¸§å¤±ç—‡çš„å¤©æ‰ã€‚
Thought Signatures è®© AI å…·å¤‡çœŸæ­£çš„"å·¥ä½œè®°å¿†"ï¼š
  â€¢ æ•è· - æ¨ç†ç»“æŸæ—¶æå– KV Cache
  â€¢ ä¼ é€’ - Base64 ç­¾ååœ¨ç½‘ç»œé—´ä¼ è¾“
  â€¢ æ¢å¤ - ä¸‹ä¸€è½®å¯¹è¯ä¸­é‡æ–°åŠ è½½

è¯¦è§: apps/gemini_api/THOUGHT_SIGNATURE.md

å‚è€ƒ:
â”€â”€â”€â”€â”€
â€¢ LMCache: https://github.com/LMCache/LMCache
â€¢ llama.cpp slots: https://github.com/ggml-org/llama.cpp/discussions/13606
â€¢ Gemini API: https://ai.google.dev/gemini-api/docs/thought-signatures

ä¸æ”¯æŒ:
â”€â”€â”€â”€â”€â”€â”€
â€¢ googleSearch (éœ€è¦å¤–éƒ¨ API)
â€¢ å›¾åƒç”Ÿæˆ (gemini-3-pro-image-preview)
"""

import os
import sys
import json
import time
import uuid
import base64
import hashlib
import subprocess
import re
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Any

from flask import Flask, request, jsonify, Response
from flask_cors import CORS

# è·¯å¾„é…ç½®
REPO_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(REPO_ROOT))

app = Flask(__name__)
CORS(app)

# ========== å…¨å±€é…ç½® ==========
MODEL_VERSION = "gemma-3n-local"
API_VERSION = "v1beta"
DEFAULT_MAX_TOKENS = 2048
DEFAULT_TEMPERATURE = 1.0

# llama.cpp åç«¯é…ç½®
LLAMA_SERVER_BIN = os.environ.get("LLAMA_SERVER_BIN", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-server"))
LLAMA_MTMD_BIN = os.environ.get("LLAMA_MTMD_BIN", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-mtmd-cli"))
LLAMA_SERVER_PORT = int(os.environ.get("GEMINI_API_LLAMA_PORT", "8090"))

# è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹
LLAMA_MODEL = os.environ.get("LLAMA_MODEL", "")
if not LLAMA_MODEL:
    for candidate in [
        REPO_ROOT / "artifacts/gguf/gemma-3n-finetuned-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-fp16.gguf",
    ]:
        if candidate.exists():
            LLAMA_MODEL = str(candidate)
            break

# éŸ³é¢‘æ¨ç†ä¸“ç”¨æ¨¡å‹ (éŸ³é¢‘ mmproj ä»…ä¸åŸå§‹æ¨¡å‹å…¼å®¹ï¼Œä¸å…¼å®¹å¾®è°ƒç‰ˆ)
LLAMA_MODEL_AUDIO = os.environ.get("LLAMA_MODEL_AUDIO", "")
if not LLAMA_MODEL_AUDIO:
    # ä¼˜å…ˆä½¿ç”¨åŸå§‹æ¨¡å‹ (gemma-3n-E2B-it)ï¼Œå› ä¸ºå¾®è°ƒæ¨¡å‹å¯èƒ½ä¸éŸ³é¢‘ mmproj ä¸å…¼å®¹
    for candidate in [
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-fp16.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-finetuned-Q4_K_M.gguf",  # å¤‡é€‰
    ]:
        if candidate.exists():
            LLAMA_MODEL_AUDIO = str(candidate)
            break

# å¤šæ¨¡æ€æ”¯æŒ: Vision Projector (mmproj)
LLAMA_MMPROJ_VISION = os.environ.get("LLAMA_MMPROJ_VISION", "")
if not LLAMA_MMPROJ_VISION:
    for candidate in [
        REPO_ROOT / "artifacts/gguf/gemma-3n-vision-mmproj-f16.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-vision-mmproj-Q8_0.gguf",
    ]:
        if candidate.exists():
            LLAMA_MMPROJ_VISION = str(candidate)
            break

# å¤šæ¨¡æ€æ”¯æŒ: Audio Projector (mmproj)
LLAMA_MMPROJ_AUDIO = os.environ.get("LLAMA_MMPROJ_AUDIO", "")
if not LLAMA_MMPROJ_AUDIO:
    for candidate in [
        REPO_ROOT / "artifacts/gguf/gemma-3n-audio-mmproj-f16.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-audio-mmproj-Q8_0.gguf",
    ]:
        if candidate.exists():
            LLAMA_MMPROJ_AUDIO = str(candidate)
            break

# åˆå¹¶ vision+audio mmproj (llama.cpp æ”¯æŒé€—å·åˆ†éš”)
def get_combined_mmproj():
    """è·å–åˆå¹¶çš„ mmproj è·¯å¾„ (vision,audio æ ¼å¼)"""
    projectors = []
    if LLAMA_MMPROJ_VISION and Path(LLAMA_MMPROJ_VISION).exists():
        projectors.append(LLAMA_MMPROJ_VISION)
    if LLAMA_MMPROJ_AUDIO and Path(LLAMA_MMPROJ_AUDIO).exists():
        projectors.append(LLAMA_MMPROJ_AUDIO)
    return ",".join(projectors) if projectors else ""

LLAMA_MMPROJ = os.environ.get("LLAMA_MMPROJ", "") or get_combined_mmproj()

# å¤šæ¨¡æ€èƒ½åŠ›æ ‡è®°
VISION_ENABLED = bool(LLAMA_MMPROJ_VISION) and Path(LLAMA_MMPROJ_VISION).exists() if LLAMA_MMPROJ_VISION else False
AUDIO_ENABLED = bool(LLAMA_MMPROJ_AUDIO) and Path(LLAMA_MMPROJ_AUDIO).exists() if LLAMA_MMPROJ_AUDIO else False
MULTIMODAL_ENABLED = VISION_ENABLED or AUDIO_ENABLED

# llama-server è¿›ç¨‹ç®¡ç†
llama_server_process = None
llama_server_ready = False

# ========== Thought Signature ç³»ç»Ÿ ==========
#
# è®¾è®¡å“²å­¦: AI ä»"æ¦‚ç‡é¢„æµ‹"å‘"é€»è¾‘å®ä½“"çš„è¿›åŒ–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# ä¼ ç»Ÿ LLM åƒä¸€ä¸ªæ‚£æœ‰"çŸ­æœŸè®°å¿†ä¸§å¤±ç—‡"çš„å¤©æ‰ï¼š
#   æ¯æ¬¡å¯¹è¯éƒ½è¦é‡æ–°é˜…è¯»"ç—…å†æœ¬"ï¼ˆå¯¹è¯å†å²ï¼‰æ¥æ‰¾å›çŠ¶æ€ã€‚
#
# Thought Signatures çš„æœ¬è´¨æ˜¯è®© AI å…·å¤‡çœŸæ­£çš„"å·¥ä½œè®°å¿†" (Working Memory)ï¼š
#
#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#   â”‚  ä¼ ç»Ÿå¯¹è¯å†å² (Text History)     æ€ç»´ç­¾å (Thought Signature)        â”‚
#   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
#   â”‚  ğŸ“„ çº¸ä¸Šçš„ä¼šè®®çºªè¦                ğŸ§  å¤§è„‘ä¸­çš„ç¬æ—¶ç¥ç»ç”µä¿¡å·            â”‚
#   â”‚     â†“                               â†“                               â”‚
#   â”‚  é‡æ–°é˜…è¯»ã€é‡æ–°ç†è§£              çŠ¶æ€åŠ è½½ã€ç¬é—´å”¤é†’                   â”‚
#   â”‚     â†“                               â†“                               â”‚
#   â”‚  å¯èƒ½äº§ç”Ÿæ–°çš„å¹»è§‰                é€»è¾‘é“¾æ¡é”å®š                        â”‚
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# æ€ç»´ç­¾åè¯æ˜äº† AI çš„æ€è€ƒä¸å†æ˜¯éšæœºçš„æ¦‚ç‡ç¢°æ’ï¼Œè€Œæ˜¯ä¸€ä¸ªå¯ä»¥è¢«ï¼š
#   â€¢ æ•è· (Captured)   - åœ¨æ¨ç†ç»“æŸæ—¶æå– KV Cache
#   â€¢ ä¼ é€’ (Transferred) - ä½œä¸º Base64 ç­¾ååœ¨ç½‘ç»œé—´ä¼ è¾“
#   â€¢ æ¢å¤ (Restored)   - åœ¨ä¸‹ä¸€è½®å¯¹è¯ä¸­é‡æ–°åŠ è½½
#
# æœªæ¥å±•æœ›ï¼š
#   1. åˆ†å¸ƒå¼ Agent åä½œçš„"æ¥åŠ›æ£’" - æ€ç»´å±‚é¢çš„åŒæ­¥
#   2. è·¨å¹³å°çš„"é€»è¾‘æ¼«æ¸¸" - æ‰‹æœºåˆ°ç”µè„‘çš„æ— ç¼æ¥ç®¡
#   3. è°ƒè¯•ä¸å®¡è®¡çš„æ–°ç»´åº¦ - å›æº¯ç²¾ç¡®é€»è¾‘çŠ¶æ€
#
# å‚è€ƒ: apps/gemini_api/THOUGHT_SIGNATURE.md
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THOUGHT_SIGNATURE_SECRET = os.urandom(32).hex()  # æ¯æ¬¡å¯åŠ¨éšæœºç”Ÿæˆå¯†é’¥
SIGNATURE_TTL = 3600  # ç­¾åæœ‰æ•ˆæœŸ 1 å°æ—¶

# æ¨ç†çŠ¶æ€ç¼“å­˜: signature_id -> ThoughtState
# è¿™æ˜¯ AI çš„"å·¥ä½œè®°å¿†"å­˜å‚¨
thought_state_cache: Dict[str, Dict] = {}


class ThoughtState:
    """
    æ¨ç†çŠ¶æ€ - AI çš„"çµé­‚æŒ‡çº¹"

    ä¿å­˜æ¨¡å‹åœ¨æŸä¸€æ—¶åˆ»çš„å®Œæ•´æ€ç»´çŠ¶æ€ï¼Œ
    ä½¿å…¶èƒ½åœ¨åç»­å¯¹è¯ä¸­è¢«ç²¾ç¡®æ¢å¤ã€‚
    """

    def __init__(
        self,
        prompt_context: str,           # å®Œæ•´çš„ prompt ä¸Šä¸‹æ–‡
        response_text: str,            # æ¨¡å‹çš„å›å¤
        function_call: Optional[Dict], # å·¥å…·è°ƒç”¨ (å¦‚æœæœ‰)
        thinking_level: str,           # æ€è€ƒç­‰çº§
        session_id: str,               # ä¼šè¯ ID
    ):
        self.prompt_context = prompt_context
        self.response_text = response_text
        self.function_call = function_call
        self.thinking_level = thinking_level
        self.session_id = session_id
        self.created_at = time.time()
        self.turn_count = 1

    def to_dict(self) -> Dict:
        return {
            "prompt_context": self.prompt_context,
            "response_text": self.response_text,
            "function_call": self.function_call,
            "thinking_level": self.thinking_level,
            "session_id": self.session_id,
            "created_at": self.created_at,
            "turn_count": self.turn_count,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "ThoughtState":
        state = cls(
            prompt_context=data["prompt_context"],
            response_text=data["response_text"],
            function_call=data.get("function_call"),
            thinking_level=data["thinking_level"],
            session_id=data["session_id"],
        )
        state.created_at = data["created_at"]
        state.turn_count = data.get("turn_count", 1)
        return state


def generate_thought_signature(
    prompt_context: str,
    response_text: str,
    function_call: Optional[Dict] = None,
    thinking_level: str = "high",
    session_id: str = "",
    slot_id: int = 0,  # æ–°å¢: ç”¨äº KV Cache æŒä¹…åŒ–
) -> str:
    """
    ç”Ÿæˆ thought signature (ä¸é€æ˜çš„åŠ å¯†ç­¾å)

    æ”¹è¿›ç‰ˆ: ä½¿ç”¨ä¸çœŸå® Gemini API ç›¸åŒçš„ JSON + Base64 æ ¼å¼

    çœŸå® API ç­¾åè§£ç ç¤ºä¾‹:
      {"v": 1, "ts": 1767355771, "hash": "mZmireGV1ihesgGv3E2G0A=="}

    Args:
        prompt_context: å®Œæ•´çš„ prompt ä¸Šä¸‹æ–‡
        response_text: æ¨¡å‹çš„å›å¤æ–‡æœ¬
        function_call: å·¥å…·è°ƒç”¨è¯¦æƒ… (å¦‚æœæœ‰)
        thinking_level: æ€è€ƒç­‰çº§
        session_id: ä¼šè¯ ID
        slot_id: llama-server slot ID (ç”¨äº KV Cache)

    Returns:
        Base64 ç¼–ç çš„ JSON ç­¾å (ä¸çœŸå® API æ ¼å¼ä¸€è‡´)
    """
    # 1. ç”Ÿæˆæ—¶é—´æˆ³ (ç§’çº§ï¼Œä¸çœŸå® API ä¸€è‡´)
    timestamp = int(time.time())

    # 2. ç”Ÿæˆå”¯ä¸€ç­¾å ID
    random_bytes = os.urandom(16)
    signature_id = hashlib.sha256(
        f"{timestamp}:{random_bytes.hex()}:{session_id}".encode()
    ).hexdigest()[:32]

    # 3. å°è¯•ä¿å­˜ KV Cache (çœŸæ­£çš„çŠ¶æ€æŒä¹…åŒ–)
    kv_cache_id = None
    if KV_CACHE_ENABLED:
        kv_cache_id = save_kv_cache(slot_id)
        if kv_cache_id:
            print(f"[thoughtSignature] KV Cache saved: {kv_cache_id}")

    # 4. ä¿å­˜æ¨ç†çŠ¶æ€åˆ°å†…å­˜ç¼“å­˜
    state = ThoughtState(
        prompt_context=prompt_context,
        response_text=response_text,
        function_call=function_call,
        thinking_level=thinking_level,
        session_id=session_id or str(uuid.uuid4()),
    )
    state_dict = state.to_dict()
    state_dict["kv_cache_id"] = kv_cache_id  # å…³è” KV Cache
    state_dict["slot_id"] = slot_id
    thought_state_cache[signature_id] = state_dict

    # 5. ç”Ÿæˆç­¾å hash (ç”¨äºéªŒè¯)
    # åŒ…å« signature_id å’Œ secret çš„ HMAC
    hash_input = f"{signature_id}:{THOUGHT_SIGNATURE_SECRET}:{timestamp}"
    hash_value = base64.b64encode(
        hashlib.md5(hash_input.encode()).digest()
    ).decode()

    # 6. æ„å»ºç­¾å (æ¨¡æ‹ŸçœŸå® API çš„ Protocol Buffer æ ¼å¼)
    #
    # çœŸå® Gemini 3 API åˆ†æ:
    #   - ç­¾åé•¿åº¦: ~792 chars (è§£ç å ~593 bytes)
    #   - æ ¼å¼: Protocol Buffer (ç¬¬ä¸€ä¸ªå­—èŠ‚ 0x12)
    #   - åŒ…å«: æ³¨æ„åŠ›çŠ¶æ€ã€KV Cache å“ˆå¸Œã€æ¨ç†è·¯å¾„ç¼–ç 
    #
    # æˆ‘ä»¬æ¨¡æ‹Ÿè¿™ä¸ªç»“æ„:
    #   [0x12][é•¿åº¦][payload][ç­¾å]

    # æ„å»º payload (æ¨¡æ‹Ÿ Protobuf ç»“æ„)
    payload_parts = []

    # Field 1: ç‰ˆæœ¬ (varint)
    version = 2 if kv_cache_id else 1
    payload_parts.append(bytes([0x08, version]))  # field 1, varint

    # Field 2: æ—¶é—´æˆ³ (fixed64)
    payload_parts.append(bytes([0x11]))  # field 2, fixed64
    payload_parts.append(timestamp.to_bytes(8, 'little'))

    # Field 3: ç­¾å ID (length-delimited string)
    sig_id_bytes = signature_id.encode()
    payload_parts.append(bytes([0x1a, len(sig_id_bytes)]))  # field 3, string
    payload_parts.append(sig_id_bytes)

    # Field 4: çŠ¶æ€å“ˆå¸Œ (æ¨¡æ‹Ÿ KV Cache çŠ¶æ€)
    state_hash = hashlib.sha256(
        f"{prompt_context}:{response_text}:{thinking_level}".encode()
    ).digest()
    payload_parts.append(bytes([0x22, len(state_hash)]))  # field 4, bytes
    payload_parts.append(state_hash)

    # Field 5: å¡«å……æ•°æ® (æ¨¡æ‹ŸçœŸå® API çš„å¤§é‡çŠ¶æ€æ•°æ®)
    # çœŸå® API æœ‰ ~593 bytesï¼Œæˆ‘ä»¬æ ¹æ® thinking_level è°ƒæ•´
    padding_size = {
        "minimal": 64,
        "low": 128,
        "medium": 256,
        "high": 400,  # æ¥è¿‘çœŸå® API çš„å¤§å°
    }.get(thinking_level, 200)
    padding = os.urandom(padding_size)
    payload_parts.append(bytes([0x2a]) + bytes([padding_size & 0x7f | 0x80, padding_size >> 7]) if padding_size > 127 else bytes([0x2a, padding_size]))
    payload_parts.append(padding)

    # Field 6: HMAC ç­¾å
    payload_data = b''.join(payload_parts)
    hmac_sig = hashlib.sha256(
        THOUGHT_SIGNATURE_SECRET.encode() + payload_data
    ).digest()
    payload_parts.append(bytes([0x32, len(hmac_sig)]))  # field 6, bytes
    payload_parts.append(hmac_sig)

    # ç»„è£…æœ€ç»ˆç­¾å (Protobuf æ ¼å¼)
    final_payload = b''.join(payload_parts)

    # æ·»åŠ å¤–å±‚åŒ…è£… (field 2, length-delimited)
    # 0x12 æ˜¯çœŸå® API ç­¾åçš„ç¬¬ä¸€ä¸ªå­—èŠ‚
    signature_bytes = bytes([0x12]) + _encode_varint(len(final_payload)) + final_payload

    # 7. Base64 ç¼–ç 
    signature_str = base64.b64encode(signature_bytes).decode()

    # æ¸…ç†è¿‡æœŸç¼“å­˜
    _cleanup_expired_signatures()
    cleanup_old_kv_cache()

    return signature_str


def validate_thought_signature(signature: str) -> Optional[Dict]:
    """
    éªŒè¯ thought signature å¹¶è¿”å›æ¨ç†çŠ¶æ€

    æ”¯æŒä¸¤ç§æ ¼å¼:
      1. Protocol Buffer æ ¼å¼ (æ–°): ç¬¬ä¸€ä¸ªå­—èŠ‚ 0x12
      2. JSON æ ¼å¼ (å…¼å®¹æ—§ç‰ˆ)

    Returns:
        æ¨ç†çŠ¶æ€å­—å…¸ (åŒ…å« kv_cache_id å¦‚æœæœ‰)ï¼Œç­¾åæ— æ•ˆåˆ™è¿”å› None
    """
    try:
        # è§£ç  Base64
        raw = base64.b64decode(signature)

        # æ£€æµ‹æ ¼å¼
        if raw[0] == 0x12:
            # Protocol Buffer æ ¼å¼
            return _validate_protobuf_signature(raw)
        else:
            # å°è¯• JSON æ ¼å¼ (å…¼å®¹æ—§ç‰ˆ)
            return _validate_json_signature(raw)

    except Exception as e:
        print(f"[validate_thought_signature] Error: {e}")
        return None


def _validate_protobuf_signature(raw: bytes) -> Optional[Dict]:
    """éªŒè¯ Protocol Buffer æ ¼å¼çš„ç­¾å"""
    try:
        # è·³è¿‡å¤–å±‚åŒ…è£… (0x12 + varint length)
        pos = 1
        length, bytes_read = _decode_varint(raw[pos:])
        pos += bytes_read

        # è§£æ payload
        payload = raw[pos:pos + length]

        # æå–å­—æ®µ
        fields = {}
        p = 0
        while p < len(payload):
            if p >= len(payload):
                break
            tag = payload[p]
            field_num = tag >> 3
            wire_type = tag & 0x07
            p += 1

            if wire_type == 0:  # varint
                val, bytes_read = _decode_varint(payload[p:])
                fields[field_num] = val
                p += bytes_read
            elif wire_type == 1:  # fixed64
                fields[field_num] = int.from_bytes(payload[p:p+8], 'little')
                p += 8
            elif wire_type == 2:  # length-delimited
                length, bytes_read = _decode_varint(payload[p:])
                p += bytes_read
                fields[field_num] = payload[p:p+length]
                p += length
            else:
                break

        # æå–å…³é”®å­—æ®µ
        version = fields.get(1, 1)
        timestamp = fields.get(2, 0)
        sig_id = fields.get(3, b'').decode() if isinstance(fields.get(3), bytes) else ''

        # æ£€æŸ¥æ—¶é—´æˆ³æœ‰æ•ˆæ€§
        current_time = int(time.time())
        if current_time - timestamp > SIGNATURE_TTL:
            print(f"[validate_thought_signature] Signature expired")
            return None

        # ä»ç¼“å­˜ä¸­æ¢å¤çŠ¶æ€
        state = thought_state_cache.get(sig_id)
        if state:
            state["turn_count"] = state.get("turn_count", 1) + 1

            # å¦‚æœæœ‰ KV Cacheï¼Œå°è¯•æ¢å¤
            kv_cache_id = state.get("kv_cache_id")
            slot_id = state.get("slot_id", 0)
            if kv_cache_id and version == 2:
                if restore_kv_cache(kv_cache_id, slot_id):
                    state["kv_cache_restored"] = True
                    print(f"[validate_thought_signature] KV Cache restored: {kv_cache_id}")
                else:
                    state["kv_cache_restored"] = False

            return state

        print(f"[validate_thought_signature] No matching state found")
        return None

    except Exception as e:
        print(f"[_validate_protobuf_signature] Error: {e}")
        return None


def _validate_json_signature(raw: bytes) -> Optional[Dict]:
    """éªŒè¯ JSON æ ¼å¼çš„ç­¾å (å…¼å®¹æ—§ç‰ˆ)"""
    try:
        sig_data = json.loads(raw.decode())
        version = sig_data.get("v", 1)
        timestamp = sig_data.get("ts", 0)
        hash_value = sig_data.get("hash", "")

        current_time = int(time.time())
        if current_time - timestamp > SIGNATURE_TTL:
            return None

        for sig_id, state in thought_state_cache.items():
            hash_input = f"{sig_id}:{THOUGHT_SIGNATURE_SECRET}:{timestamp}"
            expected_hash = base64.b64encode(
                hashlib.md5(hash_input.encode()).digest()
            ).decode()

            if expected_hash == hash_value:
                state["turn_count"] = state.get("turn_count", 1) + 1
                return state

        return None

    except json.JSONDecodeError:
        return None


def _decode_varint(data: bytes) -> tuple:
    """è§£ç  Protocol Buffer varintï¼Œè¿”å› (å€¼, è¯»å–çš„å­—èŠ‚æ•°)"""
    result = 0
    shift = 0
    pos = 0
    while pos < len(data):
        byte = data[pos]
        result |= (byte & 0x7f) << shift
        pos += 1
        if not (byte & 0x80):
            break
        shift += 7
    return result, pos


def _cleanup_expired_signatures():
    """æ¸…ç†è¿‡æœŸçš„ç­¾åç¼“å­˜"""
    current_time = time.time()
    expired_keys = [
        key for key, state in thought_state_cache.items()
        if current_time - state.get("created_at", 0) > SIGNATURE_TTL
    ]
    for key in expired_keys:
        del thought_state_cache[key]


def _encode_varint(value: int) -> bytes:
    """ç¼–ç  Protocol Buffer varint æ ¼å¼"""
    result = []
    while value > 127:
        result.append((value & 0x7f) | 0x80)
        value >>= 7
    result.append(value)
    return bytes(result)


def restore_context_from_signature(contents: List[Dict]) -> Optional[str]:
    """
    ä»å¯¹è¯å†å²ä¸­çš„ thoughtSignature æ¢å¤ä¸Šä¸‹æ–‡

    æ‰«æ contents ä¸­çš„ model å›å¤ï¼Œæå– thoughtSignatureï¼Œ
    æ¢å¤ä¹‹å‰çš„æ¨ç†ä¸Šä¸‹æ–‡ç”¨äºå¢å¼ºå½“å‰è½®çš„ promptã€‚

    Returns:
        æ¢å¤çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œç”¨äºæ³¨å…¥åˆ° prompt ä¸­
    """
    restored_context = []

    for content in contents:
        if content.get("role") == "model":
            parts = content.get("parts", [])
            for part in parts:
                if "thoughtSignature" in part:
                    sig = part["thoughtSignature"]
                    state = validate_thought_signature(sig)
                    if state:
                        # æ¢å¤æ¨ç†ä¸Šä¸‹æ–‡
                        if state.get("function_call"):
                            restored_context.append(
                                f"[Previous reasoning: Called {state['function_call'].get('name')} "
                                f"with intent to process the result]"
                            )

    return "\n".join(restored_context) if restored_context else None


# ========== llama-server ç®¡ç† ==========

# KV Cache æŒä¹…åŒ–ç›®å½• (ç”¨äºçœŸæ­£çš„ thoughtSignature)
KV_CACHE_DIR = Path(os.environ.get("KV_CACHE_DIR", "/tmp/gemma3n_thought_cache"))
KV_CACHE_ENABLED = os.environ.get("KV_CACHE_ENABLED", "true").lower() == "true"


def start_llama_server():
    """
    å¯åŠ¨ llama-server

    æ”¹è¿›: å¼€å¯ slot persistence æ”¯æŒçœŸæ­£çš„ KV Cache æŒä¹…åŒ–
    å‚è€ƒ: https://github.com/ggml-org/llama.cpp/discussions/13606
    """
    global llama_server_process, llama_server_ready

    if not Path(LLAMA_SERVER_BIN).exists():
        print(f"[ERROR] llama-server ä¸å­˜åœ¨: {LLAMA_SERVER_BIN}")
        return False

    if not LLAMA_MODEL or not Path(LLAMA_MODEL).exists():
        print(f"[ERROR] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_MODEL}")
        return False

    # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿è¡Œ
    try:
        import requests
        resp = requests.get(f"http://127.0.0.1:{LLAMA_SERVER_PORT}/health", timeout=2)
        if resp.status_code == 200:
            print(f"[llama-server] å·²åœ¨ç«¯å£ {LLAMA_SERVER_PORT} è¿è¡Œ")
            llama_server_ready = True
            return True
    except:
        pass

    # åˆ›å»º KV Cache ç›®å½•
    if KV_CACHE_ENABLED:
        KV_CACHE_DIR.mkdir(parents=True, exist_ok=True)
        print(f"[llama-server] KV Cache ç›®å½•: {KV_CACHE_DIR}")

    print(f"[llama-server] å¯åŠ¨ä¸­... ç«¯å£: {LLAMA_SERVER_PORT}")
    print(f"[llama-server] æ¨¡å‹: {LLAMA_MODEL}")

    env = os.environ.copy()
    bin_dir = str(Path(LLAMA_SERVER_BIN).parent)
    env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"

    cmd = [
        LLAMA_SERVER_BIN,
        "-m", LLAMA_MODEL,
        "--port", str(LLAMA_SERVER_PORT),
        "--host", "127.0.0.1",
        "-ngl", "999",
        "-t", "8",
        "--ctx-size", "8192",
    ]

    # å¤šæ¨¡æ€æ”¯æŒ: æ·»åŠ  vision/audio projector
    if MULTIMODAL_ENABLED:
        cmd.extend(["-mm", LLAMA_MMPROJ])
        capabilities = []
        if VISION_ENABLED:
            capabilities.append("è§†è§‰")
        if AUDIO_ENABLED:
            capabilities.append("éŸ³é¢‘")
        print(f"[llama-server] å¤šæ¨¡æ€å·²å¯ç”¨: {'+'.join(capabilities)}")
        if VISION_ENABLED:
            print(f"  - Vision: {Path(LLAMA_MMPROJ_VISION).name}")
        if AUDIO_ENABLED:
            print(f"  - Audio: {Path(LLAMA_MMPROJ_AUDIO).name}")

    # å¼€å¯ slot persistence (çœŸæ­£çš„ KV Cache æŒä¹…åŒ–)
    if KV_CACHE_ENABLED:
        cmd.extend([
            "--slot-save-path", str(KV_CACHE_DIR),  # KV Cache ä¿å­˜ç›®å½•
            "-np", "4",  # 4 ä¸ª slots æ”¯æŒå¹¶å‘
        ])
        print(f"[llama-server] KV Cache æŒä¹…åŒ–å·²å¯ç”¨")

    llama_server_process = subprocess.Popen(
        cmd,
        env=env,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    import requests
    for i in range(60):
        try:
            resp = requests.get(f"http://127.0.0.1:{LLAMA_SERVER_PORT}/health", timeout=1)
            if resp.status_code == 200:
                print(f"[llama-server] å¯åŠ¨æˆåŠŸï¼")
                llama_server_ready = True
                return True
        except:
            pass
        time.sleep(1)
        if i % 10 == 0:
            print(f"[llama-server] ç­‰å¾…å¯åŠ¨... {i}s")

    print("[llama-server] å¯åŠ¨è¶…æ—¶")
    return False


def query_llama_server(
    prompt: str,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    temperature: float = DEFAULT_TEMPERATURE,
    stop_sequences: List[str] = None,
    image_data: List[str] = None  # å¤šæ¨¡æ€: Base64 å›¾åƒæ•°æ®åˆ—è¡¨
) -> Dict[str, Any]:
    """æŸ¥è¯¢ llama-server"""
    global llama_server_ready

    if not llama_server_ready:
        if not start_llama_server():
            return {"error": "llama-server æœªå°±ç»ª"}

    start = time.time()
    try:
        import requests

        stop = stop_sequences or ["</s>", "<eos>", "<end_of_turn>"]

        # æ„å»ºè¯·æ±‚ä½“
        request_body = {
            "n_predict": max_tokens,
            "temperature": temperature,
            "stop": stop,
            "stream": False,
        }

        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒæ•°æ® (å¤šæ¨¡æ€)
        if image_data and MULTIMODAL_ENABLED:
            # å¤„ç†å›¾åƒæ•°æ®: llama-server éœ€è¦çº¯ base64ï¼Œä¸å¸¦ data: å‰ç¼€
            processed_images = []
            for img in image_data:
                if img.startswith("data:"):
                    # ç§»é™¤ data:image/xxx;base64, å‰ç¼€
                    try:
                        _, b64_data = img.split(",", 1)
                        processed_images.append(b64_data)
                    except:
                        processed_images.append(img)
                else:
                    processed_images.append(img)

            # ä½¿ç”¨å¤šæ¨¡æ€æ ¼å¼
            request_body["prompt"] = {
                "prompt_string": prompt,
                "multimodal_data": processed_images
            }
            print(f"[query] å¤šæ¨¡æ€è¯·æ±‚: {len(processed_images)} å¼ å›¾åƒ")
        else:
            # çº¯æ–‡æœ¬æ ¼å¼
            request_body["prompt"] = prompt

        # æ ¹æ®æ˜¯å¦å¤šæ¨¡æ€é€‰æ‹©ç«¯ç‚¹
        # å¤šæ¨¡æ€éœ€è¦ä½¿ç”¨ /completions (å¸¦ s)
        endpoint = "/completions" if (image_data and MULTIMODAL_ENABLED) else "/completion"

        resp = requests.post(
            f"http://127.0.0.1:{LLAMA_SERVER_PORT}{endpoint}",
            json=request_body,
            timeout=120
        )

        elapsed = time.time() - start

        if resp.status_code != 200:
            return {"error": f"llama-server è¯·æ±‚å¤±è´¥: {resp.status_code}"}

        data = resp.json()
        response = data.get("content", "").strip()
        tokens_predicted = data.get("tokens_predicted", len(response) // 4)
        tokens_evaluated = data.get("tokens_evaluated", 0)

        timings = data.get("timings", {})
        speed = timings.get("predicted_per_second", 0)
        if speed == 0:
            speed = tokens_predicted / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "prompt_tokens": tokens_evaluated,
            "completion_tokens": tokens_predicted,
            "total_time": elapsed,
            "speed": speed
        }
    except Exception as e:
        llama_server_ready = False
        return {"error": f"llama-server é”™è¯¯: {str(e)}"}


def run_llama_mtmd_cli(
    prompt: str,
    image_path: str = None,
    audio_path: str = None,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    temperature: float = DEFAULT_TEMPERATURE
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ llama-mtmd-cli è¿›è¡Œå¤šæ¨¡æ€æ¨ç† (æ”¯æŒéŸ³é¢‘)

    llama-server åªæ”¯æŒè§†è§‰å¤šæ¨¡æ€ï¼ŒéŸ³é¢‘éœ€è¦ä½¿ç”¨ llama-mtmd-cli å‘½ä»¤è¡Œå·¥å…·

    Args:
        prompt: æ–‡æœ¬æç¤º
        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„ (å¯é€‰)
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (å¯é€‰)
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        åŒ…å« responseã€prompt_tokensã€completion_tokens ç­‰çš„å­—å…¸
    """
    start = time.time()

    # æ£€æŸ¥ llama-mtmd-cli æ˜¯å¦å­˜åœ¨
    if not Path(LLAMA_MTMD_BIN).exists():
        return {"error": f"llama-mtmd-cli ä¸å­˜åœ¨: {LLAMA_MTMD_BIN}"}

    # é€‰æ‹©æ¨¡å‹: éŸ³é¢‘æ¨ç†ä½¿ç”¨ä¸“ç”¨æ¨¡å‹ (åŸå§‹æ¨¡å‹ï¼Œéå¾®è°ƒç‰ˆ)
    # å› ä¸ºéŸ³é¢‘ mmproj ä»…ä¸åŸå§‹æ¨¡å‹å…¼å®¹
    model_path = LLAMA_MODEL_AUDIO if audio_path else LLAMA_MODEL

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not Path(model_path).exists():
        return {"error": f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"}

    # åŠ¨æ€é€‰æ‹© mmproj
    mmproj_list = []
    if image_path and LLAMA_MMPROJ_VISION and Path(LLAMA_MMPROJ_VISION).exists():
        mmproj_list.append(LLAMA_MMPROJ_VISION)
    if audio_path and LLAMA_MMPROJ_AUDIO and Path(LLAMA_MMPROJ_AUDIO).exists():
        mmproj_list.append(LLAMA_MMPROJ_AUDIO)

    if not mmproj_list:
        return {"error": "æœªæ‰¾åˆ°æ‰€éœ€çš„ mmproj æ–‡ä»¶"}

    mmproj_combined = ",".join(mmproj_list)

    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    bin_dir = str(Path(LLAMA_MTMD_BIN).parent)
    env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"
    env["LD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('LD_LIBRARY_PATH', '')}"

    # æ„å»ºå‘½ä»¤ (ä¸ WebUI ä¿æŒä¸€è‡´)
    # å‚è€ƒ: apps/webui/server.py run_llama_mmproj_cli()
    # - --log-verbosity 0: å‡å°‘æ—¥å¿—è¾“å‡º
    # - --no-warmup: è·³è¿‡ warmup (å¯¹éŸ³é¢‘å¿…é¡»)
    # - ä¸å¼ºåˆ¶ CPU æ¨¡å¼ï¼Œè®© llama.cpp è‡ªåŠ¨é€‰æ‹© GPU
    cmd = [
        LLAMA_MTMD_BIN,
        "--log-verbosity", "0",
        "--no-warmup",
        "-m", model_path,
        "--mmproj", mmproj_combined,
        "-p", prompt,
        "-n", str(max_tokens),
        "--temp", str(temperature),
    ]

    # æ·»åŠ å›¾åƒ
    if image_path:
        cmd.extend(["--image", image_path])
        print(f"[llama-mtmd-cli] å›¾åƒ: {image_path}")

    # æ·»åŠ éŸ³é¢‘
    if audio_path:
        cmd.extend(["--audio", audio_path])
        print(f"[llama-mtmd-cli] éŸ³é¢‘: {audio_path}")

    print(f"[llama-mtmd-cli] æ‰§è¡Œå‘½ä»¤: {' '.join(cmd[:8])}...")

    try:
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
        )

        elapsed = time.time() - start

        if result.returncode != 0:
            error_msg = result.stderr[:500] if result.stderr else "æœªçŸ¥é”™è¯¯"
            return {"error": f"llama-mtmd-cli æ‰§è¡Œå¤±è´¥: {error_msg}"}

        # è¿‡æ»¤è¾“å‡º (ä¸ WebUI ä¿æŒä¸€è‡´)
        # å‚è€ƒ: apps/webui/server.py run_llama_mmproj_cli()
        lines = [ln for ln in result.stdout.splitlines() if ln.strip()]
        content_lines = [
            ln for ln in lines
            if not ln.startswith(("ggml", "AVX", "gguf", "llama", "clip", "Using", "model", "warmup", "load"))
        ]
        response = "\n".join(content_lines).strip() if content_lines else "\n".join(lines).strip()

        # ä¼°ç®— token æ•°é‡
        tokens_predicted = len(response) // 4

        print(f"[llama-mtmd-cli] å®Œæˆ, è€—æ—¶ {elapsed:.2f}s, è¾“å‡º {len(response)} å­—ç¬¦")

        return {
            "response": response,
            "prompt_tokens": 0,  # CLI æ¨¡å¼æ— æ³•è·å–
            "completion_tokens": tokens_predicted,
            "total_time": elapsed,
            "speed": tokens_predicted / elapsed if elapsed > 0 else 0
        }

    except subprocess.TimeoutExpired:
        return {"error": "llama-mtmd-cli æ‰§è¡Œè¶…æ—¶"}
    except Exception as e:
        return {"error": f"llama-mtmd-cli é”™è¯¯: {str(e)}"}


# ========== KV Cache æŒä¹…åŒ– (çœŸæ­£çš„ thoughtSignature) ==========

def save_kv_cache(slot_id: int = 0) -> Optional[str]:
    """
    ä¿å­˜æŒ‡å®š slot çš„ KV Cache åˆ°ç£ç›˜

    è¿™æ˜¯çœŸæ­£çš„ thoughtSignature å®ç°æ ¸å¿ƒ:
    å°†æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€ (KV Cache) æŒä¹…åŒ–ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¿å­˜ prompt æ–‡æœ¬

    å‚è€ƒ: https://github.com/ggml-org/llama.cpp/discussions/13606

    Returns:
        ä¿å­˜çš„æ–‡ä»¶å (ç”¨ä½œ thoughtSignature)ï¼Œå¤±è´¥è¿”å› None
    """
    if not KV_CACHE_ENABLED:
        return None

    try:
        import urllib.request
        import urllib.parse

        # ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜æ–‡ä»¶å
        cache_id = f"thought_{int(time.time() * 1000)}_{uuid.uuid4().hex[:8]}"

        # è°ƒç”¨ llama-server çš„ slot save API
        url = f"http://127.0.0.1:{LLAMA_SERVER_PORT}/slots/{slot_id}?action=save&filename={cache_id}"

        req = urllib.request.Request(url, method="POST")
        with urllib.request.urlopen(req, timeout=30) as resp:
            if resp.status == 200:
                print(f"[KV Cache] Saved slot {slot_id} to {cache_id}")
                return cache_id

    except Exception as e:
        print(f"[KV Cache] Save failed: {e}")

    return None


def restore_kv_cache(cache_id: str, slot_id: int = 0) -> bool:
    """
    ä»ç£ç›˜æ¢å¤ KV Cache åˆ°æŒ‡å®š slot

    è¿™å…è®¸æ¨¡å‹"æ¢å¤"ä¹‹å‰çš„æ€è€ƒçŠ¶æ€ï¼Œå®ç°çœŸæ­£çš„å¤šè½®æ¨ç†è¿ç»­æ€§

    Args:
        cache_id: ä¹‹å‰ä¿å­˜æ—¶è¿”å›çš„ç¼“å­˜ ID (thoughtSignature)
        slot_id: è¦æ¢å¤åˆ°çš„ slot ID

    Returns:
        æ˜¯å¦æ¢å¤æˆåŠŸ
    """
    if not KV_CACHE_ENABLED or not cache_id:
        return False

    try:
        import urllib.request

        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cache_file = KV_CACHE_DIR / f"{cache_id}.bin"
        if not cache_file.exists():
            print(f"[KV Cache] Cache file not found: {cache_id}")
            return False

        # è°ƒç”¨ llama-server çš„ slot restore API
        url = f"http://127.0.0.1:{LLAMA_SERVER_PORT}/slots/{slot_id}?action=restore&filename={cache_id}"

        req = urllib.request.Request(url, method="POST")
        with urllib.request.urlopen(req, timeout=30) as resp:
            if resp.status == 200:
                print(f"[KV Cache] Restored slot {slot_id} from {cache_id}")
                return True

    except Exception as e:
        print(f"[KV Cache] Restore failed: {e}")

    return False


def cleanup_old_kv_cache(max_age_hours: int = 1):
    """æ¸…ç†è¿‡æœŸçš„ KV Cache æ–‡ä»¶"""
    if not KV_CACHE_ENABLED:
        return

    try:
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600

        for cache_file in KV_CACHE_DIR.glob("thought_*.bin"):
            file_age = current_time - cache_file.stat().st_mtime
            if file_age > max_age_seconds:
                cache_file.unlink()
                print(f"[KV Cache] Cleaned up: {cache_file.name}")

    except Exception as e:
        print(f"[KV Cache] Cleanup error: {e}")


# ========== Gemini API æ ¼å¼è½¬æ¢ ==========

# æ”¯æŒçš„å›¾åƒ MIME ç±»å‹
SUPPORTED_IMAGE_TYPES = {
    "image/png", "image/jpeg", "image/jpg", "image/webp",
    "image/heic", "image/heif", "image/gif"
}

SUPPORTED_AUDIO_TYPES = {
    "audio/wav", "audio/wave", "audio/x-wav",
    "audio/mp3", "audio/mpeg",
    "audio/ogg", "audio/vorbis",
    "audio/flac", "audio/x-flac",
    "audio/aac", "audio/mp4",
    "audio/webm"
}

SUPPORTED_MEDIA_TYPES = SUPPORTED_IMAGE_TYPES | SUPPORTED_AUDIO_TYPES


def extract_audio_from_media(media_data_list: List[str]) -> tuple:
    """
    ä»åª’ä½“æ•°æ®åˆ—è¡¨ä¸­åˆ†ç¦»éŸ³é¢‘å’Œå›¾åƒ

    Args:
        media_data_list: data URI æ ¼å¼çš„åª’ä½“æ•°æ®åˆ—è¡¨

    Returns:
        (image_data_list, audio_data_list)
        - image_data_list: ä»…åŒ…å«å›¾åƒçš„ data URI åˆ—è¡¨
        - audio_data_list: ä»…åŒ…å«éŸ³é¢‘çš„ data URI åˆ—è¡¨
    """
    image_data = []
    audio_data = []

    for data_uri in media_data_list:
        # è§£æ data URI è·å– MIME ç±»å‹
        # æ ¼å¼: data:mime/type;base64,DATA
        if data_uri.startswith("data:"):
            try:
                # æå– mime type
                mime_part = data_uri.split(";")[0]  # "data:audio/flac"
                mime_type = mime_part.replace("data:", "")  # "audio/flac"

                if mime_type in SUPPORTED_AUDIO_TYPES:
                    audio_data.append(data_uri)
                elif mime_type in SUPPORTED_IMAGE_TYPES:
                    image_data.append(data_uri)
                else:
                    # æœªçŸ¥ç±»å‹ï¼Œå‡è®¾ä¸ºå›¾åƒ
                    image_data.append(data_uri)
            except:
                # è§£æå¤±è´¥ï¼Œå‡è®¾ä¸ºå›¾åƒ
                image_data.append(data_uri)
        else:
            # ä¸æ˜¯ data URIï¼Œå‡è®¾ä¸ºå›¾åƒ
            image_data.append(data_uri)

    return image_data, audio_data


def save_audio_to_temp_file(audio_data_uri: str) -> Optional[str]:
    """
    å°†éŸ³é¢‘ data URI ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶

    Args:
        audio_data_uri: data URI æ ¼å¼çš„éŸ³é¢‘æ•°æ®

    Returns:
        ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
    """
    import tempfile

    try:
        # è§£æ data URI
        # æ ¼å¼: data:audio/flac;base64,DATA
        header, b64_data = audio_data_uri.split(",", 1)
        mime_part = header.split(";")[0]  # "data:audio/flac"
        mime_type = mime_part.replace("data:", "")  # "audio/flac"

        # ç¡®å®šæ–‡ä»¶æ‰©å±•å
        ext_map = {
            "audio/wav": ".wav",
            "audio/wave": ".wav",
            "audio/x-wav": ".wav",
            "audio/mp3": ".mp3",
            "audio/mpeg": ".mp3",
            "audio/ogg": ".ogg",
            "audio/vorbis": ".ogg",
            "audio/flac": ".flac",
            "audio/x-flac": ".flac",
            "audio/aac": ".aac",
            "audio/mp4": ".m4a",
            "audio/webm": ".webm",
        }
        ext = ext_map.get(mime_type, ".audio")

        # è§£ç  base64 å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶
        audio_bytes = base64.b64decode(b64_data)

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ (ä¸è‡ªåŠ¨åˆ é™¤ï¼Œéœ€è¦æ‰‹åŠ¨æ¸…ç†)
        fd, temp_path = tempfile.mkstemp(suffix=ext, prefix="gemini_audio_")
        with os.fdopen(fd, 'wb') as f:
            f.write(audio_bytes)

        print(f"[save_audio] ä¿å­˜éŸ³é¢‘åˆ°: {temp_path} ({len(audio_bytes)} bytes)")
        return temp_path

    except Exception as e:
        print(f"[save_audio] ä¿å­˜éŸ³é¢‘å¤±è´¥: {e}")
        return None


def parse_gemini_contents(contents: List[Dict]) -> tuple:
    """
    è§£æ Gemini contents æ ¼å¼

    Returns:
        (prompt_text, media_data_list)
        - prompt_text: çº¯æ–‡æœ¬ prompt (ä½¿ç”¨ <__media__> æ ‡è®°åª’ä½“ä½ç½®)
        - media_data_list: Base64 ç¼–ç çš„åª’ä½“æ•°æ®åˆ—è¡¨ (å›¾åƒå’ŒéŸ³é¢‘)
    """
    messages = []
    media_data_list = []

    for content in contents:
        role = content.get("role", "user")
        parts = content.get("parts", [])

        text_parts = []
        for part in parts:
            if isinstance(part, str):
                text_parts.append(part)
            elif isinstance(part, dict):
                if "text" in part:
                    text_parts.append(part["text"])
                elif "inlineData" in part:
                    # å†…è”åª’ä½“æ•°æ® (Gemini API æ ¼å¼)
                    inline = part["inlineData"]
                    mime_type = inline.get("mimeType", "image/jpeg")
                    data = inline.get("data", "")

                    if mime_type in SUPPORTED_MEDIA_TYPES and data:
                        # æ·»åŠ åª’ä½“å ä½ç¬¦
                        text_parts.append("<__media__>")
                        # æ·»åŠ  base64 æ•°æ® (å¯èƒ½å¸¦æˆ–ä¸å¸¦ data URI å‰ç¼€)
                        if data.startswith("data:"):
                            # å·²ç»æ˜¯ data URI æ ¼å¼
                            media_data_list.append(data)
                        else:
                            # çº¯ base64ï¼Œæ·»åŠ  data URI å‰ç¼€
                            media_data_list.append(f"data:{mime_type};base64,{data}")

                        # æ—¥å¿—è®°å½•åª’ä½“ç±»å‹
                        if mime_type in SUPPORTED_IMAGE_TYPES:
                            print(f"[parse] æ£€æµ‹åˆ°å›¾åƒ: {mime_type}")
                        elif mime_type in SUPPORTED_AUDIO_TYPES:
                            print(f"[parse] æ£€æµ‹åˆ°éŸ³é¢‘: {mime_type}")
                    else:
                        print(f"[parse] ä¸æ”¯æŒçš„åª’ä½“ç±»å‹: {mime_type}")
                elif "fileData" in part:
                    # æ–‡ä»¶å¼•ç”¨ (éœ€è¦ä¸‹è½½)
                    file_data = part["fileData"]
                    file_uri = file_data.get("fileUri", "")
                    mime_type = file_data.get("mimeType", "image/jpeg")

                    if file_uri and mime_type in SUPPORTED_MEDIA_TYPES:
                        # å°è¯•ä¸‹è½½æ–‡ä»¶
                        try:
                            import requests as req
                            resp = req.get(file_uri, timeout=30)
                            if resp.status_code == 200:
                                data = base64.b64encode(resp.content).decode()
                                text_parts.append("<__media__>")
                                media_data_list.append(f"data:{mime_type};base64,{data}")

                                if mime_type in SUPPORTED_IMAGE_TYPES:
                                    print(f"[parse] ä¸‹è½½å›¾åƒæˆåŠŸ: {file_uri[:50]}...")
                                elif mime_type in SUPPORTED_AUDIO_TYPES:
                                    print(f"[parse] ä¸‹è½½éŸ³é¢‘æˆåŠŸ: {file_uri[:50]}...")
                        except Exception as e:
                            print(f"[parse] ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
                elif "thought" in part and part.get("thought"):
                    # æ€è€ƒè¿‡ç¨‹ï¼Œè·³è¿‡
                    continue
                elif "thoughtSignature" in part:
                    # ç­¾åï¼Œè·³è¿‡
                    continue
                elif "functionCall" in part:
                    # å‡½æ•°è°ƒç”¨
                    fc = part["functionCall"]
                    text_parts.append(f"[Function Call: {fc.get('name')}({json.dumps(fc.get('args', {}))})]")
                elif "functionResponse" in part:
                    # å‡½æ•°å“åº”
                    fr = part["functionResponse"]
                    text_parts.append(f"[Function Response: {fr.get('name')} = {json.dumps(fr.get('response', {}))}]")

        if text_parts:
            text = "\n".join(text_parts)
            if role == "user":
                messages.append(f"<start_of_turn>user\n{text}<end_of_turn>")
            elif role == "model":
                messages.append(f"<start_of_turn>model\n{text}<end_of_turn>")
            elif role == "function":
                messages.append(f"<start_of_turn>user\n{text}<end_of_turn>")

    # æ·»åŠ æ¨¡å‹å›å¤å¼€å¤´
    messages.append("<start_of_turn>model\n")

    return "".join(messages), media_data_list


def parse_system_instruction(system_instruction: Dict) -> str:
    """è§£æç³»ç»ŸæŒ‡ä»¤"""
    if not system_instruction:
        return ""

    parts = system_instruction.get("parts", [])
    texts = []
    for part in parts:
        if isinstance(part, dict) and "text" in part:
            texts.append(part["text"])
        elif isinstance(part, str):
            texts.append(part)

    return "\n".join(texts)


def parse_tools(tools: List[Dict], tool_config: Dict = None) -> str:
    """
    è§£æå·¥å…·å£°æ˜ä¸ºæç¤ºè¯

    Args:
        tools: å·¥å…·å£°æ˜åˆ—è¡¨
        tool_config: å·¥å…·é…ç½® (toolConfig)
            - functionCallingConfig.mode: AUTO | ANY | NONE
            - functionCallingConfig.allowedFunctionNames: å…è®¸çš„å‡½æ•°ååˆ—è¡¨

    Returns:
        å·¥å…·æç¤ºè¯å­—ç¬¦ä¸²
    """
    if not tools:
        return ""

    # è§£æ toolConfig
    fc_config = tool_config.get("functionCallingConfig", {}) if tool_config else {}
    mode = fc_config.get("mode", "AUTO")  # é»˜è®¤ AUTO
    allowed_names = fc_config.get("allowedFunctionNames", [])

    # NONE æ¨¡å¼: ç¦ç”¨å‡½æ•°è°ƒç”¨
    if mode == "NONE":
        return ""

    tool_descriptions = []
    has_code_execution = False

    for tool in tools:
        if "functionDeclarations" in tool:
            for func in tool["functionDeclarations"]:
                name = func.get("name", "unknown")

                # å¦‚æœæŒ‡å®šäº† allowedFunctionNamesï¼Œè¿‡æ»¤å‡½æ•°
                if allowed_names and name not in allowed_names:
                    continue

                desc = func.get("description", "")
                params = func.get("parameters", {})

                tool_descriptions.append(f"""
Function: {name}
Description: {desc}
Parameters: {json.dumps(params, indent=2)}
""")
        elif "codeExecution" in tool:
            has_code_execution = True
        elif "googleSearch" in tool:
            # ä¸æ”¯æŒï¼Œè·³è¿‡
            pass

    prompt_parts = []

    # å‡½æ•°å£°æ˜æç¤º
    if tool_descriptions:
        mode_instruction = ""
        if mode == "ANY":
            mode_instruction = "\nIMPORTANT: You MUST use one of the available tools to respond. Do not respond with plain text."
        else:  # AUTO
            mode_instruction = "\nUse a tool when appropriate, or respond with plain text if no tool is needed."

        prompt_parts.append(f"""
You have access to the following tools. When you need to use a tool, respond with a function call in the following format:
{{
  "functionCall": {{
    "name": "tool_name",
    "args": {{...}}
  }}
}}

For multiple function calls, include multiple functionCall objects:
{{
  "functionCalls": [
    {{"name": "tool1", "args": {{...}}}},
    {{"name": "tool2", "args": {{...}}}}
  ]
}}
{mode_instruction}

Available tools:
{"".join(tool_descriptions)}
""")

    # codeExecution æç¤º
    if has_code_execution:
        prompt_parts.append("""
You also have access to a Python code execution tool. When you need to run Python code, respond with:
{
  "executableCode": {
    "language": "PYTHON",
    "code": "your python code here"
  }
}
After execution, you will receive the result and should provide a final text response.
""")

    return "\n".join(prompt_parts)


def parse_response_schema(generation_config: Dict) -> Optional[Dict]:
    """è§£æå“åº” schema"""
    if not generation_config:
        return None

    mime_type = generation_config.get("responseMimeType")
    schema = generation_config.get("responseSchema")

    if mime_type == "application/json" and schema:
        return schema
    return None


# ========== æ€è€ƒæ¨¡æ‹Ÿç³»ç»Ÿ ==========
#
# é€šè¿‡ prompt engineering æ¨¡æ‹Ÿ Gemini 3 çš„ thinking åŠŸèƒ½
# è®©æ¨¡å‹å…ˆè¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œå†è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
#

THINKING_PROMPT_TEMPLATE = {
    "none": "",  # æ˜¾å¼ç¦ç”¨æ€è€ƒ
    "minimal": """Think briefly before answering. Use <thinking> for your thoughts and <answer> for your response.

""",
    "low": """Before answering, briefly consider the key points in 1-2 sentences inside <thinking> tags, then give your answer inside <answer> tags.

Format:
<thinking>Brief consideration...</thinking>
<answer>Your response...</answer>

""",
    "medium": """Before answering, think through the problem step by step inside <thinking> tags. Consider:
- What is being asked?
- What are the key points?
- What's the best approach?

Then provide your answer inside <answer> tags.

Format:
<thinking>Your reasoning process...</thinking>
<answer>Your final response...</answer>

""",
    "high": """Before answering, engage in thorough reasoning inside <thinking> tags. Consider:
- What exactly is the user asking?
- What background knowledge is relevant?
- What are different approaches or perspectives?
- What's the most helpful way to respond?

Take your time to think deeply, then provide your comprehensive answer inside <answer> tags.

Format:
<thinking>
Your detailed reasoning process...
- First, I need to understand...
- The key considerations are...
- My approach will be...
</thinking>
<answer>
Your final, well-considered response...
</answer>

"""
}

# é»˜è®¤æ€è€ƒç­‰çº§ (gemini-3-pro-preview é»˜è®¤ä¼šæ€è€ƒ)
DEFAULT_THINKING_LEVEL = "medium"


def parse_thinking_response(response_text: str) -> tuple:
    """
    è§£æåŒ…å«æ€è€ƒå’Œç­”æ¡ˆçš„å“åº”

    Returns:
        (thinking_text, answer_text, thoughts_tokens)
    """
    import re

    thinking_text = ""
    answer_text = response_text  # é»˜è®¤æ•´ä¸ªå“åº”éƒ½æ˜¯ç­”æ¡ˆ

    # å°è¯•æå– <thinking>...</thinking>
    thinking_match = re.search(r'<thinking>([\s\S]*?)</thinking>', response_text, re.IGNORECASE)
    if thinking_match:
        thinking_text = thinking_match.group(1).strip()

    # å°è¯•æå– <answer>...</answer>
    answer_match = re.search(r'<answer>([\s\S]*?)</answer>', response_text, re.IGNORECASE)
    if answer_match:
        answer_text = answer_match.group(1).strip()
    elif thinking_match:
        # å¦‚æœæœ‰ thinking ä½†æ²¡æœ‰ answer æ ‡ç­¾ï¼Œå– thinking ä¹‹åçš„å†…å®¹
        after_thinking = response_text[thinking_match.end():].strip()
        if after_thinking:
            answer_text = after_thinking
        else:
            # åªæœ‰æ€è€ƒï¼Œæ²¡æœ‰ç­”æ¡ˆï¼ˆå¯èƒ½æ˜¯ MAX_TOKENSï¼‰
            answer_text = ""

    # ä¼°ç®— thoughts tokens (å¤§çº¦ 4 å­—ç¬¦ = 1 token)
    thoughts_tokens = len(thinking_text) // 4 if thinking_text else 0

    return thinking_text, answer_text, thoughts_tokens


def build_gemini_response(
    response_text: str,
    prompt_tokens: int,
    completion_tokens: int,
    model_version: str = MODEL_VERSION,
    function_call: Dict = None,
    function_calls: List[Dict] = None,  # æ–°å¢: å¤šä¸ªå‡½æ•°è°ƒç”¨
    executable_code: Dict = None,       # æ–°å¢: codeExecution
    code_execution_result: Dict = None, # æ–°å¢: ä»£ç æ‰§è¡Œç»“æœ
    finish_reason: str = "STOP",
    thoughts_tokens: int = 0,
    prompt_context: str = "",       # å®Œæ•´çš„ prompt ä¸Šä¸‹æ–‡
    thinking_level: str = "medium", # æ€è€ƒç­‰çº§
    session_id: str = "",           # ä¼šè¯ ID
    thinking_text: str = "",        # æ€è€ƒå†…å®¹
    show_thinking: bool = True,     # æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ (low ç­‰çº§éšè—)
    tool_use_tokens: int = 0,       # æ–°å¢: å·¥å…·ä½¿ç”¨çš„ token æ•°
    has_audio: bool = False,        # æ–°å¢: æ˜¯å¦åŒ…å«éŸ³é¢‘è¾“å…¥
    audio_tokens: int = 0,          # æ–°å¢: éŸ³é¢‘ token æ•°
) -> Dict:
    """
    æ„å»º Gemini API æ ¼å¼çš„å“åº”
    å®Œå…¨å¯¹é½çœŸå® Gemini API çš„ JSON ç»“æ„å’Œå­—æ®µé¡ºåº

    æ”¹è¿›: æ”¯æŒå¤šä¸ªå‡½æ•°è°ƒç”¨ (Parallel Function Calling) å’Œ codeExecution
    """

    response_id = hashlib.md5(f"{time.time()}:{response_text[:50] if response_text else 'fc'}".encode()).hexdigest()[:22]

    # ç”Ÿæˆ thoughtSignature (ä¿å­˜å®Œæ•´æ¨ç†çŠ¶æ€)
    signature = generate_thought_signature(
        prompt_context=prompt_context,
        response_text=response_text,
        function_call=function_call or (function_calls[0] if function_calls else None),
        thinking_level=thinking_level,
        session_id=session_id or response_id,
    )

    parts = []

    # å½“ finish_reason=MAX_TOKENS æ—¶ï¼ŒçœŸå® API åªè¿”å›ä¸€ä¸ª part
    # è¿™è¡¨ç¤ºç”Ÿæˆè¢«æˆªæ–­ï¼Œæˆ‘ä»¬åº”è¯¥åªè¿”å›å·²ç”Ÿæˆçš„å†…å®¹
    if finish_reason == "MAX_TOKENS":
        # MAX_TOKENS æ—¶ï¼š
        # - å¦‚æœæœ‰ thinking_textï¼Œè¯´æ˜æ€è€ƒè¿‡ç¨‹å­˜åœ¨ï¼Œè¿”å›æ€è€ƒ part (with thought: true)
        # - å¦‚æœæ²¡æœ‰ thinking ä½†æœ‰ response_textï¼Œè¿”å› response part
        # çœŸå® Gemini API åœ¨é«˜æ€è€ƒæ¨¡å¼ä¸‹ MAX_TOKENS é€šå¸¸è¿”å› thought: true çš„ part
        if thinking_text:
            # è¿”å›æ€è€ƒå†…å®¹ä½œä¸ºå”¯ä¸€çš„ part
            parts.append({
                "text": thinking_text,
                "thought": True,
                "thoughtSignature": signature
            })
        elif response_text:
            parts.append({
                "text": response_text,
                "thoughtSignature": signature
            })
    else:
        # æ­£å¸¸æƒ…å†µï¼šå¦‚æœæœ‰æ€è€ƒå†…å®¹ä¸”éœ€è¦æ˜¾ç¤ºï¼Œå…ˆæ·»åŠ æ€è€ƒ part
        # æ³¨æ„: thinkingLevel=low æ—¶ä¸æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œä½† thoughtsTokenCount ä»ç„¶è®¡æ•°
        if thinking_text and show_thinking:
            parts.append({
                "text": thinking_text,
                "thought": True
            })

    # æ·»åŠ ä¸»å“åº” (åªåœ¨é MAX_TOKENS æ—¶å¤„ç†)
    has_tool_call = False

    # MAX_TOKENS å·²åœ¨ä¸Šé¢å¤„ç†ï¼Œè¿™é‡Œåªå¤„ç†æ­£å¸¸æƒ…å†µ
    if finish_reason != "MAX_TOKENS":
        # å¤„ç†å¤šä¸ªå‡½æ•°è°ƒç”¨ (Parallel Function Calling)
        if function_calls and len(function_calls) > 0:
            has_tool_call = True
            # ç¬¬ä¸€ä¸ªå‡½æ•°è°ƒç”¨å¸¦ thoughtSignature
            parts.append({
                "thoughtSignature": signature,
                "functionCall": function_calls[0]
            })
            # åç»­å‡½æ•°è°ƒç”¨ä¸å¸¦ thoughtSignature
            for fc in function_calls[1:]:
                parts.append({
                    "functionCall": fc
                })
        # å¤„ç†å•ä¸ªå‡½æ•°è°ƒç”¨ (å‘åå…¼å®¹)
        elif function_call:
            has_tool_call = True
            parts.append({
                "thoughtSignature": signature,
                "functionCall": function_call
            })
        # å¤„ç† codeExecution
        elif executable_code:
            has_tool_call = True
            parts.append({
                "thoughtSignature": signature,
                "executableCode": executable_code
            })
            # å¦‚æœæœ‰æ‰§è¡Œç»“æœï¼Œæ·»åŠ ç»“æœ part
            if code_execution_result:
                parts.append({
                    "codeExecutionResult": code_execution_result
                })
            # å¦‚æœæœ‰æœ€ç»ˆæ–‡æœ¬å“åº”
            if response_text:
                parts.append({
                    "text": response_text
                })
        elif response_text:
            # æ™®é€šæ–‡æœ¬å“åº” (åªæœ‰åœ¨æœ‰ç­”æ¡ˆæ—¶æ‰æ·»åŠ )
            parts.append({
                "text": response_text,
                "thoughtSignature": signature
            })
        elif thinking_text:
            # åªæœ‰æ€è€ƒæ²¡æœ‰ç­”æ¡ˆæ—¶ï¼Œåœ¨æ€è€ƒ part ä¸Šæ·»åŠ ç­¾å
            if parts:
                parts[-1]["thoughtSignature"] = signature

    # æŒ‰çœŸå® Gemini API çš„å­—æ®µé¡ºåºæ„å»ºå“åº”
    # çœŸå®é¡ºåº: candidates -> usageMetadata -> modelVersion -> responseId
    # æ³¨æ„: content å†…éƒ¨é¡ºåºæ˜¯ role -> parts
    from collections import OrderedDict

    # æ„å»º candidate å¯¹è±¡
    candidate = OrderedDict([
        ("content", OrderedDict([
            ("role", "model"),
            ("parts", parts)
        ])),
        ("finishReason", finish_reason),
        ("index", 0),
        ("safetyRatings", None)  # å§‹ç»ˆè¿”å› null (ä¸çœŸå® API ä¸€è‡´)
    ])

    # å¦‚æœæ˜¯å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ  finishMessage (ä¸çœŸå® API ä¸€è‡´)
    if has_tool_call:
        candidate["finishMessage"] = "Model generated function call(s)."

    # æ„å»º usageMetadata (å­—æ®µé¡ºåºä¸çœŸå® API å®Œå…¨ä¸€è‡´)
    # çœŸå® API é¡ºåº: promptTokenCount â†’ candidatesTokenCount â†’ totalTokenCount â†’ thoughtsTokenCount â†’ promptTokensDetails

    # æ„å»º promptTokensDetails (æ”¯æŒå¤šæ¨¡æ€)
    prompt_tokens_details = [
        {
            "modality": "TEXT",
            "tokenCount": prompt_tokens - audio_tokens  # æ–‡æœ¬ token = æ€» prompt token - éŸ³é¢‘ token
        }
    ]

    # å¦‚æœæœ‰éŸ³é¢‘ï¼Œæ·»åŠ  AUDIO æ¨¡æ€
    if has_audio and audio_tokens > 0:
        prompt_tokens_details.append({
            "modality": "AUDIO",
            "tokenCount": audio_tokens
        })

    usage_metadata = OrderedDict([
        ("promptTokenCount", prompt_tokens),
        ("candidatesTokenCount", completion_tokens),
        ("totalTokenCount", prompt_tokens + completion_tokens + thoughts_tokens + tool_use_tokens),
        ("thoughtsTokenCount", thoughts_tokens),
        ("promptTokensDetails", prompt_tokens_details)
    ])

    # å¦‚æœæœ‰å·¥å…·ä½¿ç”¨ï¼Œæ·»åŠ  toolUsePromptTokenCount
    if tool_use_tokens > 0:
        usage_metadata["toolUsePromptTokenCount"] = tool_use_tokens
        usage_metadata["toolUsePromptTokensDetails"] = [
            {
                "modality": "TEXT",
                "tokenCount": tool_use_tokens
            }
        ]

    # çœŸå® API å­—æ®µé¡ºåº: candidates -> promptFeedback -> usageMetadata -> modelVersion -> responseId
    response = OrderedDict([
        ("candidates", [candidate]),
        ("promptFeedback", {
            "safetyRatings": None
        }),
        ("usageMetadata", usage_metadata),
        ("modelVersion", model_version),
        ("responseId", response_id)
    ])

    return response


def extract_function_call(response_text: str) -> Optional[Dict]:
    """ä»å“åº”ä¸­æå–å‡½æ•°è°ƒç”¨ï¼ˆå•ä¸ªï¼‰"""
    result = extract_tool_calls(response_text)
    if result and result.get("function_calls"):
        return result["function_calls"][0]
    return None


def extract_tool_calls(response_text: str) -> Optional[Dict]:
    """
    ä»å“åº”ä¸­æå–å·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒå¤šä¸ªå‡½æ•°è°ƒç”¨å’Œ codeExecutionï¼‰

    æ”¹è¿›ç‰ˆï¼šä½¿ç”¨æ›´å¯é çš„ JSON è§£ææ–¹æ³•

    Returns:
        {
            "function_calls": [{"name": ..., "args": ...}, ...],
            "executable_code": {"language": "PYTHON", "code": ...} | None
        }
        æˆ– None (å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å·¥å…·è°ƒç”¨)
    """
    result = {
        "function_calls": [],
        "executable_code": None
    }

    def try_parse_json(text: str) -> Optional[Dict]:
        """å°è¯•è§£æ JSONï¼Œæ”¯æŒåµŒå¥—ç»“æ„"""
        text = text.strip()
        if not text.startswith('{'):
            return None
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            return None

    def find_json_objects(text: str) -> list:
        """æŸ¥æ‰¾æ–‡æœ¬ä¸­æ‰€æœ‰å®Œæ•´çš„ JSON å¯¹è±¡"""
        objects = []
        i = 0
        while i < len(text):
            if text[i] == '{':
                # æ‰¾åˆ°å®Œæ•´çš„ JSON å¯¹è±¡
                depth = 0
                start = i
                in_string = False
                escape_next = False
                j = i
                while j < len(text):
                    c = text[j]
                    if escape_next:
                        escape_next = False
                    elif c == '\\':
                        escape_next = True
                    elif c == '"' and not escape_next:
                        in_string = not in_string
                    elif not in_string:
                        if c == '{':
                            depth += 1
                        elif c == '}':
                            depth -= 1
                            if depth == 0:
                                json_str = text[start:j+1]
                                parsed = try_parse_json(json_str)
                                if parsed:
                                    objects.append(parsed)
                                break
                    j += 1
            i += 1
        return objects

    def extract_from_data(data: Dict):
        """ä»è§£æåçš„æ•°æ®ä¸­æå–å·¥å…·è°ƒç”¨"""
        # æ£€æŸ¥ functionCalls (å¤æ•° - å¤šä¸ªå‡½æ•°è°ƒç”¨)
        if "functionCalls" in data:
            for fc in data["functionCalls"]:
                if isinstance(fc, dict) and "name" in fc:
                    result["function_calls"].append(fc)
        # æ£€æŸ¥ functionCall (å•æ•°)
        elif "functionCall" in data:
            fc = data["functionCall"]
            if isinstance(fc, dict) and "name" in fc:
                result["function_calls"].append(fc)
        # å¦‚æœç›´æ¥æ˜¯ {name, args} æ ¼å¼
        elif "name" in data and "args" in data:
            result["function_calls"].append(data)

        # æ£€æŸ¥ executableCode
        if "executableCode" in data and not result["executable_code"]:
            result["executable_code"] = data["executableCode"]

    # 1. é¦–å…ˆå°è¯•æå– ```json ``` ä»£ç å—
    json_block_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
    if json_block_match:
        parsed = try_parse_json(json_block_match.group(1))
        if parsed:
            extract_from_data(parsed)

    # 2. å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬ä¸º JSON
    if not result["function_calls"]:
        parsed = try_parse_json(response_text)
        if parsed:
            extract_from_data(parsed)

    # 3. æŸ¥æ‰¾æ–‡æœ¬ä¸­çš„ JSON å¯¹è±¡
    if not result["function_calls"]:
        json_objects = find_json_objects(response_text)
        for obj in json_objects:
            extract_from_data(obj)
            if result["function_calls"]:
                break

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å·¥å…·è°ƒç”¨ï¼Œè¿”å› None
    if not result["function_calls"] and not result["executable_code"]:
        return None

    return result


def enforce_json_schema(response_text: str, schema: Dict) -> str:
    """å°è¯•å¼ºåˆ¶å“åº”ç¬¦åˆ JSON schema"""
    # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
    try:
        data = json.loads(response_text)
        return json.dumps(data, ensure_ascii=False, indent=2)
    except:
        pass

    # å°è¯•æå– JSON å—
    json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
    if json_match:
        try:
            data = json.loads(json_match.group(1))
            return json.dumps(data, ensure_ascii=False, indent=2)
        except:
            pass

    # å°è¯•æ‰¾åˆ° { } åŒ…å›´çš„ JSON
    json_match = re.search(r'\{[\s\S]*\}', response_text)
    if json_match:
        try:
            data = json.loads(json_match.group())
            return json.dumps(data, ensure_ascii=False, indent=2)
        except:
            pass

    # è¿”å›åŸå§‹å“åº”
    return response_text


# ========== API è·¯ç”± ==========

@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "ok",
        "model": LLAMA_MODEL,
        "server_ready": llama_server_ready
    })


@app.route(f"/{API_VERSION}/models/<model_name>:generateContent", methods=["POST"])
def generate_content(model_name: str):
    """
    Gemini generateContent API

    æ”¯æŒçš„æ¨¡å‹åç§°:
    - gemini-3-pro-preview
    - gemini-3-flash-preview
    - gemma-3n (æœ¬åœ°åˆ«å)
    """
    try:
        # è·å– API key (å¯é€‰éªŒè¯)
        api_key = request.args.get("key", "")

        data = request.json
        if not data:
            return jsonify({"error": {"message": "è¯·æ±‚ä½“ä¸ºç©º", "code": "400"}}), 400

        # è§£æè¯·æ±‚
        contents = data.get("contents", [])
        system_instruction = data.get("systemInstruction")
        generation_config = data.get("generationConfig", {})
        safety_settings = data.get("safetySettings", [])
        tools = data.get("tools", [])
        tool_config = data.get("toolConfig", {})

        if not contents:
            return jsonify({"error": {"message": "contents ä¸èƒ½ä¸ºç©º", "code": "400"}}), 400

        # è§£æç”Ÿæˆé…ç½®
        thinking_config = generation_config.get("thinkingConfig", {})
        # é»˜è®¤ä½¿ç”¨ medium æ€è€ƒç­‰çº§ (ä¸çœŸå® gemini-3-pro-preview è¡Œä¸ºä¸€è‡´)
        thinking_level = thinking_config.get("thinkingLevel", DEFAULT_THINKING_LEVEL)
        # includeThoughts: æ§åˆ¶æ˜¯å¦åœ¨å“åº”ä¸­æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ (ç‹¬ç«‹äº thinkingLevel)
        include_thoughts = thinking_config.get("includeThoughts", True)  # é»˜è®¤æ˜¾ç¤º

        max_tokens = generation_config.get("maxOutputTokens", DEFAULT_MAX_TOKENS)
        temperature = generation_config.get("temperature", DEFAULT_TEMPERATURE)

        # è§£æå“åº” schema
        response_schema = parse_response_schema(generation_config)

        # æ„å»º prompt
        prompt_parts = []

        # 1. ç³»ç»ŸæŒ‡ä»¤
        if system_instruction:
            sys_text = parse_system_instruction(system_instruction)
            if sys_text:
                prompt_parts.append(f"<start_of_turn>user\nSystem: {sys_text}<end_of_turn>")

        # 2. å·¥å…·å£°æ˜
        if tools:
            tools_prompt = parse_tools(tools, tool_config)
            if tools_prompt:
                prompt_parts.append(f"<start_of_turn>user\n{tools_prompt}<end_of_turn>")

        # 3. æ€è€ƒæç¤º (æ ¹æ® thinkingLevel æ·»åŠ )
        # æ³¨æ„: å·¥å…·è°ƒç”¨å’Œ JSON Schema æ¨¡å¼éƒ½æ”¯æŒæ€è€ƒ (çœŸå® API è¡Œä¸º)
        thinking_prompt = THINKING_PROMPT_TEMPLATE.get(thinking_level, "")
        if thinking_prompt:
            prompt_parts.append(f"<start_of_turn>user\n{thinking_prompt}<end_of_turn>")

        # 4. ç»“æ„åŒ–è¾“å‡ºæŒ‡ä»¤
        if response_schema:
            schema_prompt = f"""
Please respond with valid JSON that matches this schema:
{json.dumps(response_schema, indent=2)}

Your response must be valid JSON only, no other text.
"""
            prompt_parts.append(f"<start_of_turn>user\n{schema_prompt}<end_of_turn>")

        # 5. å¯¹è¯å†…å®¹ (åŒ…æ‹¬å¤šæ¨¡æ€è§£æ)
        conversation, media_data = parse_gemini_contents(contents)
        prompt_parts.append(conversation)

        full_prompt = "".join(prompt_parts)

        # æ—¥å¿—
        print(f"[generateContent] model={model_name}, thinking={thinking_level}")
        print(f"[generateContent] prompt length: {len(full_prompt)} chars")
        if media_data:
            print(f"[generateContent] å¤šæ¨¡æ€: {len(media_data)} ä¸ªåª’ä½“æ–‡ä»¶")

        # åˆ†ç¦»å›¾åƒå’ŒéŸ³é¢‘
        image_data, audio_data = extract_audio_from_media(media_data) if media_data else ([], [])

        # è°ƒç”¨æ¨¡å‹ (æ ¹æ®æ˜¯å¦æœ‰éŸ³é¢‘é€‰æ‹©ä¸åŒçš„åç«¯)
        temp_audio_file = None
        try:
            if audio_data:
                # éŸ³é¢‘æ¨¡å¼: ä½¿ç”¨ llama-mtmd-cli (æ”¯æŒéŸ³é¢‘)
                print(f"[generateContent] éŸ³é¢‘æ¨¡å¼: {len(audio_data)} ä¸ªéŸ³é¢‘æ–‡ä»¶")

                # ä¿å­˜ç¬¬ä¸€ä¸ªéŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_audio_file = save_audio_to_temp_file(audio_data[0])
                if not temp_audio_file:
                    return jsonify({
                        "error": {
                            "message": "éŸ³é¢‘æ–‡ä»¶ä¿å­˜å¤±è´¥",
                            "code": "500"
                        }
                    }), 500

                result = run_llama_mtmd_cli(
                    prompt=full_prompt,
                    audio_path=temp_audio_file,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
            elif image_data:
                # å›¾åƒæ¨¡å¼: ä½¿ç”¨ llama-server (ä»…æ”¯æŒè§†è§‰)
                print(f"[generateContent] å›¾åƒæ¨¡å¼: {len(image_data)} ä¸ªå›¾åƒæ–‡ä»¶")
                result = query_llama_server(
                    prompt=full_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    image_data=image_data
                )
            else:
                # çº¯æ–‡æœ¬æ¨¡å¼
                result = query_llama_server(
                    prompt=full_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    image_data=None
                )
        finally:
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            if temp_audio_file and Path(temp_audio_file).exists():
                try:
                    Path(temp_audio_file).unlink()
                    print(f"[generateContent] æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_audio_file}")
                except:
                    pass

        if "error" in result:
            return jsonify({
                "error": {
                    "message": result["error"],
                    "code": "500"
                }
            }), 500

        response_text = result["response"]
        prompt_tokens = result.get("prompt_tokens", 0)
        completion_tokens = result.get("completion_tokens", 0)

        # è§£ææ€è€ƒå†…å®¹ (å¦‚æœå¯ç”¨äº†æ€è€ƒæ¨¡å¼)
        thinking_text = ""
        thoughts_tokens = 0
        use_thinking = bool(thinking_prompt)  # æ‰€æœ‰æ¨¡å¼éƒ½æ”¯æŒæ€è€ƒ

        if use_thinking:
            thinking_text, answer_text, thoughts_tokens = parse_thinking_response(response_text)
            if thinking_text:
                response_text = answer_text
                print(f"[generateContent] Thinking: {len(thinking_text)} chars, Answer: {len(response_text)} chars")

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ (æ”¯æŒå¤šä¸ªå‡½æ•°è°ƒç”¨å’Œ codeExecution)
        function_call = None
        function_calls = None
        executable_code = None
        tool_use_tokens = 0

        if tools:
            tool_result = extract_tool_calls(response_text)
            if tool_result:
                if tool_result.get("function_calls"):
                    if len(tool_result["function_calls"]) == 1:
                        function_call = tool_result["function_calls"][0]
                    else:
                        function_calls = tool_result["function_calls"]
                    print(f"[generateContent] Function calls: {len(tool_result['function_calls'])}")

                if tool_result.get("executable_code"):
                    executable_code = tool_result["executable_code"]
                    # codeExecution çš„ token ä¼°ç®—
                    tool_use_tokens = len(str(executable_code)) // 4
                    print(f"[generateContent] Code execution detected")

                # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œåªä¿ç•™å·¥å…·è°ƒç”¨
                response_text = ""

        # å¼ºåˆ¶ JSON schema (å¦‚æœéœ€è¦)
        if response_schema and not function_call and not function_calls and not executable_code:
            response_text = enforce_json_schema(response_text, response_schema)

        # å¦‚æœæ²¡æœ‰è§£æå‡ºæ€è€ƒå†…å®¹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå€¼
        if not use_thinking or not thinking_text:
            if thinking_level == "high":
                thoughts_tokens = max(50, completion_tokens * 2)
            elif thinking_level == "medium":
                thoughts_tokens = max(20, completion_tokens)
            elif thinking_level == "low":
                thoughts_tokens = max(10, completion_tokens // 2)

        # ç¡®å®š finishReason
        # MAX_TOKENS: è¾¾åˆ° token é™åˆ¶ã€åªæœ‰æ€è€ƒæ²¡æœ‰ç­”æ¡ˆã€å“åº”è¢«æˆªæ–­
        finish_reason = "STOP"

        # è®¡ç®—æ€»è¾“å‡º token æ•° (åŒ…å« thinking)
        total_output_chars = len(response_text or "") + len(thinking_text or "")
        estimated_total_tokens = total_output_chars // 4

        import sys
        print(f"[finishReason debug] completion_tokens={completion_tokens}, max_tokens={max_tokens}, "
              f"response_text_len={len(response_text) if response_text else 0}, "
              f"thinking_text_len={len(thinking_text) if thinking_text else 0}, "
              f"estimated_total_tokens={estimated_total_tokens}, thinking_level={thinking_level}", flush=True)
        sys.stdout.flush()

        if not response_text and thinking_text:
            # åªæœ‰æ€è€ƒæ²¡æœ‰ç­”æ¡ˆï¼Œå¯èƒ½æ˜¯ token ä¸å¤Ÿ
            finish_reason = "MAX_TOKENS"
            print(f"[finishReason] MAX_TOKENS: no response, only thinking", flush=True)
        elif completion_tokens >= max_tokens - 10:
            # è¾“å‡º token æ¥è¿‘é™åˆ¶ï¼Œå¯èƒ½è¢«æˆªæ–­
            finish_reason = "MAX_TOKENS"
            print(f"[finishReason] MAX_TOKENS: tokens near limit", flush=True)
        elif thinking_level == "high" and thinking_text:
            # é«˜æ€è€ƒæ¨¡å¼ä¸‹ï¼Œåªè¦æœ‰æ€è€ƒå†…å®¹å°±è®¤ä¸ºå¯èƒ½è¾¾åˆ°é™åˆ¶
            # å› ä¸ºçœŸå® Gemini API åœ¨é«˜æ€è€ƒ+éŸ³é¢‘åœºæ™¯ä¸‹é€šå¸¸è¿”å› MAX_TOKENS
            finish_reason = "MAX_TOKENS"
            print(f"[finishReason] MAX_TOKENS: high thinking mode with thinking content", flush=True)

        # å°è¯•ä»å†å² thoughtSignature æ¢å¤ä¸Šä¸‹æ–‡ (ç”¨äºå¤šè½®å¯¹è¯)
        restored_context = restore_context_from_signature(contents)
        if restored_context:
            print(f"[generateContent] Restored context from thoughtSignature")

        # ç¡®å®šæ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
        # 1. includeThoughts=false æ—¶å®Œå…¨éšè—
        # 2. thinkingLevel=low/minimal/none æ—¶ä¹Ÿéšè—
        show_thinking = include_thoughts and thinking_level not in ("low", "minimal", "none")

        # è®¡ç®—éŸ³é¢‘ token æ•° (ä¼°ç®—: æ¯ç§’éŸ³é¢‘çº¦ 32-50 tokensï¼ŒæŒ‰ 325 tokens ä¼°ç®—çŸ­éŸ³é¢‘)
        has_audio_input = bool(audio_data)
        audio_token_count = 325 if has_audio_input else 0  # ä¸çœŸå® API ä¸€è‡´çš„ä¼°ç®—

        # æ„å»ºå“åº” (ä½¿ç”¨æ”¹è¿›çš„ thoughtSignature ç³»ç»Ÿ)
        response = build_gemini_response(
            response_text=response_text,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            model_version=MODEL_VERSION,  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹åç§° gemma-3n-local
            function_call=function_call,
            function_calls=function_calls,
            executable_code=executable_code,
            finish_reason=finish_reason,
            thoughts_tokens=thoughts_tokens,
            prompt_context=full_prompt,   # ä¿å­˜å®Œæ•´ prompt ç”¨äºæ¢å¤
            thinking_level=thinking_level,
            session_id=api_key or "",     # ä½¿ç”¨ API key ä½œä¸ºä¼šè¯æ ‡è¯†
            thinking_text=thinking_text,  # ä¼ é€’æ€è€ƒå†…å®¹
            show_thinking=show_thinking,  # æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            tool_use_tokens=tool_use_tokens,  # å·¥å…·ä½¿ç”¨çš„ token æ•°
            has_audio=has_audio_input,    # æ˜¯å¦åŒ…å«éŸ³é¢‘
            audio_tokens=audio_token_count,  # éŸ³é¢‘ token æ•°
        )

        # ä½¿ç”¨ Response + json.dumps ä¿æŒå­—æ®µé¡ºåº
        return Response(
            json.dumps(response, ensure_ascii=False),
            mimetype='application/json'
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": {
                "message": str(e),
                "code": "500"
            }
        }), 500


@app.route(f"/{API_VERSION}/models/<model_name>:streamGenerateContent", methods=["POST"])
def stream_generate_content(model_name: str):
    """
    Gemini streamGenerateContent API - æµå¼è¾“å‡º

    ä¸ generateContent ç›¸åŒçš„è¾“å…¥æ ¼å¼ï¼Œä½†è¿”å› Server-Sent Events (SSE) æµã€‚
    æ¯ä¸ª chunk æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON å“åº”ï¼Œé€æ­¥åŒ…å«æ›´å¤šæ–‡æœ¬ã€‚

    çœŸå® Gemini API æµå¼å“åº”æ ¼å¼:
    data: {"candidates":[{"content":{"parts":[{"text":"Hello"}],"role":"model"},...}],...}

    æ³¨æ„: alt=sse å‚æ•°ç”±å®¢æˆ·ç«¯æŒ‡å®š
    """
    try:
        api_key = request.args.get("key", "")
        alt = request.args.get("alt", "json")  # é»˜è®¤ jsonï¼Œå¯ä»¥æ˜¯ sse

        data = request.json
        if not data:
            return jsonify({"error": {"message": "è¯·æ±‚ä½“ä¸ºç©º", "code": "400"}}), 400

        contents = data.get("contents", [])
        system_instruction = data.get("systemInstruction")
        generation_config = data.get("generationConfig", {})
        tools = data.get("tools", [])
        tool_config = data.get("toolConfig", {})

        if not contents:
            return jsonify({"error": {"message": "contents ä¸èƒ½ä¸ºç©º", "code": "400"}}), 400

        # è§£æé…ç½® (ä¸ generateContent ç›¸åŒ)
        thinking_config = generation_config.get("thinkingConfig", {})
        thinking_level = thinking_config.get("thinkingLevel", DEFAULT_THINKING_LEVEL)
        max_tokens = generation_config.get("maxOutputTokens", DEFAULT_MAX_TOKENS)
        temperature = generation_config.get("temperature", DEFAULT_TEMPERATURE)

        # æ„å»º prompt
        prompt_parts = []

        if system_instruction:
            sys_text = parse_system_instruction(system_instruction)
            if sys_text:
                prompt_parts.append(f"<start_of_turn>user\nSystem: {sys_text}<end_of_turn>")

        if tools:
            tools_prompt = parse_tools(tools, tool_config)
            if tools_prompt:
                prompt_parts.append(f"<start_of_turn>user\n{tools_prompt}<end_of_turn>")

        thinking_prompt = THINKING_PROMPT_TEMPLATE.get(thinking_level, "")
        if thinking_prompt:
            prompt_parts.append(f"<start_of_turn>user\n{thinking_prompt}<end_of_turn>")

        conversation, media_data = parse_gemini_contents(contents)
        prompt_parts.append(conversation)

        full_prompt = "".join(prompt_parts)

        print(f"[streamGenerateContent] model={model_name}, thinking={thinking_level}")

        def generate_stream():
            """ç”Ÿæˆ SSE æµ"""
            global llama_server_ready

            if not llama_server_ready:
                if not start_llama_server():
                    yield f"data: {json.dumps({'error': {'message': 'llama-server æœªå°±ç»ª'}})}\n\n"
                    return

            try:
                import requests as req

                # ä½¿ç”¨ llama-server çš„æµå¼ API
                resp = req.post(
                    f"http://127.0.0.1:{LLAMA_SERVER_PORT}/completion",
                    json={
                        "prompt": full_prompt,
                        "n_predict": max_tokens,
                        "temperature": temperature,
                        "stop": ["</s>", "<eos>", "<end_of_turn>"],
                        "stream": True,
                    },
                    stream=True,
                    timeout=120
                )

                accumulated_text = ""
                prompt_tokens = 0
                completion_tokens = 0

                for line in resp.iter_lines():
                    if line:
                        line_str = line.decode('utf-8')
                        if line_str.startswith('data: '):
                            chunk_data = json.loads(line_str[6:])

                            if "content" in chunk_data:
                                new_text = chunk_data["content"]
                                accumulated_text += new_text
                                completion_tokens = len(accumulated_text) // 4

                                # æ„å»ºæµå¼å“åº” (ä¸çœŸå® API æ ¼å¼ä¸€è‡´)
                                stream_response = {
                                    "candidates": [{
                                        "content": {
                                            "parts": [{"text": accumulated_text}],
                                            "role": "model"
                                        },
                                        "finishReason": None,
                                        "index": 0,
                                        "safetyRatings": None
                                    }],
                                    "usageMetadata": {
                                        "promptTokenCount": prompt_tokens,
                                        "candidatesTokenCount": completion_tokens,
                                        "totalTokenCount": prompt_tokens + completion_tokens
                                    },
                                    "modelVersion": MODEL_VERSION
                                }

                                yield f"data: {json.dumps(stream_response, ensure_ascii=False)}\n\n"

                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                            if chunk_data.get("stop"):
                                # æœ€ç»ˆå“åº”
                                final_response = {
                                    "candidates": [{
                                        "content": {
                                            "parts": [{"text": accumulated_text}],
                                            "role": "model"
                                        },
                                        "finishReason": "STOP",
                                        "index": 0,
                                        "safetyRatings": None
                                    }],
                                    "usageMetadata": {
                                        "promptTokenCount": chunk_data.get("tokens_evaluated", 0),
                                        "candidatesTokenCount": chunk_data.get("tokens_predicted", completion_tokens),
                                        "totalTokenCount": chunk_data.get("tokens_evaluated", 0) + chunk_data.get("tokens_predicted", completion_tokens)
                                    },
                                    "modelVersion": MODEL_VERSION
                                }
                                yield f"data: {json.dumps(final_response, ensure_ascii=False)}\n\n"

            except Exception as e:
                yield f"data: {json.dumps({'error': {'message': str(e)}})}\n\n"

        # è¿”å› SSE æµ
        return Response(
            generate_stream(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": {
                "message": str(e),
                "code": "500"
            }
        }), 500


@app.route(f"/{API_VERSION}/models", methods=["GET"])
def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return jsonify({
        "models": [
            {
                "name": "models/gemini-3-pro-preview",
                "displayName": "Gemini 3 Pro Preview (Local)",
                "description": "Local Gemma 3n model emulating Gemini 3 Pro API",
                "inputTokenLimit": 1000000,
                "outputTokenLimit": 64000,
                "supportedGenerationMethods": ["generateContent"]
            },
            {
                "name": "models/gemini-3-flash-preview",
                "displayName": "Gemini 3 Flash Preview (Local)",
                "description": "Local Gemma 3n model emulating Gemini 3 Flash API",
                "inputTokenLimit": 1000000,
                "outputTokenLimit": 64000,
                "supportedGenerationMethods": ["generateContent"]
            }
        ]
    })


@app.route("/")
def index():
    """API é¦–é¡µ"""
    return jsonify({
        "name": "Gemini API Compatible Server",
        "version": "1.1.0",
        "description": "Local Gemma 3n model with Gemini API format",
        "endpoints": {
            "generateContent": f"/{API_VERSION}/models/{{model}}:generateContent",
            "streamGenerateContent": f"/{API_VERSION}/models/{{model}}:streamGenerateContent",
            "listModels": f"/{API_VERSION}/models",
            "health": "/health"
        },
        "supported_features": [
            "generateContent - æ–‡æœ¬ç”Ÿæˆ",
            "streamGenerateContent - æµå¼è¾“å‡º (SSE)",
            "thinkingConfig - æ€è€ƒç­‰çº§ (minimal/low/medium/high)",
            "systemInstruction - ç³»ç»ŸæŒ‡ä»¤/äººè®¾",
            "responseMimeType + responseSchema - JSON ç»“æ„åŒ–è¾“å‡º",
            "safetySettings - å®‰å…¨è®¾ç½® (å¿½ç•¥)",
            "multi-turn conversation - å¤šè½®å¯¹è¯",
            "thoughtSignature - æ€ç»´ç­¾å (KV Cache æŒä¹…åŒ–)",
            "functionDeclarations + functionCall - å·¥å…·è°ƒç”¨",
            "toolConfig (mode: AUTO/ANY/NONE) - å·¥å…·é…ç½®",
            "parallel function calling - å¹¶è¡Œå‡½æ•°è°ƒç”¨",
            "codeExecution - ä»£ç æ‰§è¡Œ (ä»…è¯†åˆ«æ ¼å¼)",
            "vision multimodal - è§†è§‰å¤šæ¨¡æ€ (å•/å¤šå›¾ç‰‡)",
            "audio multimodal - éŸ³é¢‘å¤šæ¨¡æ€ (å•éŸ³é¢‘)",
            "includeThoughts - æ§åˆ¶æ€è€ƒè¿‡ç¨‹æ˜¾ç¤º"
        ],
        "unsupported_features": [
            "googleSearch - éœ€è¦å¤–éƒ¨ API",
            "image generation - gemini-3-pro-image-preview",
            "multiple audio files - ä»…æ”¯æŒå•éŸ³é¢‘"
        ],
        "multimodal_status": {
            "vision_enabled": VISION_ENABLED,
            "audio_enabled": AUDIO_ENABLED
        }
    })


# ========== æµ‹è¯•å·¥å…· ==========

@app.route("/test/basic", methods=["GET"])
def test_basic():
    """åŸºç¡€æ–‡æœ¬æµ‹è¯•"""
    test_request = {
        "contents": [
            {
                "parts": [{"text": "hi"}]
            }
        ]
    }

    # æ¨¡æ‹Ÿè°ƒç”¨ - è¿”å›åŸå§‹ Response ä¿æŒå­—æ®µé¡ºåº
    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/thinking", methods=["GET"])
def test_thinking():
    """æ€è€ƒç­‰çº§æµ‹è¯•"""
    test_request = {
        "contents": [
            {
                "parts": [{"text": "What is 2 + 2?"}]
            }
        ],
        "generationConfig": {
            "thinkingConfig": {
                "thinkingLevel": "low"
            }
        }
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/system", methods=["GET"])
def test_system():
    """ç³»ç»ŸæŒ‡ä»¤æµ‹è¯•"""
    test_request = {
        "contents": [
            {
                "parts": [{"text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}]
            }
        ],
        "systemInstruction": {
            "parts": [
                {"text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡åŠ©æ‰‹ï¼Œåå«å°æ™ºã€‚å›ç­”æ—¶è¦ç®€æ´ã€ä¸“ä¸šï¼Œä½¿ç”¨ä¸­æ–‡ã€‚æ¯æ¬¡å›ç­”ä»¥ã€Œå°æ™ºï¼šã€å¼€å¤´ã€‚"}
            ]
        }
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/json-schema", methods=["GET"])
def test_json_schema():
    """ç»“æ„åŒ–è¾“å‡ºæµ‹è¯•"""
    test_request = {
        "contents": [
            {
                "parts": [{"text": "åˆ—å‡ºä¸‰ç§å¸¸è§çš„ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬åç§°ã€å‘å¸ƒå¹´ä»½å’Œä¸»è¦ç”¨é€”ã€‚"}]
            }
        ],
        "generationConfig": {
            "responseMimeType": "application/json",
            "responseSchema": {
                "type": "object",
                "properties": {
                    "languages": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string"},
                                "year": {"type": "integer"},
                                "use_case": {"type": "string"}
                            },
                            "required": ["name", "year", "use_case"]
                        }
                    }
                },
                "required": ["languages"]
            }
        }
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/function-call", methods=["GET"])
def test_function_call():
    """å·¥å…·è°ƒç”¨æµ‹è¯•"""
    test_request = {
        "contents": [
            {
                "role": "user",
                "parts": [{"text": "ä½ èƒ½ç”¨bashæ¥æ‰§è¡Œlså—ï¼Ÿ"}]
            }
        ],
        "tools": [
            {
                "functionDeclarations": [
                    {
                        "name": "shell_command",
                        "description": "Runs a shell command and returns its output.",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "command": {
                                    "type": "string",
                                    "description": "The shell script to execute"
                                },
                                "workdir": {
                                    "type": "string",
                                    "description": "The working directory"
                                }
                            },
                            "required": ["command"]
                        }
                    }
                ]
            }
        ]
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/multi-turn", methods=["GET"])
def test_multi_turn():
    """å¤šè½®å¯¹è¯æµ‹è¯•"""
    # è¿™é‡Œéœ€è¦ä½¿ç”¨ä¹‹å‰è·å–çš„ thoughtSignature
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥æ„å»ºè¯·æ±‚
    test_request = {
        "contents": [
            {
                "role": "user",
                "parts": [{"text": "æˆ‘æƒ³å­¦ä¹  Python ç¼–ç¨‹ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ"}]
            },
            {
                "role": "model",
                "parts": [
                    {"text": "Python æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼ä½ å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å¼€å§‹ï¼š1. å®‰è£… Python ç¯å¢ƒ 2. å­¦ä¹ åŸºç¡€è¯­æ³• 3. åšä¸€äº›å°é¡¹ç›®"},
                    {"thoughtSignature": "test-signature-placeholder"}
                ]
            },
            {
                "role": "user",
                "parts": [{"text": "æœ‰ä»€ä¹ˆæ¨èçš„åœ¨çº¿å­¦ä¹ èµ„æºå—ï¼Ÿ"}]
            }
        ]
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/tool-config-any", methods=["GET"])
def test_tool_config_any():
    """toolConfig mode:ANY æµ‹è¯• - å¼ºåˆ¶å‡½æ•°è°ƒç”¨"""
    test_request = {
        "contents": [{"role": "user", "parts": [{"text": "What is the weather?"}]}],
        "tools": [{
            "functionDeclarations": [{
                "name": "get_weather",
                "description": "Get weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {"location": {"type": "string"}},
                    "required": ["location"]
                }
            }]
        }],
        "toolConfig": {
            "functionCallingConfig": {
                "mode": "ANY"
            }
        },
        "generationConfig": {"maxOutputTokens": 256}
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/tool-config-none", methods=["GET"])
def test_tool_config_none():
    """toolConfig mode:NONE æµ‹è¯• - ç¦ç”¨å‡½æ•°è°ƒç”¨"""
    test_request = {
        "contents": [{"role": "user", "parts": [{"text": "What is the weather in Tokyo?"}]}],
        "tools": [{
            "functionDeclarations": [{
                "name": "get_weather",
                "description": "Get weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {"location": {"type": "string"}},
                    "required": ["location"]
                }
            }]
        }],
        "toolConfig": {
            "functionCallingConfig": {
                "mode": "NONE"
            }
        },
        "generationConfig": {"maxOutputTokens": 256}
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/parallel-function-call", methods=["GET"])
def test_parallel_function_call():
    """Parallel Function Calling æµ‹è¯• - å¤šä¸ªå‡½æ•°è°ƒç”¨"""
    test_request = {
        "contents": [{"role": "user", "parts": [{"text": "What is the weather in San Francisco and Tokyo?"}]}],
        "tools": [{
            "functionDeclarations": [{
                "name": "get_weather",
                "description": "Get weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {"location": {"type": "string"}},
                    "required": ["location"]
                }
            }]
        }],
        "generationConfig": {"maxOutputTokens": 512}
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/code-execution", methods=["GET"])
def test_code_execution():
    """codeExecution å·¥å…·æµ‹è¯•"""
    test_request = {
        "contents": [{"role": "user", "parts": [{"text": "Calculate 123 * 456 using Python code"}]}],
        "tools": [{"codeExecution": {}}],
        "generationConfig": {"maxOutputTokens": 1024}
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/include-thoughts-false", methods=["GET"])
def test_include_thoughts_false():
    """includeThoughts=false æµ‹è¯• - éšè—æ€è€ƒè¿‡ç¨‹"""
    test_request = {
        "contents": [{"role": "user", "parts": [{"text": "What is 2+2?"}]}],
        "generationConfig": {
            "thinkingConfig": {
                "thinkingLevel": "high",
                "includeThoughts": False  # æ˜¾å¼éšè—æ€è€ƒ
            },
            "maxOutputTokens": 256
        }
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/vision", methods=["GET"])
def test_vision():
    """å¤šæ¨¡æ€è§†è§‰æµ‹è¯• - ä½¿ç”¨ URL å›¾åƒ"""
    # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ (CIFAR-10 æ ·å¼)
    test_image_url = "https://huggingface.co/ggml-org/tinygemma3-GGUF/resolve/main/test/11_truck.png"

    # ä¸‹è½½å›¾åƒå¹¶è½¬æ¢ä¸º base64
    try:
        import requests as req
        resp = req.get(test_image_url, timeout=30)
        if resp.status_code == 200:
            image_base64 = base64.b64encode(resp.content).decode()
        else:
            return jsonify({"error": "æ— æ³•ä¸‹è½½æµ‹è¯•å›¾åƒ"}), 500
    except Exception as e:
        return jsonify({"error": f"ä¸‹è½½å›¾åƒå¤±è´¥: {e}"}), 500

    test_request = {
        "contents": [{
            "role": "user",
            "parts": [
                {"text": "What is in this image? Describe it briefly."},
                {
                    "inlineData": {
                        "mimeType": "image/png",
                        "data": image_base64
                    }
                }
            ]
        }],
        "generationConfig": {
            "maxOutputTokens": 256,
            "thinkingConfig": {
                "thinkingLevel": "low"
            }
        }
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/vision-base64", methods=["POST"])
def test_vision_base64():
    """
    å¤šæ¨¡æ€è§†è§‰æµ‹è¯• - æ¥å— base64 å›¾åƒ

    è¯·æ±‚ä½“æ ¼å¼:
    {
        "image": "base64_encoded_image_data",
        "mimeType": "image/jpeg",  # å¯é€‰ï¼Œé»˜è®¤ image/jpeg
        "prompt": "What is in this image?"  # å¯é€‰
    }
    """
    data = request.json or {}
    image_data = data.get("image", "")
    mime_type = data.get("mimeType", "image/jpeg")
    prompt = data.get("prompt", "What is in this image? Describe it briefly.")

    if not image_data:
        return jsonify({"error": "ç¼ºå°‘ image å­—æ®µ"}), 400

    test_request = {
        "contents": [{
            "role": "user",
            "parts": [
                {"text": prompt},
                {
                    "inlineData": {
                        "mimeType": mime_type,
                        "data": image_data
                    }
                }
            ]
        }],
        "generationConfig": {
            "maxOutputTokens": 512,
            "thinkingConfig": {
                "thinkingLevel": "medium"
            }
        }
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/audio", methods=["POST"])
def test_audio():
    """
    å¤šæ¨¡æ€éŸ³é¢‘æµ‹è¯• - æ¥å— base64 éŸ³é¢‘

    è¯·æ±‚ä½“æ ¼å¼:
    {
        "audio": "base64_encoded_audio_data",
        "mimeType": "audio/wav",  # å¯é€‰ï¼Œé»˜è®¤ audio/wav
        "prompt": "Transcribe this audio."  # å¯é€‰
    }

    æ”¯æŒçš„éŸ³é¢‘æ ¼å¼:
    - audio/wav, audio/wave, audio/x-wav
    - audio/mp3, audio/mpeg
    - audio/ogg, audio/vorbis
    - audio/flac, audio/x-flac
    - audio/aac, audio/mp4
    - audio/webm
    """
    if not AUDIO_ENABLED:
        return jsonify({
            "error": "éŸ³é¢‘åŠŸèƒ½æœªå¯ç”¨ã€‚è¯·ç¡®ä¿éŸ³é¢‘ mmproj æ–‡ä»¶å­˜åœ¨ã€‚",
            "audio_enabled": False,
            "required_file": "gemma-3n-audio-mmproj-f16.gguf"
        }), 400

    data = request.json or {}
    audio_data = data.get("audio", "")
    mime_type = data.get("mimeType", "audio/wav")
    prompt = data.get("prompt", "Transcribe this audio and describe what you hear.")

    if not audio_data:
        return jsonify({"error": "ç¼ºå°‘ audio å­—æ®µ"}), 400

    if mime_type not in SUPPORTED_AUDIO_TYPES:
        return jsonify({
            "error": f"ä¸æ”¯æŒçš„éŸ³é¢‘ç±»å‹: {mime_type}",
            "supported_types": list(SUPPORTED_AUDIO_TYPES)
        }), 400

    test_request = {
        "contents": [{
            "role": "user",
            "parts": [
                {"text": prompt},
                {
                    "inlineData": {
                        "mimeType": mime_type,
                        "data": audio_data
                    }
                }
            ]
        }],
        "generationConfig": {
            "maxOutputTokens": 1024,
            "thinkingConfig": {
                "thinkingLevel": "medium"
            }
        }
    }

    with app.test_client() as client:
        resp = client.post(
            f"/{API_VERSION}/models/gemini-3-pro-preview:generateContent",
            json=test_request,
            content_type="application/json"
        )
        return Response(resp.data, mimetype='application/json')


@app.route("/test/audio-status", methods=["GET"])
def test_audio_status():
    """æ£€æŸ¥éŸ³é¢‘åŠŸèƒ½çŠ¶æ€"""
    return jsonify({
        "audio_enabled": AUDIO_ENABLED,
        "vision_enabled": VISION_ENABLED,
        "multimodal_enabled": MULTIMODAL_ENABLED,
        "audio_mmproj": str(LLAMA_MMPROJ_AUDIO) if LLAMA_MMPROJ_AUDIO else None,
        "vision_mmproj": str(LLAMA_MMPROJ_VISION) if LLAMA_MMPROJ_VISION else None,
        "combined_mmproj": str(LLAMA_MMPROJ) if LLAMA_MMPROJ else None,
        "supported_audio_types": list(SUPPORTED_AUDIO_TYPES),
        "supported_image_types": list(SUPPORTED_IMAGE_TYPES)
    })

if __name__ == "__main__":
    print("=" * 60)
    print("Gemini API å…¼å®¹æœåŠ¡å™¨")
    print("=" * 60)
    print(f"æ¨¡å‹: {LLAMA_MODEL}")
    if MULTIMODAL_ENABLED:
        capabilities = []
        if VISION_ENABLED:
            capabilities.append("è§†è§‰")
        if AUDIO_ENABLED:
            capabilities.append("éŸ³é¢‘")
        print(f"å¤šæ¨¡æ€: {'+'.join(capabilities)}")
        if VISION_ENABLED:
            print(f"  - Vision: {Path(LLAMA_MMPROJ_VISION).name}")
        if AUDIO_ENABLED:
            print(f"  - Audio: {Path(LLAMA_MMPROJ_AUDIO).name}")
            if LLAMA_MODEL_AUDIO and LLAMA_MODEL_AUDIO != LLAMA_MODEL:
                print(f"  - Audio Model: {Path(LLAMA_MODEL_AUDIO).name}")
    else:
        print("å¤šæ¨¡æ€: æœªå¯ç”¨")
    print(f"llama-server ç«¯å£: {LLAMA_SERVER_PORT}")
    print()

    # é¢„å¯åŠ¨ llama-server
    start_llama_server()

    print()
    print("API ç«¯ç‚¹:")
    print(f"  POST /{API_VERSION}/models/{{model}}:generateContent")
    print(f"  GET  /{API_VERSION}/models")
    print(f"  GET  /health")
    print()
    print("æµ‹è¯•ç«¯ç‚¹:")
    print("  GET  /test/basic")
    print("  GET  /test/thinking")
    print("  GET  /test/system")
    print("  GET  /test/json-schema")
    print("  GET  /test/function-call")
    print("  GET  /test/multi-turn")
    print("  GET  /test/tool-config-any")
    print("  GET  /test/tool-config-none")
    print("  GET  /test/parallel-function-call")
    print("  GET  /test/code-execution")
    print("  GET  /test/include-thoughts-false")
    if VISION_ENABLED:
        print("  GET  /test/vision")
        print("  POST /test/vision-base64")
    if AUDIO_ENABLED:
        print("  POST /test/audio")
        print("  GET  /test/audio-status")
    print()
    print("å¯åŠ¨æœåŠ¡å™¨: http://localhost:5001")
    print("=" * 60)

    app.run(host="0.0.0.0", port=5001, debug=False, threaded=True)
