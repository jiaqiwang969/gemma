"""
ç¤ºä¾‹2c: å¤šæ¨¡æ€è”åˆåˆ†æ - åŒæ—¶å¤„ç† æ–‡å­— + å›¾ç‰‡ + éŸ³é¢‘

Gemma 3n æ”¯æŒåœ¨å•æ¬¡è¯·æ±‚ä¸­åŒæ—¶å¤„ç†ï¼š
  - æ–‡æœ¬ (Text)
  - å›¾åƒ (Image) - å¯å¤šå¼ 
  - éŸ³é¢‘ (Audio)

è¿™ä½¿å¾—æ¨¡å‹å¯ä»¥ï¼š
  - æ ¹æ®è¯­éŸ³æŒ‡ä»¤åˆ†æå›¾ç‰‡
  - å°†å›¾ç‰‡å†…å®¹ä¸éŸ³é¢‘å†…å®¹å…³è”
  - å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç†è§£
"""
import os
from pathlib import Path
import torch
from transformers import AutoProcessor, Gemma3nForConditionalGeneration
from PIL import Image
import librosa
import time
import warnings
warnings.filterwarnings("ignore")

os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

# Gemma 3n ä¸Šä¸‹æ–‡é™åˆ¶
MAX_CONTEXT_TOKENS = 32768  # 32K tokens

print("=" * 60)
print("ç¤ºä¾‹2c: Gemma 3n å¤šæ¨¡æ€è”åˆåˆ†æ")
print("=" * 60)
print("æ”¯æŒ: æ–‡å­— + å›¾ç‰‡ + éŸ³é¢‘ åŒæ—¶è¾“å…¥")
print("=" * 60)

model_name = "google/gemma-3n-E2B-it"

print("\n[1] åŠ è½½æ¨¡å‹...")
processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
model = Gemma3nForConditionalGeneration.from_pretrained(
    model_name,
    device_map="auto",
    max_memory={"mps": "64GiB", "cpu": "64GiB"},
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
)
model.eval()
print("    æ¨¡å‹åŠ è½½å®Œæˆ!")

# æ•°æ®è·¯å¾„
ROOT_DIR = Path(__file__).resolve().parents[2]
IMAGE_DIR = ROOT_DIR / "assets" / "data" / "images"
AUDIO_DIR = ROOT_DIR / "assets" / "data" / "audio"


def load_image(filename):
    """åŠ è½½å›¾ç‰‡"""
    path = IMAGE_DIR / filename
    if path.exists():
        img = Image.open(path).convert("RGB")
        print(f"    å›¾ç‰‡: {filename} ({img.size[0]}x{img.size[1]})")
        return img
    else:
        print(f"    å›¾ç‰‡ä¸å­˜åœ¨: {path}")
        return None


def load_audio(filename, target_sr=16000):
    """åŠ è½½éŸ³é¢‘"""
    path = AUDIO_DIR / filename
    if path.exists():
        audio_array, sr = librosa.load(path, sr=target_sr)
        duration = len(audio_array) / target_sr
        print(f"    éŸ³é¢‘: {filename} ({duration:.2f}s, {target_sr}Hz)")
        return audio_array, target_sr
    else:
        print(f"    éŸ³é¢‘ä¸å­˜åœ¨: {path}")
        return None, None


def multimodal_analysis(text, images=None, audio=None):
    """
    å¤šæ¨¡æ€è”åˆåˆ†æ

    Args:
        text: æ–‡æœ¬æç¤º/é—®é¢˜
        images: [(name, PIL.Image), ...] å›¾ç‰‡åˆ—è¡¨
        audio: (audio_array, sample_rate) éŸ³é¢‘æ•°æ®
    """
    print(f"\n{'='*60}")
    print("å¤šæ¨¡æ€è”åˆåˆ†æ")
    print(f"{'='*60}")

    # ç»Ÿè®¡è¾“å…¥
    has_text = bool(text)
    has_images = images and len(images) > 0
    has_audio = audio and audio[0] is not None

    modalities = []
    if has_text:
        modalities.append("æ–‡å­—")
    if has_images:
        modalities.append(f"å›¾ç‰‡x{len(images)}")
    if has_audio:
        modalities.append("éŸ³é¢‘")

    print(f"è¾“å…¥æ¨¡æ€: {' + '.join(modalities)}")
    print(f"æ–‡æœ¬: {text[:100]}..." if len(text) > 100 else f"æ–‡æœ¬: {text}")

    # æ„å»ºæ¶ˆæ¯å†…å®¹
    content = []

    # æ·»åŠ å›¾ç‰‡
    if has_images:
        for name, img in images:
            content.append({"type": "image", "image": img})

    # æ·»åŠ éŸ³é¢‘
    if has_audio:
        audio_array, sr = audio
        content.append({"type": "audio", "audio": audio_array, "sample_rate": sr})

    # æ·»åŠ æ–‡æœ¬
    content.append({"type": "text", "text": text})

    messages = [{"role": "user", "content": content}]

    start_time = time.time()

    # å¤„ç†è¾“å…¥
    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    )

    input_ids = inputs["input_ids"].to(model.device)
    attention_mask = inputs["attention_mask"].to(model.device)

    input_tokens = input_ids.shape[1]
    remaining_tokens = MAX_CONTEXT_TOKENS - input_tokens

    print(f"\nğŸ“Š Token ç»Ÿè®¡:")
    print(f"    è¾“å…¥ tokens: {input_tokens:,} / {MAX_CONTEXT_TOKENS:,} ({input_tokens/MAX_CONTEXT_TOKENS*100:.1f}%)")
    print(f"    å‰©ä½™ç©ºé—´: {remaining_tokens:,} tokens")

    # å‡†å¤‡ç”Ÿæˆå‚æ•°
    generate_kwargs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "max_new_tokens": min(500, remaining_tokens - 100),
        "do_sample": False,
    }

    # æ·»åŠ å›¾ç‰‡æ•°æ®
    if "pixel_values" in inputs and inputs["pixel_values"] is not None:
        generate_kwargs["pixel_values"] = inputs["pixel_values"].to(model.device, dtype=model.dtype)
        print(f"    å›¾ç‰‡å¼ é‡: {generate_kwargs['pixel_values'].shape}")

    # æ·»åŠ éŸ³é¢‘æ•°æ®
    if "input_features" in inputs and inputs["input_features"] is not None:
        generate_kwargs["input_features"] = inputs["input_features"].to(model.device, dtype=model.dtype)
        generate_kwargs["input_features_mask"] = inputs["input_features_mask"].to(model.device)
        print(f"    éŸ³é¢‘å¼ é‡: {generate_kwargs['input_features'].shape}")

    print("\nç”Ÿæˆå›å¤ä¸­...")

    with torch.inference_mode():
        outputs = model.generate(**generate_kwargs)

    elapsed = time.time() - start_time
    output_tokens = len(outputs[0]) - input_tokens
    total_tokens = input_tokens + output_tokens
    response = processor.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)

    print(f"\nå›ç­”:\n{'-'*40}")
    print(response)
    print(f"{'-'*40}")
    print(f"è¾“å‡º tokens: {output_tokens:,}")
    print(f"æ€»è®¡ tokens: {total_tokens:,} / {MAX_CONTEXT_TOKENS:,} ({total_tokens/MAX_CONTEXT_TOKENS*100:.1f}%)")
    print(f"è€—æ—¶: {elapsed:.2f}s")

    return response


# ============================================================
# ä¸»ç¨‹åº
# ============================================================
if __name__ == "__main__":
    print("\n[2] åŠ è½½æµ‹è¯•æ•°æ®...")

    # åŠ è½½å›¾ç‰‡
    cat_img = load_image("cat.jpg")
    dog_img = load_image("dog.jpg")
    food_img = load_image("food.jpg")

    # åŠ è½½éŸ³é¢‘
    audio_data = load_audio("mlk_speech.flac")

    # ============================================================
    # æ¼”ç¤º1: å›¾ç‰‡ + æ–‡å­—
    # ============================================================
    print("\n" + "=" * 60)
    print("æ¼”ç¤º1: å›¾ç‰‡ + æ–‡å­—")
    print("=" * 60)

    if cat_img:
        multimodal_analysis(
            text="Describe this image in detail. What animal is it? What is it doing?",
            images=[("cat.jpg", cat_img)]
        )

    # ============================================================
    # æ¼”ç¤º2: éŸ³é¢‘ + æ–‡å­—
    # ============================================================
    print("\n" + "=" * 60)
    print("æ¼”ç¤º2: éŸ³é¢‘ + æ–‡å­—")
    print("=" * 60)

    if audio_data[0] is not None:
        multimodal_analysis(
            text="Please transcribe this audio and summarize its main message.",
            audio=audio_data
        )

    # ============================================================
    # æ¼”ç¤º3: å›¾ç‰‡ + éŸ³é¢‘ + æ–‡å­— (å®Œæ•´å¤šæ¨¡æ€)
    # ============================================================
    print("\n" + "=" * 60)
    print("æ¼”ç¤º3: å›¾ç‰‡ + éŸ³é¢‘ + æ–‡å­— (å®Œæ•´å¤šæ¨¡æ€)")
    print("=" * 60)

    if cat_img and audio_data[0] is not None:
        multimodal_analysis(
            text="I'm showing you an image and playing an audio clip. "
                 "First, describe what you see in the image. "
                 "Then, transcribe what you hear in the audio. "
                 "Finally, tell me if there's any connection between them.",
            images=[("cat.jpg", cat_img)],
            audio=audio_data
        )

    # ============================================================
    # æ¼”ç¤º4: å¤šå¼ å›¾ç‰‡ + éŸ³é¢‘ + æ–‡å­—
    # ============================================================
    print("\n" + "=" * 60)
    print("æ¼”ç¤º4: å¤šå¼ å›¾ç‰‡ + éŸ³é¢‘ + æ–‡å­—")
    print("=" * 60)

    images_list = []
    if cat_img:
        images_list.append(("cat.jpg", cat_img))
    if dog_img:
        images_list.append(("dog.jpg", dog_img))

    if len(images_list) >= 2 and audio_data[0] is not None:
        multimodal_analysis(
            text="I'm showing you two images (a cat and a dog) and playing an audio clip. "
                 "Compare the two animals in the images. "
                 "Also transcribe the audio and tell me the topic.",
            images=images_list,
            audio=audio_data
        )

    # ============================================================
    # æ¼”ç¤º5: çº¯æ–‡å­— (åŸºçº¿å¯¹æ¯”)
    # ============================================================
    print("\n" + "=" * 60)
    print("æ¼”ç¤º5: çº¯æ–‡å­— (åŸºçº¿å¯¹æ¯”)")
    print("=" * 60)

    # çº¯æ–‡å­—éœ€è¦ dummy image (Gemma 3n çš„é™åˆ¶)
    from PIL import Image as PILImage
    dummy = PILImage.new('RGB', (64, 64), color='white')

    multimodal_analysis(
        text="Ignore the blank image. What are the key differences between cats and dogs as pets?",
        images=[("dummy", dummy)]
    )

    print("\n" + "=" * 60)
    print("æ‰€æœ‰å¤šæ¨¡æ€æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
