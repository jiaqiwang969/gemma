"""
AI å¤šæ¨¡æ€èŠå¤©æœåŠ¡å™¨
æ”¯æŒ: æ–‡æœ¬ + å›¾ç‰‡ + éŸ³é¢‘ + å¤šè½®å¯¹è¯å†å²
å­˜å‚¨: ~/.gemma3n/ (å‚è€ƒ Codex æ¶æ„)
"""
import os
import io
import base64
import torch
import numpy as np
import uuid
import json
from pathlib import Path
from datetime import datetime
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from transformers import AutoProcessor, Gemma3nForConditionalGeneration
from PIL import Image
import librosa
import warnings
import time
import psutil
import subprocess
import platform
import threading

warnings.filterwarnings("ignore")
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

app = Flask(__name__, static_folder="static")
CORS(app)

# å…¨å±€å˜é‡
model = None
processor = None
model_loaded = False
model_info = {}
dummy_image = None
DEFAULT_BACKEND = os.environ.get("GEMMA3N_BACKEND", "mps")  # mps | mmproj

# llama.cpp è·¯å¾„/æ¨¡å‹ï¼ˆmmproj æ¨¡å¼ä½¿ç”¨ï¼‰
REPO_ROOT = Path(__file__).resolve().parents[2]
LLAMA_MTMD_BIN = os.environ.get("LLAMA_MTMD_BIN", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-mtmd-cli"))
LLAMA_MM_MODEL = os.environ.get("LLAMA_MM_MODEL", str(REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-Q4_K_M.gguf"))
LLAMA_MM_PROJ_IMAGE = os.environ.get("LLAMA_MM_PROJ_IMAGE", str(REPO_ROOT / "artifacts/gguf/gemma-3n-vision-mmproj-f16.gguf"))
LLAMA_MM_PROJ_AUDIO = os.environ.get("LLAMA_MM_PROJ_AUDIO", str(REPO_ROOT / "artifacts/gguf/gemma-3n-audio-mmproj-f16.gguf"))
LLAMA_MM_PROJ = os.environ.get("LLAMA_MM_PROJ", "")
LLAMA_MM_PROJ_COMBINED = ",".join([p for p in [LLAMA_MM_PROJ_IMAGE, LLAMA_MM_PROJ_AUDIO] if p]) if (LLAMA_MM_PROJ_IMAGE or LLAMA_MM_PROJ_AUDIO) else LLAMA_MM_PROJ
LLAMA_MM_N_PREDICT = int(os.environ.get("LLAMA_MM_N_PREDICT", "128"))
LLAMA_MM_DEVICE = os.environ.get("LLAMA_MM_DEVICE", "none")
LLAMA_MM_N_GPU_LAYERS = os.environ.get("LLAMA_MM_N_GPU_LAYERS", "0")

# llama-run è·¯å¾„/æ¨¡å‹ï¼ˆllama.cpp çº¯æ–‡æœ¬æ¨¡å¼ï¼‰
LLAMA_RUN_BIN = os.environ.get("LLAMA_RUN_BIN", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-run"))
LLAMA_SERVER_BIN = os.environ.get("LLAMA_SERVER_BIN", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-server"))
LLAMA_SERVER_PORT = int(os.environ.get("LLAMA_SERVER_PORT", "8081"))
LLAMA_RUN_MODEL = os.environ.get("LLAMA_RUN_MODEL", "")
# è‡ªåŠ¨æŸ¥æ‰¾å¯ç”¨çš„ GGUF æ¨¡å‹
if not LLAMA_RUN_MODEL:
    for candidate in [
        REPO_ROOT / "artifacts/gguf/gemma-3n-finetuned-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-fp16.gguf",
    ]:
        if candidate.exists():
            LLAMA_RUN_MODEL = str(candidate)
            break

# llama-server è¿›ç¨‹ç®¡ç† (çº¯æ–‡æœ¬æ¨¡å¼)
llama_server_process = None
llama_server_ready = False

# llama-server mmproj æ¨¡å¼ (å¤šæ¨¡æ€)
LLAMA_MMPROJ_SERVER_PORT = int(os.environ.get("LLAMA_MMPROJ_SERVER_PORT", "8082"))
llama_mmproj_server_process = None
llama_mmproj_server_ready = False

# å­˜å‚¨è·¯å¾„ (~/.gemma3n/)
GEMMA3N_HOME = Path.home() / ".gemma3n"
SESSIONS_DIR = GEMMA3N_HOME / "sessions"
HISTORY_FILE = GEMMA3N_HOME / "history.jsonl"

# ä¼šè¯ç®¡ç† (å†…å­˜ç¼“å­˜)
sessions = {}  # session_id -> {"messages": [...], "created_at": timestamp, "file_path": ...}
MAX_SESSIONS = 100
MAX_HISTORY_TURNS = 10  # ä¿ç•™æœ€è¿‘Nè½®å¯¹è¯

# sudo æƒé™çŠ¶æ€
sudo_authorized = False
sudo_refresh_thread = None

# ========== Thought Signature ç³»ç»Ÿ (å‹ç¼©è®°å¿†) ==========
# æ ¸å¿ƒæ€æƒ³: å›¾ç‰‡/éŸ³é¢‘ â†’ æ¨¡å‹ç†è§£ â†’ å­˜å…¥ signature â†’ åç»­è½®æ¬¡æ¢å¤
import hashlib
import hmac

THOUGHT_SIGNATURE_SECRET = "gemma3n-thought-signature-key"
media_understanding_cache = {}  # media_ref -> {"understanding": "...", "session_id": "..."}
thought_states = {}  # session_id -> {"turn_index": 0, "media_refs": [...]}


def generate_media_signature(session_id: str, turn_index: int, understanding: str) -> str:
    """
    ä¸ºåª’ä½“ç†è§£ç”Ÿæˆç­¾åå¼•ç”¨

    Returns: media_ref (ç”¨äºåç»­æ¢å¤)
    """
    import base64
    timestamp = int(time.time())
    media_ref = hashlib.md5(f"{session_id}:{turn_index}:{timestamp}".encode()).hexdigest()[:12]

    # å­˜å‚¨ç†è§£åˆ°ç¼“å­˜
    media_understanding_cache[media_ref] = {
        "session_id": session_id,
        "turn_index": turn_index,
        "understanding": understanding,
        "created_at": timestamp
    }

    # æ›´æ–°ä¼šè¯çš„æ€ç»´çŠ¶æ€
    if session_id not in thought_states:
        thought_states[session_id] = {"turn_index": 0, "media_refs": []}
    thought_states[session_id]["media_refs"].append(media_ref)
    thought_states[session_id]["turn_index"] = turn_index

    return media_ref


def get_session_media_context(session_id: str) -> str:
    """
    è·å–ä¼šè¯ä¸­æ‰€æœ‰åª’ä½“ç†è§£çš„ä¸Šä¸‹æ–‡

    è¿™æ˜¯ thought signature ä½œä¸º"å‹ç¼©è®°å¿†"çš„æ ¸å¿ƒï¼š
    - ç¬¬1è½®: ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ â†’ æ¨¡å‹ç”Ÿæˆç†è§£ â†’ å­˜å…¥ signature
    - ç¬¬2è½®: ä» signature æ¢å¤ç†è§£ â†’ æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­
    """
    context_parts = []

    state = thought_states.get(session_id, {})
    media_refs = state.get("media_refs", [])

    for media_ref in media_refs:
        cached = media_understanding_cache.get(media_ref)
        if cached:
            turn = cached.get("turn_index", 0)
            understanding = cached.get("understanding", "")
            if understanding:
                context_parts.append(f"[Turn {turn} - Media Understanding]: {understanding}")

    return "\n".join(context_parts) if context_parts else ""

stats = {
    "total_requests": 0,
    "total_tokens": 0,
    "total_time": 0,
    "avg_speed": 0
}

def init_storage():
    """åˆå§‹åŒ–å­˜å‚¨ç›®å½•ç»“æ„"""
    GEMMA3N_HOME.mkdir(exist_ok=True)
    SESSIONS_DIR.mkdir(exist_ok=True)
    if not HISTORY_FILE.exists():
        HISTORY_FILE.touch()
    print(f"[Storage] åˆå§‹åŒ–å®Œæˆ: {GEMMA3N_HOME}")

def get_session_dir():
    """è·å–å½“å¤©çš„ä¼šè¯ç›®å½• sessions/YYYY/MM/DD/"""
    now = datetime.now()
    day_dir = SESSIONS_DIR / str(now.year) / f"{now.month:02d}" / f"{now.day:02d}"
    day_dir.mkdir(parents=True, exist_ok=True)
    return day_dir

def save_session_to_disk(session_id, session_data):
    """å°†ä¼šè¯ä¿å­˜åˆ° JSONL æ–‡ä»¶"""
    if "file_path" not in session_data:
        # åˆ›å»ºæ–°æ–‡ä»¶
        now = datetime.now()
        timestamp = now.strftime("%H%M%S")
        filename = f"session-{timestamp}-{session_id}.jsonl"
        session_data["file_path"] = str(get_session_dir() / filename)

    file_path = Path(session_data["file_path"])

    # å†™å…¥ä¼šè¯æ•°æ® (å®Œæ•´è¦†ç›–)
    with open(file_path, "w", encoding="utf-8") as f:
        # ç¬¬ä¸€è¡Œ: ä¼šè¯å…ƒæ•°æ®
        meta = {
            "type": "meta",
            "session_id": session_id,
            "created_at": session_data["created_at"],
            "title": session_data.get("title", "æ–°å¯¹è¯"),
            "updated_at": time.time()
        }
        f.write(json.dumps(meta, ensure_ascii=False) + "\n")

        # åç»­è¡Œ: æ¶ˆæ¯
        for msg in session_data["messages"]:
            item = {"type": "message", **msg}
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

def load_session_from_disk(file_path):
    """ä» JSONL æ–‡ä»¶åŠ è½½ä¼šè¯"""
    file_path = Path(file_path)
    if not file_path.exists():
        return None

    session_data = {"messages": [], "file_path": str(file_path)}

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            item = json.loads(line)
            if item.get("type") == "meta":
                session_data["created_at"] = item.get("created_at", time.time())
                session_data["title"] = item.get("title", "æ–°å¯¹è¯")
                session_data["session_id"] = item.get("session_id")
            elif item.get("type") == "message":
                del item["type"]
                session_data["messages"].append(item)

    return session_data

def append_to_history(session_id, text):
    """è¿½åŠ åˆ°å…¨å±€å†å²è®°å½•"""
    entry = {
        "session_id": session_id,
        "ts": int(time.time()),
        "text": text[:100]  # åªä¿å­˜å‰100å­—ç¬¦ä½œä¸ºæ‘˜è¦
    }
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def list_all_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯æ–‡ä»¶"""
    session_list = []
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        try:
            with open(jsonl_file, "r", encoding="utf-8") as f:
                first_line = f.readline().strip()
                if first_line:
                    meta = json.loads(first_line)
                    if meta.get("type") == "meta":
                        session_list.append({
                            "session_id": meta.get("session_id"),
                            "title": meta.get("title", "æ–°å¯¹è¯"),
                            "created_at": meta.get("created_at", 0),
                            "updated_at": meta.get("updated_at", 0),
                            "file_path": str(jsonl_file)
                        })
        except Exception as e:
            print(f"[Warning] æ— æ³•è¯»å–ä¼šè¯æ–‡ä»¶ {jsonl_file}: {e}")

    # æŒ‰æ›´æ–°æ—¶é—´å€’åº
    session_list.sort(key=lambda x: x.get("updated_at", 0), reverse=True)
    return session_list[:50]  # æœ€å¤šè¿”å›50ä¸ª

def get_memory_usage():
    process = psutil.Process(os.getpid())
    mem_gb = process.memory_info().rss / 1024 / 1024 / 1024
    return round(mem_gb, 2)


def request_sudo_permission():
    """
    å¯åŠ¨æ—¶è¯·æ±‚ sudo æƒé™ï¼ˆç”¨æˆ·è¾“å…¥ä¸€æ¬¡å¯†ç ï¼‰
    å¹¶å¯åŠ¨åå°çº¿ç¨‹å®šæœŸåˆ·æ–°å‡­è¯
    """
    global sudo_authorized, sudo_refresh_thread

    if platform.system() != "Darwin":
        return False

    print("\n" + "=" * 60)
    print("ğŸ” è¯·æ±‚ç¡¬ä»¶ç›‘æ§æƒé™ (ç”¨äºè·å– GPU æ¸©åº¦)")
    print("   è¯·è¾“å…¥æ‚¨çš„ macOS å¯†ç ï¼ˆå¯é€‰ï¼ŒæŒ‰ Ctrl+C è·³è¿‡ï¼‰")
    print("=" * 60)

    try:
        # è¯·æ±‚ sudo æƒé™
        result = subprocess.run(
            ["sudo", "-v"],
            timeout=60  # ç»™ç”¨æˆ·60ç§’è¾“å…¥å¯†ç 
        )
        if result.returncode == 0:
            sudo_authorized = True
            print("âœ… æƒé™æˆæƒæˆåŠŸï¼GPU æ¸©åº¦ç›‘æ§å·²å¯ç”¨")

            # å¯åŠ¨åå°çº¿ç¨‹å®šæœŸåˆ·æ–° sudo å‡­è¯
            def refresh_sudo():
                while sudo_authorized:
                    time.sleep(240)  # æ¯4åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡ï¼ˆsudo é»˜è®¤5åˆ†é’Ÿè¶…æ—¶ï¼‰
                    try:
                        subprocess.run(["sudo", "-v"], capture_output=True, timeout=5)
                    except:
                        pass

            sudo_refresh_thread = threading.Thread(target=refresh_sudo, daemon=True)
            sudo_refresh_thread.start()
            return True
        else:
            print("âš ï¸  æƒé™æœªæˆæƒï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
            return False
    except subprocess.TimeoutExpired:
        print("âš ï¸  æˆæƒè¶…æ—¶ï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
        return False
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²è·³è¿‡æƒé™æˆæƒï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
        return False
    except Exception as e:
        print(f"âš ï¸  æˆæƒå¤±è´¥: {e}")
        return False


def get_hardware_stats():
    """è·å–ç¡¬ä»¶ç›‘æ§ä¿¡æ¯ (GPUä½¿ç”¨ç‡ã€æ˜¾å­˜ã€æ¸©åº¦ç­‰)"""
    hw_stats = {
        "gpu_usage": "-",
        "vram_usage": "-",
        "gpu_temp": "-",
        "memory_usage": "-",
        "cpu_usage": "-"
    }

    try:
        # CPU ä½¿ç”¨ç‡ (ä¸ä½¿ç”¨ interval é¿å…é˜»å¡)
        hw_stats["cpu_usage"] = f"{psutil.cpu_percent():.1f}%"

        # å†…å­˜ä½¿ç”¨
        mem = psutil.virtual_memory()
        hw_stats["memory_usage"] = f"{mem.used / 1024**3:.1f} GB / {mem.total / 1024**3:.1f} GB"

        system = platform.system()

        if system == "Darwin":  # macOS
            try:
                # æ£€æµ‹ MPS çŠ¶æ€
                if torch.backends.mps.is_available():
                    hw_stats["gpu_usage"] = "MPS è¿è¡Œä¸­"
                else:
                    hw_stats["gpu_usage"] = "CPU æ¨¡å¼"

                # macOS ç»Ÿä¸€å†…å­˜æ¶æ„ï¼Œæ˜¾å­˜å’Œå†…å­˜å…±äº«
                hw_stats["vram_usage"] = "ç»Ÿä¸€å†…å­˜"

                # å°è¯•é€šè¿‡ powermetrics è·å– GPU æ¸©åº¦ (éœ€è¦ sudo æƒé™)
                if sudo_authorized:
                    try:
                        result = subprocess.run(
                            ["sudo", "powermetrics", "--samplers", "smc", "-i", "1", "-n", "1"],
                            capture_output=True, text=True, timeout=3
                        )
                        if result.returncode == 0:
                            output = result.stdout
                            # æŸ¥æ‰¾ GPU æ¸©åº¦ç›¸å…³è¡Œ
                            for line in output.split("\n"):
                                if "GPU" in line and "die" in line.lower():
                                    parts = line.split(":")
                                    if len(parts) >= 2:
                                        temp_str = parts[1].strip().replace("C", "").strip()
                                        try:
                                            temp = float(temp_str)
                                            hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                            break
                                        except:
                                            pass
                                elif "GPU" in line and "temp" in line.lower():
                                    parts = line.split(":")
                                    if len(parts) >= 2:
                                        temp_str = parts[1].strip().replace("C", "").strip()
                                        try:
                                            temp = float(temp_str)
                                            hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                            break
                                        except:
                                            pass

                            # å¦‚æœæ²¡æ‰¾åˆ° GPU æ¸©åº¦ï¼Œå°è¯•æ‰¾ SOC æ¸©åº¦
                            if hw_stats["gpu_temp"] == "-":
                                for line in output.split("\n"):
                                    if "SOC" in line and "temp" in line.lower():
                                        parts = line.split(":")
                                        if len(parts) >= 2:
                                            temp_str = parts[1].strip().replace("C", "").strip()
                                            try:
                                                temp = float(temp_str)
                                                hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                                break
                                            except:
                                                pass
                    except subprocess.TimeoutExpired:
                        hw_stats["gpu_temp"] = "è¶…æ—¶"
                    except Exception:
                        hw_stats["gpu_temp"] = "è·å–å¤±è´¥"
                else:
                    hw_stats["gpu_temp"] = "æœªæˆæƒ"

            except Exception as e:
                print(f"[DEBUG] macOS GPU info error: {e}")

        elif system == "Linux":
            # NVIDIA GPU (ä½¿ç”¨ nvidia-smi)
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")
                    if lines:
                        # å¯èƒ½æœ‰å¤šä¸ª GPUï¼Œå–ç¬¬ä¸€ä¸ª
                        parts = lines[0].split(",")
                        if len(parts) >= 4:
                            gpu_util = parts[0].strip()
                            mem_used = float(parts[1].strip())
                            mem_total = float(parts[2].strip())
                            temp = parts[3].strip()

                            hw_stats["gpu_usage"] = f"{gpu_util}%"
                            hw_stats["vram_usage"] = f"{mem_used/1024:.1f} GB / {mem_total/1024:.1f} GB"
                            hw_stats["gpu_temp"] = f"{temp}Â°C"
            except FileNotFoundError:
                # nvidia-smi ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ AMD æˆ–æ—  GPU
                try:
                    # å°è¯• AMD GPU (rocm-smi)
                    result = subprocess.run(
                        ["rocm-smi", "--showuse", "--showtemp", "--showmeminfo", "vram"],
                        capture_output=True, text=True, timeout=5
                    )
                    if result.returncode == 0:
                        hw_stats["gpu_usage"] = "AMD GPU"
                except:
                    pass
            except Exception as e:
                print(f"[DEBUG] Linux GPU info error: {e}")

        elif system == "Windows":
            # Windows NVIDIA GPU
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5, shell=True
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")
                    if lines:
                        parts = lines[0].split(",")
                        if len(parts) >= 4:
                            gpu_util = parts[0].strip()
                            mem_used = float(parts[1].strip())
                            mem_total = float(parts[2].strip())
                            temp = parts[3].strip()

                            hw_stats["gpu_usage"] = f"{gpu_util}%"
                            hw_stats["vram_usage"] = f"{mem_used/1024:.1f} GB / {mem_total/1024:.1f} GB"
                            hw_stats["gpu_temp"] = f"{temp}Â°C"
            except Exception as e:
                print(f"[DEBUG] Windows GPU info error: {e}")

    except Exception as e:
        print(f"[DEBUG] Hardware stats error: {e}")

    return hw_stats

def cleanup_old_sessions():
    """æ¸…ç†å†…å­˜ä¸­çš„æ—§ä¼šè¯ç¼“å­˜ (ç£ç›˜æ–‡ä»¶ä¿ç•™)"""
    if len(sessions) > MAX_SESSIONS:
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä»å†…å­˜ä¸­ç§»é™¤æœ€è€çš„
        sorted_sessions = sorted(sessions.items(), key=lambda x: x[1]["created_at"])
        for sid, _ in sorted_sessions[:len(sessions) - MAX_SESSIONS]:
            del sessions[sid]

def load_model():
    global model, processor, model_loaded, model_info, dummy_image

    if model_loaded:
        return True

    print("=" * 60)
    print("åŠ è½½ AI å¤šæ¨¡æ€æ¨¡å‹...")
    print("=" * 60)

    model_name = "google/gemma-3n-E2B-it"
    load_start = time.time()

    print("[1/2] åŠ è½½å¤„ç†å™¨...")
    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

    print("[2/2] åŠ è½½æ¨¡å‹...")
    model = Gemma3nForConditionalGeneration.from_pretrained(
        model_name,
        device_map="auto",
        max_memory={"mps": "64GiB", "cpu": "64GiB"},
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model.eval()

    dummy_image = Image.new('RGB', (64, 64), color='white')
    load_time = time.time() - load_start
    total_params = sum(p.numel() for p in model.parameters())

    model_info = {
        "name": "AI Multimodal",
        "params": f"{total_params / 1e9:.2f}B",
        "dtype": "bfloat16",
        "device": str(model.device),
        "load_time": round(load_time, 2),
        "memory_gb": get_memory_usage(),
        "capabilities": ["æ–‡æœ¬å¯¹è¯", "å›¾åƒç†è§£", "éŸ³é¢‘è½¬å½•", "å¤šè½®å¯¹è¯"],
        "max_tokens": 8192,
    }

    model_loaded = True
    print("=" * 60)
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶ {load_time:.2f}s")
    print(f"å†…å­˜å ç”¨: {model_info['memory_gb']} GB")
    print("=" * 60)
    return True

def start_llama_mmproj_server():
    """å¯åŠ¨ llama-server with mmproj ä½œä¸ºæŒä¹…åŒ–å¤šæ¨¡æ€æ¨ç†æœåŠ¡"""
    global llama_mmproj_server_process, llama_mmproj_server_ready

    if not Path(LLAMA_SERVER_BIN).exists():
        print(f"[llama-mmproj-server] äºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_SERVER_BIN}")
        return False

    if not Path(LLAMA_MM_MODEL).exists():
        print(f"[llama-mmproj-server] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_MM_MODEL}")
        return False

    if not Path(LLAMA_MM_PROJ_IMAGE).exists():
        print(f"[llama-mmproj-server] mmproj æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_MM_PROJ_IMAGE}")
        return False

    # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿è¡Œ
    try:
        import requests
        resp = requests.get(f"http://127.0.0.1:{LLAMA_MMPROJ_SERVER_PORT}/health", timeout=2)
        if resp.status_code == 200:
            print(f"[llama-mmproj-server] å·²åœ¨ç«¯å£ {LLAMA_MMPROJ_SERVER_PORT} è¿è¡Œ")
            llama_mmproj_server_ready = True
            return True
    except:
        pass

    print(f"[llama-mmproj-server] å¯åŠ¨ä¸­... ç«¯å£: {LLAMA_MMPROJ_SERVER_PORT}")
    print(f"[llama-mmproj-server] æ¨¡å‹: {LLAMA_MM_MODEL}")
    print(f"[llama-mmproj-server] mmproj: {LLAMA_MM_PROJ_IMAGE}")

    env = os.environ.copy()
    bin_dir = str(Path(LLAMA_SERVER_BIN).parent)
    env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"

    cmd = [
        LLAMA_SERVER_BIN,
        "-m", LLAMA_MM_MODEL,
        "--mmproj", LLAMA_MM_PROJ_IMAGE,
        "--port", str(LLAMA_MMPROJ_SERVER_PORT),
        "--host", "127.0.0.1",
        "-ngl", "999",
        "-t", "8",
        "--ctx-size", "4096",
    ]

    llama_mmproj_server_process = subprocess.Popen(
        cmd,
        env=env,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    import requests
    for _ in range(60):  # æœ€å¤šç­‰å¾… 60 ç§’ (é¦–æ¬¡åŠ è½½æ¨¡å‹å¯èƒ½è¾ƒæ…¢)
        try:
            resp = requests.get(f"http://127.0.0.1:{LLAMA_MMPROJ_SERVER_PORT}/health", timeout=1)
            if resp.status_code == 200:
                print(f"[llama-mmproj-server] å¯åŠ¨æˆåŠŸï¼")
                llama_mmproj_server_ready = True
                return True
        except:
            pass
        time.sleep(1)

    print("[llama-mmproj-server] å¯åŠ¨è¶…æ—¶")
    return False


def run_llama_mmproj(prompt, image_path=None, audio_path=None,
                     messages_history=None, session_id=None, has_media=False):
    """
    ä½¿ç”¨ llama-server with mmproj ç”Ÿæˆå›å¤
    ä¼˜å…ˆä½¿ç”¨æŒä¹…åŒ–æœåŠ¡ (16x æ›´å¿«)ï¼Œå¤±è´¥æ—¶å›é€€åˆ° CLI

    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        image_path: å›¾ç‰‡è·¯å¾„
        audio_path: éŸ³é¢‘è·¯å¾„
        messages_history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "text": "..."}]
        session_id: ä¼šè¯ID (ç”¨äº thought signature)
        has_media: å½“å‰æ¶ˆæ¯æ˜¯å¦åŒ…å«åª’ä½“
    """
    global llama_mmproj_server_ready

    # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„å®Œæ•´ prompt
    full_prompt = _build_mmproj_prompt(
        prompt, messages_history, session_id, has_media
    )

    # éŸ³é¢‘æš‚ä¸æ”¯æŒ server æ¨¡å¼ï¼Œå›é€€åˆ° CLI
    if audio_path:
        return run_llama_mmproj_cli(full_prompt, image_path, audio_path)

    # å°è¯•ä½¿ç”¨ server æ¨¡å¼
    if not llama_mmproj_server_ready:
        if not start_llama_mmproj_server():
            print("[mmproj] Server å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ° CLI æ¨¡å¼")
            return run_llama_mmproj_cli(full_prompt, image_path, audio_path)

    start = time.time()
    try:
        import requests

        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content = []
        if image_path:
            # å°†å›¾ç‰‡è½¬ä¸º base64
            with open(image_path, "rb") as f:
                img_b64 = base64.b64encode(f.read()).decode("utf-8")
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}
            })
        content.append({"type": "text", "text": full_prompt})

        resp = requests.post(
            f"http://127.0.0.1:{LLAMA_MMPROJ_SERVER_PORT}/v1/chat/completions",
            json={
                "model": "gemma-3n",
                "messages": [{"role": "user", "content": content}],
                "max_tokens": LLAMA_MM_N_PREDICT,
                "temperature": 0.7,
            },
            timeout=120
        )

        elapsed = time.time() - start

        if resp.status_code != 200:
            # æœåŠ¡å™¨å¯èƒ½å‡ºé”™ï¼Œå›é€€åˆ° CLI
            llama_mmproj_server_ready = False
            return run_llama_mmproj_cli(full_prompt, image_path, audio_path)

        data = resp.json()
        response = data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()

        # ä» usage è·å– token æ•°
        usage = data.get("usage", {})
        output_tokens = usage.get("completion_tokens", len(response) // 4)
        speed = output_tokens / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "tokens": output_tokens,
                "backend": "mmproj-server"
            }
        }
    except Exception as e:
        # æœåŠ¡å™¨å¯èƒ½æŒ‚äº†ï¼Œæ ‡è®°ä¸ºä¸å¯ç”¨å¹¶å›é€€åˆ° CLI
        print(f"[mmproj] Server è¯·æ±‚å¤±è´¥: {e}ï¼Œå›é€€åˆ° CLI æ¨¡å¼")
        llama_mmproj_server_ready = False
        return run_llama_mmproj_cli(full_prompt, image_path, audio_path)


def _build_mmproj_prompt(prompt, messages_history=None, session_id=None, has_media=False):
    """
    æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡å’Œåª’ä½“ç†è§£çš„å®Œæ•´ prompt

    ç­–ç•¥:
    - å¦‚æœæœ‰æ–°åª’ä½“: ä¸æ³¨å…¥å†å²ä¸Šä¸‹æ–‡ï¼Œè®©æ¨¡å‹ä¸“æ³¨äºå½“å‰åª’ä½“
    - å¦‚æœæ²¡æœ‰æ–°åª’ä½“: æ³¨å…¥å†å²å¯¹è¯ + åª’ä½“ç†è§£ (thought signature)
    """
    if has_media:
        # æœ‰æ–°åª’ä½“æ—¶ï¼Œç›´æ¥è¿”å›åŸå§‹ prompt
        print(f"[mmproj] æœ‰æ–°åª’ä½“ï¼Œè·³è¿‡å†å²ä¸Šä¸‹æ–‡æ³¨å…¥")
        return prompt

    context_parts = []

    # 1. è·å–åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†)
    if session_id:
        media_context = get_session_media_context(session_id)
        if media_context:
            context_parts.append(f"[Previous Media Understanding]\n{media_context}")
            print(f"[mmproj] æ³¨å…¥åª’ä½“ç†è§£ä¸Šä¸‹æ–‡: {len(media_context)} å­—ç¬¦")

    # 2. è·å–å†å²å¯¹è¯ä¸Šä¸‹æ–‡
    if messages_history:
        history_parts = []
        for msg in messages_history[-MAX_HISTORY_TURNS * 2:]:
            role = "User" if msg["role"] == "user" else "Assistant"
            history_parts.append(f"{role}: {msg['text']}")
        if history_parts:
            history_context = "\n".join(history_parts)
            context_parts.append(f"[Previous Conversation]\n{history_context}")
            print(f"[mmproj] æ³¨å…¥å†å²å¯¹è¯: {len(history_parts)} æ¡æ¶ˆæ¯")

    # 3. æ„å»ºå®Œæ•´ prompt
    if context_parts:
        context_str = "\n\n".join(context_parts)
        full_prompt = f"""{context_str}

[Current Message]: {prompt}

Please respond to the current message, taking into account the context above."""
        return full_prompt
    else:
        return prompt


def run_llama_mmproj_cli(prompt, image_path=None, audio_path=None):
    """ä½¿ç”¨ llama-mtmd-cli ç”Ÿæˆå›å¤ (å›é€€æ¨¡å¼)"""
    if not Path(LLAMA_MTMD_BIN).exists():
        return {"error": f"llama-mtmd-cli ä¸å­˜åœ¨: {LLAMA_MTMD_BIN}"}
    if not Path(LLAMA_MM_MODEL).exists():
        return {"error": f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_MM_MODEL}"}

    # æ ¹æ®è¾“å…¥ç±»å‹åŠ¨æ€é€‰æ‹© mmproj
    mmproj_list = []
    if image_path and LLAMA_MM_PROJ_IMAGE and Path(LLAMA_MM_PROJ_IMAGE).exists():
        mmproj_list.append(LLAMA_MM_PROJ_IMAGE)
    if audio_path and LLAMA_MM_PROJ_AUDIO and Path(LLAMA_MM_PROJ_AUDIO).exists():
        mmproj_list.append(LLAMA_MM_PROJ_AUDIO)

    if not mmproj_list:
        if LLAMA_MM_PROJ and Path(LLAMA_MM_PROJ).exists():
            mmproj_list = [LLAMA_MM_PROJ]
        elif LLAMA_MM_PROJ_COMBINED:
            mmproj_list = [p.strip() for p in LLAMA_MM_PROJ_COMBINED.split(",") if p.strip() and Path(p.strip()).exists()]

    if not mmproj_list:
        return {"error": "æœªé…ç½®æœ‰æ•ˆçš„ mmproj è·¯å¾„"}

    mmproj_combined = ",".join(mmproj_list)
    print(f"[DEBUG] mmproj CLI ä½¿ç”¨: {mmproj_combined}")

    cmd = [
        LLAMA_MTMD_BIN,
        "--log-verbosity", "0",
        "--no-warmup",
        "-m", LLAMA_MM_MODEL,
        "--mmproj", mmproj_combined,
        "-p", prompt,
        "-n", str(LLAMA_MM_N_PREDICT),
        "--temp", "0.7",
    ]
    if image_path:
        cmd += ["--image", image_path if isinstance(image_path, str) else ",".join(image_path)]
    if audio_path:
        cmd += ["--audio", audio_path if isinstance(audio_path, str) else ",".join(audio_path)]

    start = time.time()
    try:
        env = os.environ.copy()
        bin_dir = str(Path(LLAMA_MTMD_BIN).parent)
        env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"
        out = subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=180, env=env)
        elapsed = time.time() - start
        lines = [ln for ln in out.stdout.splitlines() if ln.strip()]
        content_lines = [
            ln for ln in lines
            if not ln.startswith(("ggml", "AVX", "gguf", "llama", "clip", "Using", "model", "warmup", "load"))
        ]
        response = "\n".join(content_lines).strip() if content_lines else "\n".join(lines).strip()

        output_tokens = len(response) // 4
        speed = output_tokens / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "backend": "mmproj-cli"
            }
        }
    except subprocess.TimeoutExpired:
        return {"error": "llama-mtmd-cli è¶…æ—¶"}
    except subprocess.CalledProcessError as e:
        return {"error": f"llama-mtmd-cli å¤±è´¥: {e.stderr or e.stdout}"}


def run_llama_run(prompt, history_context=""):
    """
    ä½¿ç”¨ llama-run è¿›è¡Œçº¯æ–‡æœ¬æ¨ç†
    è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ llama.cpp åç«¯ï¼Œä¸éœ€è¦åŠ è½½ PyTorch æ¨¡å‹
    """
    if not Path(LLAMA_RUN_BIN).exists():
        return {"error": f"llama-run ä¸å­˜åœ¨: {LLAMA_RUN_BIN}"}
    if not LLAMA_RUN_MODEL or not Path(LLAMA_RUN_MODEL).exists():
        return {"error": f"GGUF æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_RUN_MODEL}"}

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«å†å²ä¸Šä¸‹æ–‡ï¼‰
    full_prompt = prompt
    if history_context:
        full_prompt = f"{history_context}\n\nUser: {prompt}\n\nAssistant:"

    start = time.time()
    try:
        env = os.environ.copy()
        bin_dir = str(Path(LLAMA_RUN_BIN).parent)
        env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"

        cmd = [
            LLAMA_RUN_BIN,
            "--ngl", "999",  # ä½¿ç”¨ GPU åŠ é€Ÿ
            "--temp", "0.7",
            "-t", "8",  # ä½¿ç”¨ 8 ä¸ªçº¿ç¨‹
            LLAMA_RUN_MODEL,
            full_prompt
        ]

        out = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120,
            env=env
        )

        elapsed = time.time() - start

        if out.returncode != 0:
            return {"error": f"llama-run å¤±è´¥: {out.stderr}"}

        # llama-run çš„è¾“å‡ºæ˜¯å¹²å‡€çš„ï¼Œç›´æ¥ä½¿ç”¨
        # è¿‡æ»¤æ‰å¯èƒ½çš„ ANSI è½¬ä¹‰ç 
        response = out.stdout.strip()
        # ç§»é™¤ ANSI æ§åˆ¶å­—ç¬¦
        import re
        response = re.sub(r'\x1b\[[0-9;]*m', '', response)
        response = response.strip()

        # ä¼°ç®— token æ•°ï¼ˆç®€å•ä¼°ç®—ï¼šå­—ç¬¦æ•°/4ï¼‰
        output_tokens = len(response) // 4
        speed = output_tokens / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "backend": "llama.cpp"
            }
        }
    except subprocess.TimeoutExpired:
        return {"error": "llama-run è¶…æ—¶"}
    except Exception as e:
        return {"error": f"llama-run é”™è¯¯: {str(e)}"}


def start_llama_server():
    """å¯åŠ¨ llama-server ä½œä¸ºæŒä¹…åŒ–æ¨ç†æœåŠ¡"""
    global llama_server_process, llama_server_ready

    if not Path(LLAMA_SERVER_BIN).exists():
        print(f"[llama-server] äºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_SERVER_BIN}")
        return False

    if not LLAMA_RUN_MODEL or not Path(LLAMA_RUN_MODEL).exists():
        print(f"[llama-server] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_RUN_MODEL}")
        return False

    # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿è¡Œ
    try:
        import requests
        resp = requests.get(f"http://127.0.0.1:{LLAMA_SERVER_PORT}/health", timeout=2)
        if resp.status_code == 200:
            print(f"[llama-server] å·²åœ¨ç«¯å£ {LLAMA_SERVER_PORT} è¿è¡Œ")
            llama_server_ready = True
            return True
    except:
        pass

    print(f"[llama-server] å¯åŠ¨ä¸­... ç«¯å£: {LLAMA_SERVER_PORT}")
    print(f"[llama-server] æ¨¡å‹: {LLAMA_RUN_MODEL}")

    env = os.environ.copy()
    bin_dir = str(Path(LLAMA_SERVER_BIN).parent)
    env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"

    cmd = [
        LLAMA_SERVER_BIN,
        "-m", LLAMA_RUN_MODEL,
        "--port", str(LLAMA_SERVER_PORT),
        "--host", "127.0.0.1",
        "-ngl", "999",
        "-t", "8",
        "--ctx-size", "4096",
        "--flash-attn", "on",
    ]

    llama_server_process = subprocess.Popen(
        cmd,
        env=env,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    import requests
    for _ in range(30):  # æœ€å¤šç­‰å¾… 30 ç§’
        try:
            resp = requests.get(f"http://127.0.0.1:{LLAMA_SERVER_PORT}/health", timeout=1)
            if resp.status_code == 200:
                print(f"[llama-server] å¯åŠ¨æˆåŠŸï¼")
                llama_server_ready = True
                return True
        except:
            pass
        time.sleep(1)

    print("[llama-server] å¯åŠ¨è¶…æ—¶")
    return False


def query_llama_server(prompt, history_context=""):
    """
    é€šè¿‡ llama-server API è¿›è¡Œæ¨ç†
    é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºæ¨¡å‹å·²ç»é¢„åŠ è½½
    """
    global llama_server_ready

    if not llama_server_ready:
        if not start_llama_server():
            # å›é€€åˆ° llama-run
            return run_llama_run(prompt, history_context)

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯
    full_prompt = prompt
    if history_context:
        full_prompt = f"{history_context}\n\nUser: {prompt}\n\nAssistant:"

    start = time.time()
    try:
        import requests
        resp = requests.post(
            f"http://127.0.0.1:{LLAMA_SERVER_PORT}/completion",
            json={
                "prompt": full_prompt,
                "n_predict": 256,
                "temperature": 0.7,
                "stop": ["</s>", "<eos>", "\n\nUser:", "\nUser:"],
                "stream": False,
            },
            timeout=60
        )

        elapsed = time.time() - start

        if resp.status_code != 200:
            return {"error": f"llama-server è¯·æ±‚å¤±è´¥: {resp.status_code}"}

        data = resp.json()
        response = data.get("content", "").strip()
        tokens_predicted = data.get("tokens_predicted", len(response) // 4)

        # ä» timings è·å–çœŸå®é€Ÿåº¦
        timings = data.get("timings", {})
        speed = timings.get("predicted_per_second", 0)
        if speed == 0:
            speed = tokens_predicted / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "tokens": tokens_predicted,
                "backend": "llama-server"
            }
        }
    except Exception as e:
        # æœåŠ¡å™¨å¯èƒ½æŒ‚äº†ï¼Œå°è¯•é‡å¯
        llama_server_ready = False
        return {"error": f"llama-server é”™è¯¯: {str(e)}"}

def generate_response(messages_history, current_content, session_id=None, has_media=False, media_type=None):
    """
    ç”Ÿæˆå›å¤
    messages_history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "text": "..."}]
    current_content: å½“å‰æ¶ˆæ¯çš„contentåˆ—è¡¨
    session_id: ä¼šè¯ID (ç”¨äº thought signature)
    has_media: å½“å‰æ¶ˆæ¯æ˜¯å¦åŒ…å«åª’ä½“ (å›¾ç‰‡/éŸ³é¢‘)
    media_type: å½“å‰åª’ä½“ç±»å‹ ("image", "audio", None)

    æ³¨æ„: Gemma 3n å¤„ç†å™¨è¦æ±‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å›¾ç‰‡ï¼Œå¦åˆ™ä¼šæŠ¥æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´é”™è¯¯ã€‚
    è§£å†³æ–¹æ¡ˆ: å°†å†å²å¯¹è¯åˆå¹¶æˆä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œåªå‘é€ä¸€æ¡åŒ…å«å›¾ç‰‡çš„å½“å‰æ¶ˆæ¯ã€‚
    """
    global stats

    if not model_loaded:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}

    start_time = time.time()
    history_turns = len(messages_history) // 2  # è®°å½•å®é™…çš„å†å²è½®æ¬¡

    # è·å–å†å²åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†)
    # ç­–ç•¥ï¼š
    # - å¦‚æœå½“å‰æ¶ˆæ¯æœ‰æ–°åª’ä½“ï¼šä¸æ³¨å…¥å†å²åª’ä½“ç†è§£ï¼ˆé¿å…å¹²æ‰°ï¼‰
    # - å¦‚æœå½“å‰æ¶ˆæ¯æ²¡æœ‰æ–°åª’ä½“ï¼šæ³¨å…¥å†å²åª’ä½“ç†è§£ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå›ç­”å…³äºä¹‹å‰åª’ä½“çš„é—®é¢˜
    media_context = ""
    if session_id and not has_media:
        # åªåœ¨æ²¡æœ‰æ–°åª’ä½“æ—¶æ‰æ³¨å…¥å†å²åª’ä½“ç†è§£
        media_context = get_session_media_context(session_id)
        if media_context:
            print(f"[DEBUG] æ³¨å…¥åª’ä½“ç†è§£ä¸Šä¸‹æ–‡: {len(media_context)} å­—ç¬¦")
    elif has_media:
        print(f"[DEBUG] å½“å‰æœ‰æ–°{media_type or 'åª’ä½“'}ï¼Œè·³è¿‡å†å²åª’ä½“ç†è§£æ³¨å…¥")

    # æ–¹æ¡ˆ: å°†å†å²å¯¹è¯åˆå¹¶æˆä¸Šä¸‹æ–‡æ–‡æœ¬
    # è¿™æ ·å°±åªæœ‰ä¸€æ¡ user æ¶ˆæ¯ï¼Œé¿å…æ‰¹æ¬¡ä¸ä¸€è‡´é—®é¢˜
    # æ³¨æ„: å½“æœ‰æ–°åª’ä½“æ—¶ï¼Œä¸æ³¨å…¥å†å²å¯¹è¯ï¼Œé¿å…å¹²æ‰°æ¨¡å‹å¯¹å½“å‰åª’ä½“çš„ç†è§£
    history_context = ""
    if messages_history and not has_media:
        history_parts = []
        for msg in messages_history[-MAX_HISTORY_TURNS * 2:]:
            role = "User" if msg["role"] == "user" else "Assistant"
            history_parts.append(f"{role}: {msg['text']}")
        history_context = "\n".join(history_parts)
    elif has_media:
        print(f"[DEBUG] å½“å‰æœ‰æ–°åª’ä½“ï¼Œè·³è¿‡å†å²å¯¹è¯æ³¨å…¥")

    # æ„å»ºå•æ¡æ¶ˆæ¯ (åŒ…å«å†å²ä¸Šä¸‹æ–‡ + å½“å‰è¾“å…¥)
    messages = []

    # ä¿®æ”¹å½“å‰æ¶ˆæ¯å†…å®¹ï¼Œæ·»åŠ å†å²ä¸Šä¸‹æ–‡
    modified_content = []
    current_text = ""

    for item in current_content:
        if item.get("type") == "text":
            current_text = item.get("text", "")
        else:
            modified_content.append(item)

    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡æç¤º
    # åŒ…å«: åª’ä½“ç†è§£ + å†å²å¯¹è¯ + å½“å‰æ¶ˆæ¯
    full_context_parts = []

    # 1. åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†) - åªåœ¨æ— æ–°åª’ä½“æ—¶æ³¨å…¥
    if media_context:
        full_context_parts.append(f"[Previous Media Understanding]\n{media_context}")

    # 2. å†å²å¯¹è¯ä¸Šä¸‹æ–‡
    if history_context:
        full_context_parts.append(f"[Previous Conversation]\n{history_context}")

    # 3. å½“å‰æ¶ˆæ¯
    if full_context_parts:
        # æœ‰ä¸Šä¸‹æ–‡ï¼Œæ„å»ºå®Œæ•´æç¤º
        context_str = "\n\n".join(full_context_parts)
        context_prompt = f"""{context_str}

[Current Message]: {current_text}

Please respond to the current message, taking into account the context above."""
        modified_content.append({"type": "text", "text": context_prompt})
    else:
        # æ— ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½¿ç”¨å½“å‰æ–‡æœ¬
        modified_content.append({"type": "text", "text": current_text})

    messages.append({"role": "user", "content": modified_content})

    print(f"[DEBUG] æ¶ˆæ¯æ•°é‡: {len(messages)}, å†å²è½®æ¬¡: {history_turns}, æœ‰æ–°åª’ä½“: {has_media}")

    # å¤„ç†è¾“å…¥
    tokenize_start = time.time()
    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    )
    tokenize_time = time.time() - tokenize_start

    # å‡†å¤‡ç”Ÿæˆå‚æ•°
    input_ids = inputs["input_ids"].to(model.device)
    attention_mask = inputs["attention_mask"].to(model.device)
    input_tokens = input_ids.shape[1]

    print(f"[DEBUG] input_tokens: {input_tokens}")

    generate_kwargs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "max_new_tokens": 512,
        "do_sample": False,
    }

    # å¤„ç†å›¾åƒ
    if "pixel_values" in inputs and inputs["pixel_values"] is not None:
        generate_kwargs["pixel_values"] = inputs["pixel_values"].to(model.device, dtype=model.dtype)

    # å¤„ç†éŸ³é¢‘
    if "input_features" in inputs and inputs["input_features"] is not None:
        generate_kwargs["input_features"] = inputs["input_features"].to(model.device, dtype=model.dtype)
        generate_kwargs["input_features_mask"] = inputs["input_features_mask"].to(model.device)

    # ç”Ÿæˆ
    generate_start = time.time()
    with torch.inference_mode():
        outputs = model.generate(**generate_kwargs)
    generate_time = time.time() - generate_start

    # è§£ç 
    output_tokens = len(outputs[0]) - input_tokens
    response = processor.tokenizer.decode(
        outputs[0][input_ids.shape[1]:],
        skip_special_tokens=True
    )

    total_time = time.time() - start_time
    speed = output_tokens / generate_time if generate_time > 0 else 0

    # æ›´æ–°ç»Ÿè®¡
    stats["total_requests"] += 1
    stats["total_tokens"] += output_tokens
    stats["total_time"] += total_time
    stats["avg_speed"] = stats["total_tokens"] / stats["total_time"] if stats["total_time"] > 0 else 0

    return {
        "response": response,
        "metrics": {
            "total_time": round(total_time, 2),
            "tokenize_time": round(tokenize_time, 3),
            "generate_time": round(generate_time, 2),
            "input_tokens": int(input_tokens),
            "output_tokens": int(output_tokens),
            "speed": round(speed, 1),
            "history_turns": history_turns  # ä½¿ç”¨å®é™…çš„å†å²è½®æ¬¡
        }
    }

@app.route("/")
def index():
    return send_from_directory("static", "index.html")

@app.route("/chat")
def chat_page():
    return send_from_directory("static", "chat.html")

@app.route("/docs")
def docs_page():
    return send_from_directory("static", "docs.html")

@app.route("/api/status")
def status():
    hw_stats = get_hardware_stats() if model_loaded else {}
    mmproj_paths = [p.strip() for part in LLAMA_MM_PROJ_COMBINED.split(",") for p in part.split(";") if p.strip()]
    mmproj_files = {
        "bin": Path(LLAMA_MTMD_BIN).exists(),
        "model": Path(LLAMA_MM_MODEL).exists(),
        "proj": all(Path(p).exists() for p in mmproj_paths),
    }
    if LLAMA_MM_PROJ_IMAGE:
        mmproj_files["proj_image"] = Path(LLAMA_MM_PROJ_IMAGE).exists()
    if LLAMA_MM_PROJ_AUDIO:
        mmproj_files["proj_audio"] = Path(LLAMA_MM_PROJ_AUDIO).exists()

    # llama.cpp åç«¯çŠ¶æ€
    llama_cpp_ready = Path(LLAMA_RUN_BIN).exists() and LLAMA_RUN_MODEL and Path(LLAMA_RUN_MODEL).exists()
    llama_cpp_files = {
        "bin": Path(LLAMA_RUN_BIN).exists(),
        "model": bool(LLAMA_RUN_MODEL and Path(LLAMA_RUN_MODEL).exists()),
        "model_path": LLAMA_RUN_MODEL or "æœªæ‰¾åˆ°"
    }

    return jsonify({
        "loaded": model_loaded,
        "stats": stats,
        "memory_gb": get_memory_usage() if model_loaded else 0,
        "hardware": hw_stats,
        "active_sessions": len(sessions),
        "default_backend": DEFAULT_BACKEND,
        "mmproj_ready": all(mmproj_files.values()),
        "mmproj_files": mmproj_files,
        "llama_cpp_ready": llama_cpp_ready,
        "llama_cpp_files": llama_cpp_files,
    })

@app.route("/api/session/new", methods=["POST"])
def new_session():
    """åˆ›å»ºæ–°ä¼šè¯"""
    session_id = str(uuid.uuid4())[:8]
    session_data = {
        "messages": [],
        "created_at": time.time(),
        "title": "æ–°å¯¹è¯"
    }
    sessions[session_id] = session_data
    # ç«‹å³ä¿å­˜åˆ°ç£ç›˜
    save_session_to_disk(session_id, session_data)
    cleanup_old_sessions()
    return jsonify({"session_id": session_id})

@app.route("/api/session/list", methods=["GET"])
def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
    session_list = list_all_sessions()
    return jsonify({"sessions": session_list})

@app.route("/api/session/<session_id>/load", methods=["GET"])
def load_session(session_id):
    """åŠ è½½æŒ‡å®šä¼šè¯"""
    # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
    if session_id in sessions:
        session = sessions[session_id]
        return jsonify({
            "session_id": session_id,
            "messages": session["messages"],
            "title": session.get("title", "æ–°å¯¹è¯")
        })

    # ä»ç£ç›˜æŸ¥æ‰¾
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        if session_id in jsonl_file.name:
            session_data = load_session_from_disk(jsonl_file)
            if session_data:
                sessions[session_id] = session_data
                return jsonify({
                    "session_id": session_id,
                    "messages": session_data["messages"],
                    "title": session_data.get("title", "æ–°å¯¹è¯")
                })

    return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

@app.route("/api/session/<session_id>/delete", methods=["POST"])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    # ä»å†…å­˜ç§»é™¤
    if session_id in sessions:
        file_path = sessions[session_id].get("file_path")
        del sessions[session_id]
        # åˆ é™¤ç£ç›˜æ–‡ä»¶
        if file_path and Path(file_path).exists():
            Path(file_path).unlink()
            return jsonify({"success": True})

    # ä»ç£ç›˜æŸ¥æ‰¾å¹¶åˆ é™¤
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        if session_id in jsonl_file.name:
            jsonl_file.unlink()
            return jsonify({"success": True})

    return jsonify({"success": True})

@app.route("/api/session/<session_id>/clear", methods=["POST"])
def clear_session(session_id):
    """æ¸…ç©ºä¼šè¯å†å²"""
    if session_id in sessions:
        sessions[session_id]["messages"] = []
        sessions[session_id]["title"] = "æ–°å¯¹è¯"
        # ä¿å­˜åˆ°ç£ç›˜
        save_session_to_disk(session_id, sessions[session_id])
    return jsonify({"success": True})

@app.route("/api/session/<session_id>/history", methods=["GET"])
def get_history(session_id):
    """è·å–ä¼šè¯å†å²"""
    if session_id not in sessions:
        return jsonify({"messages": []})
    return jsonify({"messages": sessions[session_id]["messages"]})

@app.route("/api/thought/state/<session_id>", methods=["GET"])
def get_thought_state(session_id):
    """è·å–ä¼šè¯çš„ thought signature çŠ¶æ€"""
    state = thought_states.get(session_id, {"turn_index": 0, "media_refs": []})
    media_understandings = []

    for media_ref in state.get("media_refs", []):
        cached = media_understanding_cache.get(media_ref)
        if cached:
            media_understandings.append({
                "media_ref": media_ref,
                "turn_index": cached.get("turn_index", 0),
                "understanding_preview": cached.get("understanding", "")[:100] + "...",
                "created_at": cached.get("created_at", 0)
            })

    return jsonify({
        "session_id": session_id,
        "turn_index": state.get("turn_index", 0),
        "media_count": len(state.get("media_refs", [])),
        "media_understandings": media_understandings
    })

@app.route("/api/thought/stats", methods=["GET"])
def thought_stats():
    """è·å– thought signature å…¨å±€ç»Ÿè®¡"""
    return jsonify({
        "total_sessions": len(thought_states),
        "total_media_cached": len(media_understanding_cache),
        "sessions": [
            {
                "session_id": sid,
                "turn_index": state.get("turn_index", 0),
                "media_count": len(state.get("media_refs", []))
            }
            for sid, state in thought_states.items()
        ]
    })

@app.route("/api/chat", methods=["POST"])
def chat():
    try:
        data = request.json
        text = data.get("text", "")
        image_data = data.get("image")
        audio_data = data.get("audio")
        session_id = data.get("session_id")
        backend = data.get("backend") or DEFAULT_BACKEND

        import sys
        print(f"[DEBUG /api/chat] backend={backend}, has_image={bool(image_data)}, has_audio={bool(audio_data)}, text={repr(text[:50] if text else '')}", flush=True)
        sys.stdout.flush()

        # è·å–æˆ–åˆ›å»ºä¼šè¯
        if not session_id or session_id not in sessions:
            # å°è¯•ä»ç£ç›˜åŠ è½½
            loaded = False
            if session_id:
                for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
                    if session_id in jsonl_file.name:
                        session_data = load_session_from_disk(jsonl_file)
                        if session_data:
                            sessions[session_id] = session_data
                            loaded = True
                            break
            if not loaded:
                session_id = str(uuid.uuid4())[:8]
                sessions[session_id] = {"messages": [], "created_at": time.time(), "title": "æ–°å¯¹è¯"}

        session = sessions[session_id]
        image = None
        audio = None
        audio_path = None
        image_path = None

        # å¤„ç†å›¾ç‰‡
        if image_data:
            if "," in image_data:
                image_data = image_data.split(",")[1]
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
            # ä¿å­˜ä¸´æ—¶å›¾ç‰‡ä¾› mmproj ä½¿ç”¨
            image_path = f"/tmp/image_{session_id}.png"
            image.save(image_path)

        # å¤„ç†éŸ³é¢‘
        if audio_data:
            mime_part = ""
            if "," in audio_data:
                mime_part = audio_data.split(",")[0]
                audio_data = audio_data.split(",")[1]
            audio_bytes = base64.b64decode(audio_data)

            if "wav" in mime_part:
                ext = ".wav"
            elif "webm" in mime_part:
                ext = ".webm"
            elif "ogg" in mime_part:
                ext = ".ogg"
            elif "mp3" in mime_part or "mpeg" in mime_part:
                ext = ".mp3"
            elif "flac" in mime_part:
                ext = ".flac"
            else:
                ext = ".wav"

            temp_path = f"/tmp/audio_{session_id}{ext}"
            with open(temp_path, "wb") as f:
                f.write(audio_bytes)

            audio_array, sr = librosa.load(temp_path, sr=16000)
            audio = (audio_array, sr)
            audio_path = temp_path
            print(f"[DEBUG] éŸ³é¢‘: {len(audio_array)/sr:.2f}ç§’")

        # æ„å»ºå½“å‰æ¶ˆæ¯å†…å®¹
        content = []
        has_media = (image is not None or audio is not None)
        display_text = text

        if backend == "mps":
            if not has_media:
                # çº¯æ–‡æœ¬æ¶ˆæ¯ï¼šæ·»åŠ  dummy_image
                content.append({"type": "image", "image": dummy_image})
                display_text = text
                text = "Ignore the blank image. " + text

            # Gemma 3n è¦æ±‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å›¾ç‰‡
            # å¦‚æœåªæœ‰éŸ³é¢‘æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿéœ€è¦æ·»åŠ  dummy_image
            if image is not None:
                content.append({"type": "image", "image": image})
            elif audio is not None:
                # åªæœ‰éŸ³é¢‘ï¼Œæ·»åŠ  dummy_image
                content.append({"type": "image", "image": dummy_image})
                text = "Ignore the blank image. " + text

            if audio is not None:
                content.append({"type": "audio", "audio": audio[0], "sample_rate": audio[1]})

            content.append({"type": "text", "text": text})

        # è®¡ç®—å½“å‰è½®æ¬¡
        turn_index = len(session["messages"]) // 2 + 1

        # ç”Ÿæˆå›å¤ (ä¼ å…¥ session_id å’Œ has_media)
        # ç¡®å®šå½“å‰åª’ä½“ç±»å‹
        current_media_type = None
        if image is not None:
            current_media_type = "image"
        elif audio is not None:
            current_media_type = "audio"

        # ç”Ÿæˆå›å¤ (ä¼ å…¥ session_id, has_media, media_type)
        if backend == "mps":
            result = generate_response(
                session["messages"],
                content,
                session_id=session_id,
                has_media=has_media,
                media_type=current_media_type
            )
        elif backend == "llama.cpp":
            # llama.cpp çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆä½¿ç”¨ llama-server æŒä¹…åŒ–æœåŠ¡ï¼‰
            # æ³¨æ„ï¼šåªæ”¯æŒæ–‡æœ¬ï¼Œä¸æ”¯æŒå›¾ç‰‡/éŸ³é¢‘
            if has_media:
                result = {"error": "llama.cpp åç«¯æš‚ä¸æ”¯æŒå›¾ç‰‡/éŸ³é¢‘ï¼Œè¯·åˆ‡æ¢åˆ° MPS æˆ– mmproj åç«¯"}
            else:
                # æ„å»ºå†å²ä¸Šä¸‹æ–‡
                history_context = ""
                if session["messages"]:
                    history_parts = []
                    for msg in session["messages"][-MAX_HISTORY_TURNS * 2:]:
                        role = "User" if msg["role"] == "user" else "Assistant"
                        history_parts.append(f"{role}: {msg['text']}")
                    history_context = "\n".join(history_parts)

                # ä½¿ç”¨ llama-server æŒä¹…åŒ–æœåŠ¡ï¼ˆæ›´å¿«ï¼‰
                result = query_llama_server(text, history_context)
        else:
            # llama.cpp/mmproj æ¨¡å¼
            # ä¼ å…¥å†å²æ¶ˆæ¯å’Œ session_idï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œ thought signature
            mm_prompt = text or "Please describe what you see/hear."
            print(f"[DEBUG mmproj] backend={backend}, text={repr(text)}, image_path={image_path}, audio_path={audio_path}")

            result = run_llama_mmproj(
                mm_prompt,
                image_path=image_path,
                audio_path=audio_path,
                messages_history=session["messages"],
                session_id=session_id,
                has_media=has_media
            )

        if "error" not in result:
            # å¦‚æœæœ‰åª’ä½“è¾“å…¥ï¼Œä»æ¨¡å‹å›å¤ä¸­æå–ç†è§£å¹¶å­˜å‚¨åˆ° thought signature
            if has_media:
                # æ¨¡å‹çš„å›å¤å°±æ˜¯å¯¹åª’ä½“çš„ç†è§£
                # å°†å…¶å­˜å‚¨ä¸º "å‹ç¼©è®°å¿†"
                understanding = result["response"]

                # ç”Ÿæˆåª’ä½“ç†è§£ç­¾å
                media_ref = generate_media_signature(
                    session_id=session_id,
                    turn_index=turn_index,
                    understanding=understanding[:500]  # é™åˆ¶é•¿åº¦ï¼Œåªä¿å­˜æ‘˜è¦
                )
                print(f"[Thought Signature] å­˜å‚¨ {current_media_type} ç†è§£: {media_ref}")

            # ä¿å­˜åˆ°å†å²ï¼ˆåªä¿å­˜æ–‡æœ¬æ‘˜è¦ï¼‰
            user_summary = display_text
            if image is not None:
                user_summary = "[å›¾ç‰‡] " + user_summary
            if audio is not None:
                user_summary = "[éŸ³é¢‘] " + user_summary

            session["messages"].append({
                "role": "user",
                "text": user_summary,
                "has_image": image is not None,
                "has_audio": audio is not None,
                "timestamp": time.time()
            })
            session["messages"].append({
                "role": "assistant",
                "text": result["response"],
                "timestamp": time.time()
            })

            # é™åˆ¶å†å²é•¿åº¦
            if len(session["messages"]) > MAX_HISTORY_TURNS * 2:
                session["messages"] = session["messages"][-MAX_HISTORY_TURNS * 2:]

            # æ›´æ–°æ ‡é¢˜ (ç”¨ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯)
            if session.get("title") == "æ–°å¯¹è¯" and len(session["messages"]) >= 1:
                first_user_msg = next((m for m in session["messages"] if m["role"] == "user"), None)
                if first_user_msg:
                    session["title"] = first_user_msg["text"][:30] + ("..." if len(first_user_msg["text"]) > 30 else "")

            # ä¿å­˜åˆ°ç£ç›˜
            save_session_to_disk(session_id, session)
            # è¿½åŠ åˆ°å…¨å±€å†å²
            append_to_history(session_id, display_text)

        result["session_id"] = session_id
        return jsonify(result)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    init_storage()

    # macOS: è¯·æ±‚ sudo æƒé™ç”¨äºç¡¬ä»¶ç›‘æ§
    if platform.system() == "Darwin":
        request_sudo_permission()

    load_model()
    print("\n" + "=" * 60)
    print("AI å¤šæ¨¡æ€èŠå¤©æœåŠ¡å™¨å¯åŠ¨: http://localhost:5000")
    print(f"ä¼šè¯å­˜å‚¨: {GEMMA3N_HOME}")
    print("æ”¯æŒå¤šè½®å¯¹è¯å†å²è®°å¿†")
    if sudo_authorized:
        print("GPU æ¸©åº¦ç›‘æ§: âœ… å·²å¯ç”¨")
    else:
        print("GPU æ¸©åº¦ç›‘æ§: âŒ æœªå¯ç”¨ (å¯é‡å¯æœåŠ¡å¹¶æˆæƒ)")
    print("=" * 60)
    app.run(host="0.0.0.0", port=5000, debug=False, threaded=True)
