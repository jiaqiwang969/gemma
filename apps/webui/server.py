"""
AI å¤šæ¨¡æ€èŠå¤©æœåŠ¡å™¨
æ”¯æŒ: æ–‡æœ¬ + å›¾ç‰‡ + éŸ³é¢‘ + å¤šè½®å¯¹è¯å†å²
å­˜å‚¨: ~/.gemma3n/ (å‚è€ƒ Codex æ¶æ„)

åç«¯æ¨¡å¼:
  - mmproj (é»˜è®¤): ä½¿ç”¨ llama.cpp å¤šæ¨¡æ€ï¼Œæ— éœ€ PyTorch
  - mps (è¿›é˜¶): ä½¿ç”¨ PyTorch MPS åŠ é€Ÿï¼Œéœ€è¦å®‰è£… torch/transformers
"""
import os
import io
import base64
import uuid
import json
from pathlib import Path
from datetime import datetime
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from PIL import Image
import warnings
import time
import psutil
import subprocess
import platform
import threading

warnings.filterwarnings("ignore")

# PyTorch æ˜¯å¯é€‰ä¾èµ– (ä»… MPS æ¨¡å¼éœ€è¦)
PYTORCH_AVAILABLE = False
torch = None
np = None
librosa = None
AutoProcessor = None
Gemma3nForConditionalGeneration = None

# å…ˆå•ç‹¬å¯¼å…¥ librosa (ç”¨äºéŸ³é¢‘å¤„ç†ï¼Œä¸ä¾èµ– PyTorch)
try:
    import librosa as _librosa
    librosa = _librosa
except ImportError:
    pass

try:
    import torch as _torch
    import numpy as _np
    from transformers import AutoProcessor as _AutoProcessor
    from transformers import Gemma3nForConditionalGeneration as _Gemma3nForConditionalGeneration
    torch = _torch
    np = _np
    AutoProcessor = _AutoProcessor
    Gemma3nForConditionalGeneration = _Gemma3nForConditionalGeneration
    PYTORCH_AVAILABLE = True
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
except ImportError:
    pass

app = Flask(__name__, static_folder="static")
CORS(app)

# å…¨å±€å˜é‡
model = None
processor = None
model_loaded = False
model_info = {}
dummy_image = None
# é»˜è®¤ä½¿ç”¨ llama.cpp (mmproj)ï¼ŒMPS éœ€è¦ PyTorch
DEFAULT_BACKEND = os.environ.get("GEMMA3N_BACKEND", "mmproj")  # mmproj | mps

# llama.cpp è·¯å¾„/æ¨¡å‹ï¼ˆmmproj æ¨¡å¼ä½¿ç”¨ï¼‰
REPO_ROOT = Path(__file__).resolve().parents[2]
LINGKONG_HOME = Path.home() / ".lingkong"

# ä¼˜å…ˆä½¿ç”¨ ~/.lingkong å®‰è£…ç›®å½•ï¼Œå›é€€åˆ°é¡¹ç›®ç›®å½•
def _find_binary(name, fallback):
    lingkong_path = LINGKONG_HOME / "bin" / name
    if lingkong_path.exists():
        return str(lingkong_path)
    return fallback

def _find_model(name, fallback):
    lingkong_path = LINGKONG_HOME / "models" / name
    if lingkong_path.exists():
        return str(lingkong_path)
    return fallback

LLAMA_MTMD_BIN = os.environ.get("LLAMA_MTMD_BIN", _find_binary("llama-mtmd-cli", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-mtmd-cli")))
LLAMA_MM_MODEL = os.environ.get("LLAMA_MM_MODEL", _find_model("gemma-3n-E2B-it-Q4_K_M.gguf", str(REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-Q4_K_M.gguf")))
LLAMA_MM_PROJ_IMAGE = os.environ.get("LLAMA_MM_PROJ_IMAGE", _find_model("gemma-3n-vision-mmproj-f16.gguf", str(REPO_ROOT / "artifacts/gguf/gemma-3n-vision-mmproj-f16.gguf")))
LLAMA_MM_PROJ_AUDIO = os.environ.get("LLAMA_MM_PROJ_AUDIO", _find_model("gemma-3n-audio-mmproj-f16.gguf", str(REPO_ROOT / "artifacts/gguf/gemma-3n-audio-mmproj-f16.gguf")))
LLAMA_MM_PROJ = os.environ.get("LLAMA_MM_PROJ", "")
LLAMA_MM_PROJ_COMBINED = ",".join([p for p in [LLAMA_MM_PROJ_IMAGE, LLAMA_MM_PROJ_AUDIO] if p]) if (LLAMA_MM_PROJ_IMAGE or LLAMA_MM_PROJ_AUDIO) else LLAMA_MM_PROJ
LLAMA_MM_N_PREDICT = int(os.environ.get("LLAMA_MM_N_PREDICT", "128"))
LLAMA_MM_DEVICE = os.environ.get("LLAMA_MM_DEVICE", "none")
LLAMA_MM_N_GPU_LAYERS = os.environ.get("LLAMA_MM_N_GPU_LAYERS", "0")

# llama-run è·¯å¾„/æ¨¡å‹ï¼ˆllama.cpp çº¯æ–‡æœ¬æ¨¡å¼ï¼‰
LLAMA_RUN_BIN = os.environ.get("LLAMA_RUN_BIN", _find_binary("llama-run", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-run")))
LLAMA_SERVER_BIN = os.environ.get("LLAMA_SERVER_BIN", _find_binary("llama-server", str(REPO_ROOT / "infra/llama.cpp/build/bin/llama-server")))
LLAMA_SERVER_PORT = int(os.environ.get("LLAMA_SERVER_PORT", "8081"))
LLAMA_RUN_MODEL = os.environ.get("LLAMA_RUN_MODEL", "")
# è‡ªåŠ¨æŸ¥æ‰¾å¯ç”¨çš„ GGUF æ¨¡å‹
if not LLAMA_RUN_MODEL:
    for candidate in [
        LINGKONG_HOME / "models/gemma-3n-E2B-it-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-finetuned-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-Q4_K_M.gguf",
        REPO_ROOT / "artifacts/gguf/gemma-3n-E2B-it-fp16.gguf",
    ]:
        if candidate.exists():
            LLAMA_RUN_MODEL = str(candidate)
            break

# llama-server è¿›ç¨‹ç®¡ç† (çº¯æ–‡æœ¬æ¨¡å¼)
llama_server_process = None
llama_server_ready = False

# llama-server mmproj æ¨¡å¼ (å¤šæ¨¡æ€)
LLAMA_MMPROJ_SERVER_PORT = int(os.environ.get("LLAMA_MMPROJ_SERVER_PORT", "8082"))
llama_mmproj_server_process = None
llama_mmproj_server_ready = False

# å­˜å‚¨è·¯å¾„ (~/.gemma3n/)
GEMMA3N_HOME = Path.home() / ".gemma3n"
SESSIONS_DIR = GEMMA3N_HOME / "sessions"
HISTORY_FILE = GEMMA3N_HOME / "history.jsonl"

# ä¼šè¯ç®¡ç† (å†…å­˜ç¼“å­˜)
sessions = {}  # session_id -> {"messages": [...], "created_at": timestamp, "file_path": ...}
MAX_SESSIONS = 100
MAX_HISTORY_TURNS = 10  # ä¿ç•™æœ€è¿‘Nè½®å¯¹è¯

# sudo æƒé™çŠ¶æ€
sudo_authorized = False
sudo_refresh_thread = None

# ========== Thought Signature ç³»ç»Ÿ (å‹ç¼©è®°å¿†) ==========
# æ ¸å¿ƒæ€æƒ³: å›¾ç‰‡/éŸ³é¢‘ â†’ æ¨¡å‹ç†è§£ â†’ å­˜å…¥ signature â†’ åç»­è½®æ¬¡æ¢å¤
import hashlib
import hmac

THOUGHT_SIGNATURE_SECRET = "gemma3n-thought-signature-key"
media_understanding_cache = {}  # media_ref -> {"understanding": "...", "session_id": "..."}
thought_states = {}  # session_id -> {"turn_index": 0, "media_refs": [...]}


def generate_media_signature(session_id: str, turn_index: int, understanding: str) -> str:
    """
    ä¸ºåª’ä½“ç†è§£ç”Ÿæˆç­¾åå¼•ç”¨

    Returns: media_ref (ç”¨äºåç»­æ¢å¤)
    """
    import base64
    timestamp = int(time.time())
    media_ref = hashlib.md5(f"{session_id}:{turn_index}:{timestamp}".encode()).hexdigest()[:12]

    # å­˜å‚¨ç†è§£åˆ°ç¼“å­˜
    media_understanding_cache[media_ref] = {
        "session_id": session_id,
        "turn_index": turn_index,
        "understanding": understanding,
        "created_at": timestamp
    }

    # æ›´æ–°ä¼šè¯çš„æ€ç»´çŠ¶æ€
    if session_id not in thought_states:
        thought_states[session_id] = {"turn_index": 0, "media_refs": []}
    thought_states[session_id]["media_refs"].append(media_ref)
    thought_states[session_id]["turn_index"] = turn_index

    return media_ref


def get_session_media_context(session_id: str) -> str:
    """
    è·å–ä¼šè¯ä¸­æ‰€æœ‰åª’ä½“ç†è§£çš„ä¸Šä¸‹æ–‡

    è¿™æ˜¯ thought signature ä½œä¸º"å‹ç¼©è®°å¿†"çš„æ ¸å¿ƒï¼š
    - ç¬¬1è½®: ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ â†’ æ¨¡å‹ç”Ÿæˆç†è§£ â†’ å­˜å…¥ signature
    - ç¬¬2è½®: ä» signature æ¢å¤ç†è§£ â†’ æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­
    """
    context_parts = []

    state = thought_states.get(session_id, {})
    media_refs = state.get("media_refs", [])

    for media_ref in media_refs:
        cached = media_understanding_cache.get(media_ref)
        if cached:
            turn = cached.get("turn_index", 0)
            understanding = cached.get("understanding", "")
            if understanding:
                context_parts.append(f"[Turn {turn} - Media Understanding]: {understanding}")

    return "\n".join(context_parts) if context_parts else ""

stats = {
    "total_requests": 0,
    "total_tokens": 0,
    "total_time": 0,
    "avg_speed": 0
}

def init_storage():
    """åˆå§‹åŒ–å­˜å‚¨ç›®å½•ç»“æ„"""
    GEMMA3N_HOME.mkdir(exist_ok=True)
    SESSIONS_DIR.mkdir(exist_ok=True)
    if not HISTORY_FILE.exists():
        HISTORY_FILE.touch()
    print(f"[Storage] åˆå§‹åŒ–å®Œæˆ: {GEMMA3N_HOME}")

def get_session_dir():
    """è·å–å½“å¤©çš„ä¼šè¯ç›®å½• sessions/YYYY/MM/DD/"""
    now = datetime.now()
    day_dir = SESSIONS_DIR / str(now.year) / f"{now.month:02d}" / f"{now.day:02d}"
    day_dir.mkdir(parents=True, exist_ok=True)
    return day_dir

def save_session_to_disk(session_id, session_data):
    """å°†ä¼šè¯ä¿å­˜åˆ° JSONL æ–‡ä»¶"""
    if "file_path" not in session_data:
        # åˆ›å»ºæ–°æ–‡ä»¶
        now = datetime.now()
        timestamp = now.strftime("%H%M%S")
        filename = f"session-{timestamp}-{session_id}.jsonl"
        session_data["file_path"] = str(get_session_dir() / filename)

    file_path = Path(session_data["file_path"])

    # å†™å…¥ä¼šè¯æ•°æ® (å®Œæ•´è¦†ç›–)
    with open(file_path, "w", encoding="utf-8") as f:
        # ç¬¬ä¸€è¡Œ: ä¼šè¯å…ƒæ•°æ®
        meta = {
            "type": "meta",
            "session_id": session_id,
            "created_at": session_data["created_at"],
            "title": session_data.get("title", "æ–°å¯¹è¯"),
            "updated_at": time.time()
        }
        f.write(json.dumps(meta, ensure_ascii=False) + "\n")

        # åç»­è¡Œ: æ¶ˆæ¯
        for msg in session_data["messages"]:
            item = {"type": "message", **msg}
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

def load_session_from_disk(file_path):
    """ä» JSONL æ–‡ä»¶åŠ è½½ä¼šè¯"""
    file_path = Path(file_path)
    if not file_path.exists():
        return None

    session_data = {"messages": [], "file_path": str(file_path)}

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            item = json.loads(line)
            if item.get("type") == "meta":
                session_data["created_at"] = item.get("created_at", time.time())
                session_data["title"] = item.get("title", "æ–°å¯¹è¯")
                session_data["session_id"] = item.get("session_id")
            elif item.get("type") == "message":
                del item["type"]
                session_data["messages"].append(item)

    return session_data

def append_to_history(session_id, text):
    """è¿½åŠ åˆ°å…¨å±€å†å²è®°å½•"""
    entry = {
        "session_id": session_id,
        "ts": int(time.time()),
        "text": text[:100]  # åªä¿å­˜å‰100å­—ç¬¦ä½œä¸ºæ‘˜è¦
    }
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def list_all_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯æ–‡ä»¶"""
    session_list = []
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        try:
            with open(jsonl_file, "r", encoding="utf-8") as f:
                first_line = f.readline().strip()
                if first_line:
                    meta = json.loads(first_line)
                    if meta.get("type") == "meta":
                        session_list.append({
                            "session_id": meta.get("session_id"),
                            "title": meta.get("title", "æ–°å¯¹è¯"),
                            "created_at": meta.get("created_at", 0),
                            "updated_at": meta.get("updated_at", 0),
                            "file_path": str(jsonl_file)
                        })
        except Exception as e:
            print(f"[Warning] æ— æ³•è¯»å–ä¼šè¯æ–‡ä»¶ {jsonl_file}: {e}")

    # æŒ‰æ›´æ–°æ—¶é—´å€’åº
    session_list.sort(key=lambda x: x.get("updated_at", 0), reverse=True)
    return session_list[:50]  # æœ€å¤šè¿”å›50ä¸ª

def get_memory_usage():
    process = psutil.Process(os.getpid())
    mem_gb = process.memory_info().rss / 1024 / 1024 / 1024
    return round(mem_gb, 2)


def request_sudo_permission():
    """
    å¯åŠ¨æ—¶è¯·æ±‚ sudo æƒé™ï¼ˆç”¨æˆ·è¾“å…¥ä¸€æ¬¡å¯†ç ï¼‰
    å¹¶å¯åŠ¨åå°çº¿ç¨‹å®šæœŸåˆ·æ–°å‡­è¯
    """
    global sudo_authorized, sudo_refresh_thread

    if platform.system() != "Darwin":
        return False

    print("\n" + "=" * 60)
    print("ğŸ” è¯·æ±‚ç¡¬ä»¶ç›‘æ§æƒé™ (ç”¨äºè·å– GPU æ¸©åº¦)")
    print("   è¯·è¾“å…¥æ‚¨çš„ macOS å¯†ç ï¼ˆå¯é€‰ï¼ŒæŒ‰ Ctrl+C è·³è¿‡ï¼‰")
    print("=" * 60)

    try:
        # è¯·æ±‚ sudo æƒé™
        result = subprocess.run(
            ["sudo", "-v"],
            timeout=60  # ç»™ç”¨æˆ·60ç§’è¾“å…¥å¯†ç 
        )
        if result.returncode == 0:
            sudo_authorized = True
            print("âœ… æƒé™æˆæƒæˆåŠŸï¼GPU æ¸©åº¦ç›‘æ§å·²å¯ç”¨")

            # å¯åŠ¨åå°çº¿ç¨‹å®šæœŸåˆ·æ–° sudo å‡­è¯
            def refresh_sudo():
                while sudo_authorized:
                    time.sleep(240)  # æ¯4åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡ï¼ˆsudo é»˜è®¤5åˆ†é’Ÿè¶…æ—¶ï¼‰
                    try:
                        subprocess.run(["sudo", "-v"], capture_output=True, timeout=5)
                    except:
                        pass

            sudo_refresh_thread = threading.Thread(target=refresh_sudo, daemon=True)
            sudo_refresh_thread.start()
            return True
        else:
            print("âš ï¸  æƒé™æœªæˆæƒï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
            return False
    except subprocess.TimeoutExpired:
        print("âš ï¸  æˆæƒè¶…æ—¶ï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
        return False
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²è·³è¿‡æƒé™æˆæƒï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
        return False
    except Exception as e:
        print(f"âš ï¸  æˆæƒå¤±è´¥: {e}")
        return False


def get_hardware_stats():
    """è·å–ç¡¬ä»¶ç›‘æ§ä¿¡æ¯ (GPUä½¿ç”¨ç‡ã€æ˜¾å­˜ã€æ¸©åº¦ç­‰)"""
    hw_stats = {
        "gpu_usage": "-",
        "vram_usage": "-",
        "gpu_temp": "-",
        "memory_usage": "-",
        "cpu_usage": "-"
    }

    try:
        # CPU ä½¿ç”¨ç‡ (ä¸ä½¿ç”¨ interval é¿å…é˜»å¡)
        hw_stats["cpu_usage"] = f"{psutil.cpu_percent():.1f}%"

        # å†…å­˜ä½¿ç”¨
        mem = psutil.virtual_memory()
        hw_stats["memory_usage"] = f"{mem.used / 1024**3:.1f} GB / {mem.total / 1024**3:.1f} GB"

        system = platform.system()

        if system == "Darwin":  # macOS
            try:
                # æ£€æµ‹ MPS çŠ¶æ€
                if torch.backends.mps.is_available():
                    hw_stats["gpu_usage"] = "MPS è¿è¡Œä¸­"
                else:
                    hw_stats["gpu_usage"] = "CPU æ¨¡å¼"

                # macOS ç»Ÿä¸€å†…å­˜æ¶æ„ï¼Œæ˜¾å­˜å’Œå†…å­˜å…±äº«
                hw_stats["vram_usage"] = "ç»Ÿä¸€å†…å­˜"

                # å°è¯•é€šè¿‡ powermetrics è·å– GPU æ¸©åº¦ (éœ€è¦ sudo æƒé™)
                if sudo_authorized:
                    try:
                        result = subprocess.run(
                            ["sudo", "powermetrics", "--samplers", "smc", "-i", "1", "-n", "1"],
                            capture_output=True, text=True, timeout=3
                        )
                        if result.returncode == 0:
                            output = result.stdout
                            # æŸ¥æ‰¾ GPU æ¸©åº¦ç›¸å…³è¡Œ
                            for line in output.split("\n"):
                                if "GPU" in line and "die" in line.lower():
                                    parts = line.split(":")
                                    if len(parts) >= 2:
                                        temp_str = parts[1].strip().replace("C", "").strip()
                                        try:
                                            temp = float(temp_str)
                                            hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                            break
                                        except:
                                            pass
                                elif "GPU" in line and "temp" in line.lower():
                                    parts = line.split(":")
                                    if len(parts) >= 2:
                                        temp_str = parts[1].strip().replace("C", "").strip()
                                        try:
                                            temp = float(temp_str)
                                            hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                            break
                                        except:
                                            pass

                            # å¦‚æœæ²¡æ‰¾åˆ° GPU æ¸©åº¦ï¼Œå°è¯•æ‰¾ SOC æ¸©åº¦
                            if hw_stats["gpu_temp"] == "-":
                                for line in output.split("\n"):
                                    if "SOC" in line and "temp" in line.lower():
                                        parts = line.split(":")
                                        if len(parts) >= 2:
                                            temp_str = parts[1].strip().replace("C", "").strip()
                                            try:
                                                temp = float(temp_str)
                                                hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                                break
                                            except:
                                                pass
                    except subprocess.TimeoutExpired:
                        hw_stats["gpu_temp"] = "è¶…æ—¶"
                    except Exception:
                        hw_stats["gpu_temp"] = "è·å–å¤±è´¥"
                else:
                    hw_stats["gpu_temp"] = "æœªæˆæƒ"

            except Exception as e:
                print(f"[DEBUG] macOS GPU info error: {e}")

        elif system == "Linux":
            # NVIDIA GPU (ä½¿ç”¨ nvidia-smi)
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")
                    if lines:
                        # å¯èƒ½æœ‰å¤šä¸ª GPUï¼Œå–ç¬¬ä¸€ä¸ª
                        parts = lines[0].split(",")
                        if len(parts) >= 4:
                            gpu_util = parts[0].strip()
                            mem_used = float(parts[1].strip())
                            mem_total = float(parts[2].strip())
                            temp = parts[3].strip()

                            hw_stats["gpu_usage"] = f"{gpu_util}%"
                            hw_stats["vram_usage"] = f"{mem_used/1024:.1f} GB / {mem_total/1024:.1f} GB"
                            hw_stats["gpu_temp"] = f"{temp}Â°C"
            except FileNotFoundError:
                # nvidia-smi ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ AMD æˆ–æ—  GPU
                try:
                    # å°è¯• AMD GPU (rocm-smi)
                    result = subprocess.run(
                        ["rocm-smi", "--showuse", "--showtemp", "--showmeminfo", "vram"],
                        capture_output=True, text=True, timeout=5
                    )
                    if result.returncode == 0:
                        hw_stats["gpu_usage"] = "AMD GPU"
                except:
                    pass
            except Exception as e:
                print(f"[DEBUG] Linux GPU info error: {e}")

        elif system == "Windows":
            # Windows NVIDIA GPU
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5, shell=True
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")
                    if lines:
                        parts = lines[0].split(",")
                        if len(parts) >= 4:
                            gpu_util = parts[0].strip()
                            mem_used = float(parts[1].strip())
                            mem_total = float(parts[2].strip())
                            temp = parts[3].strip()

                            hw_stats["gpu_usage"] = f"{gpu_util}%"
                            hw_stats["vram_usage"] = f"{mem_used/1024:.1f} GB / {mem_total/1024:.1f} GB"
                            hw_stats["gpu_temp"] = f"{temp}Â°C"
            except Exception as e:
                print(f"[DEBUG] Windows GPU info error: {e}")

    except Exception as e:
        print(f"[DEBUG] Hardware stats error: {e}")

    return hw_stats

def cleanup_old_sessions():
    """æ¸…ç†å†…å­˜ä¸­çš„æ—§ä¼šè¯ç¼“å­˜ (ç£ç›˜æ–‡ä»¶ä¿ç•™)"""
    if len(sessions) > MAX_SESSIONS:
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä»å†…å­˜ä¸­ç§»é™¤æœ€è€çš„
        sorted_sessions = sorted(sessions.items(), key=lambda x: x[1]["created_at"])
        for sid, _ in sorted_sessions[:len(sessions) - MAX_SESSIONS]:
            del sessions[sid]

def load_model():
    """
    åŠ è½½ PyTorch æ¨¡å‹ (ä»… MPS æ¨¡å¼éœ€è¦)
    mmproj æ¨¡å¼ä¸‹è·³è¿‡ï¼Œä½¿ç”¨ llama.cpp
    """
    global model, processor, model_loaded, model_info, dummy_image

    if model_loaded:
        return True

    # mmproj æ¨¡å¼ä¸éœ€è¦åŠ è½½ PyTorch æ¨¡å‹
    if DEFAULT_BACKEND == "mmproj":
        print("=" * 60)
        print("ä½¿ç”¨ llama.cpp å¤šæ¨¡æ€åç«¯ (mmproj)")
        print("è·³è¿‡ PyTorch æ¨¡å‹åŠ è½½")
        print("=" * 60)
        model_info = {
            "name": "Gemma 3N (llama.cpp)",
            "params": "2B",
            "dtype": "Q4_K_M",
            "device": "GPU (Metal)",
            "load_time": 0,
            "memory_gb": 0,
            "capabilities": ["æ–‡æœ¬å¯¹è¯", "å›¾åƒç†è§£", "éŸ³é¢‘è½¬å½•", "å¤šè½®å¯¹è¯"],
            "max_tokens": 8192,
            "backend": "llama.cpp mmproj"
        }
        model_loaded = True
        return True

    # MPS æ¨¡å¼éœ€è¦ PyTorch
    if not PYTORCH_AVAILABLE:
        print("=" * 60)
        print("é”™è¯¯: MPS æ¨¡å¼éœ€è¦å®‰è£… PyTorch")
        print("è¯·è¿è¡Œ: pip install torch transformers librosa")
        print("æˆ–åˆ‡æ¢åˆ° mmproj æ¨¡å¼: export GEMMA3N_BACKEND=mmproj")
        print("=" * 60)
        return False

    print("=" * 60)
    print("åŠ è½½ AI å¤šæ¨¡æ€æ¨¡å‹ (PyTorch MPS)...")
    print("=" * 60)

    model_name = "google/gemma-3n-E2B-it"
    load_start = time.time()

    print("[1/2] åŠ è½½å¤„ç†å™¨...")
    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

    print("[2/2] åŠ è½½æ¨¡å‹...")
    model = Gemma3nForConditionalGeneration.from_pretrained(
        model_name,
        device_map="auto",
        max_memory={"mps": "64GiB", "cpu": "64GiB"},
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model.eval()

    dummy_image = Image.new('RGB', (64, 64), color='white')
    load_time = time.time() - load_start
    total_params = sum(p.numel() for p in model.parameters())

    model_info = {
        "name": "AI Multimodal",
        "params": f"{total_params / 1e9:.2f}B",
        "dtype": "bfloat16",
        "device": str(model.device),
        "load_time": round(load_time, 2),
        "memory_gb": get_memory_usage(),
        "capabilities": ["æ–‡æœ¬å¯¹è¯", "å›¾åƒç†è§£", "éŸ³é¢‘è½¬å½•", "å¤šè½®å¯¹è¯"],
        "max_tokens": 8192,
    }

    model_loaded = True
    print("=" * 60)
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶ {load_time:.2f}s")
    print(f"å†…å­˜å ç”¨: {model_info['memory_gb']} GB")
    print("=" * 60)
    return True

def start_llama_mmproj_server():
    """å¯åŠ¨ llama-server with mmproj ä½œä¸ºæŒä¹…åŒ–å¤šæ¨¡æ€æ¨ç†æœåŠ¡"""
    global llama_mmproj_server_process, llama_mmproj_server_ready

    if not Path(LLAMA_SERVER_BIN).exists():
        print(f"[llama-mmproj-server] äºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_SERVER_BIN}")
        return False

    if not Path(LLAMA_MM_MODEL).exists():
        print(f"[llama-mmproj-server] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_MM_MODEL}")
        return False

    if not Path(LLAMA_MM_PROJ_IMAGE).exists():
        print(f"[llama-mmproj-server] mmproj æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_MM_PROJ_IMAGE}")
        return False

    # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿è¡Œ
    try:
        import requests
        resp = requests.get(f"http://127.0.0.1:{LLAMA_MMPROJ_SERVER_PORT}/health", timeout=2)
        if resp.status_code == 200:
            print(f"[llama-mmproj-server] å·²åœ¨ç«¯å£ {LLAMA_MMPROJ_SERVER_PORT} è¿è¡Œ")
            llama_mmproj_server_ready = True
            return True
    except:
        pass

    print(f"[llama-mmproj-server] å¯åŠ¨ä¸­... ç«¯å£: {LLAMA_MMPROJ_SERVER_PORT}")
    print(f"[llama-mmproj-server] æ¨¡å‹: {LLAMA_MM_MODEL}")
    print(f"[llama-mmproj-server] mmproj: {LLAMA_MM_PROJ_IMAGE}")

    env = os.environ.copy()
    bin_dir = str(Path(LLAMA_SERVER_BIN).parent)
    env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"

    cmd = [
        LLAMA_SERVER_BIN,
        "-m", LLAMA_MM_MODEL,
        "--mmproj", LLAMA_MM_PROJ_IMAGE,
        "--port", str(LLAMA_MMPROJ_SERVER_PORT),
        "--host", "127.0.0.1",
        "-ngl", "999",
        "-t", "8",
        "--ctx-size", "4096",
    ]

    llama_mmproj_server_process = subprocess.Popen(
        cmd,
        env=env,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    import requests
    for _ in range(60):  # æœ€å¤šç­‰å¾… 60 ç§’ (é¦–æ¬¡åŠ è½½æ¨¡å‹å¯èƒ½è¾ƒæ…¢)
        try:
            resp = requests.get(f"http://127.0.0.1:{LLAMA_MMPROJ_SERVER_PORT}/health", timeout=1)
            if resp.status_code == 200:
                print(f"[llama-mmproj-server] å¯åŠ¨æˆåŠŸï¼")
                llama_mmproj_server_ready = True
                return True
        except:
            pass
        time.sleep(1)

    print("[llama-mmproj-server] å¯åŠ¨è¶…æ—¶")
    return False


def run_llama_mmproj(prompt, image_path=None, audio_path=None,
                     messages_history=None, session_id=None, has_media=False):
    """
    ä½¿ç”¨ llama-server with mmproj ç”Ÿæˆå›å¤
    ä¼˜å…ˆä½¿ç”¨æŒä¹…åŒ–æœåŠ¡ (16x æ›´å¿«)ï¼Œå¤±è´¥æ—¶å›é€€åˆ° CLI

    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        image_path: å›¾ç‰‡è·¯å¾„ (å•ä¸ªå­—ç¬¦ä¸²æˆ–è·¯å¾„åˆ—è¡¨)
        audio_path: éŸ³é¢‘è·¯å¾„ (å•ä¸ªå­—ç¬¦ä¸²æˆ–è·¯å¾„åˆ—è¡¨)
        messages_history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "text": "..."}]
        session_id: ä¼šè¯ID (ç”¨äº thought signature)
        has_media: å½“å‰æ¶ˆæ¯æ˜¯å¦åŒ…å«åª’ä½“
    """
    global llama_mmproj_server_ready

    # æ ‡å‡†åŒ–è·¯å¾„ä¸ºåˆ—è¡¨
    image_paths = []
    if image_path:
        if isinstance(image_path, str):
            image_paths = [image_path]
        else:
            image_paths = list(image_path)

    audio_paths = []
    if audio_path:
        if isinstance(audio_path, str):
            audio_paths = [audio_path]
        else:
            audio_paths = list(audio_path)

    # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„å®Œæ•´ prompt
    full_prompt = _build_mmproj_prompt(
        prompt, messages_history, session_id, has_media
    )

    # éŸ³é¢‘æš‚ä¸æ”¯æŒ server æ¨¡å¼ï¼Œå›é€€åˆ° CLI
    # å¤šå›¾ç‰‡ä¹Ÿä½¿ç”¨ CLI (llama-server å•æ¬¡è¯·æ±‚åªæ”¯æŒä¸€å¼ å›¾)
    if audio_paths or len(image_paths) > 1:
        return run_llama_mmproj_cli(full_prompt, image_paths, audio_paths)

    # å°è¯•ä½¿ç”¨ server æ¨¡å¼ (å•å›¾ç‰‡æƒ…å†µ)
    if not llama_mmproj_server_ready:
        if not start_llama_mmproj_server():
            print("[mmproj] Server å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ° CLI æ¨¡å¼")
            return run_llama_mmproj_cli(full_prompt, image_paths, audio_paths)

    start = time.time()
    try:
        import requests

        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content = []
        if image_paths:
            # å°†å›¾ç‰‡è½¬ä¸º base64 (server æ¨¡å¼åªå¤„ç†ç¬¬ä¸€å¼ )
            with open(image_paths[0], "rb") as f:
                img_b64 = base64.b64encode(f.read()).decode("utf-8")
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}
            })
        content.append({"type": "text", "text": full_prompt})

        resp = requests.post(
            f"http://127.0.0.1:{LLAMA_MMPROJ_SERVER_PORT}/v1/chat/completions",
            json={
                "model": "gemma-3n",
                "messages": [{"role": "user", "content": content}],
                "max_tokens": LLAMA_MM_N_PREDICT,
                "temperature": 0.7,
            },
            timeout=120
        )

        elapsed = time.time() - start

        if resp.status_code != 200:
            # æœåŠ¡å™¨å¯èƒ½å‡ºé”™ï¼Œå›é€€åˆ° CLI
            llama_mmproj_server_ready = False
            return run_llama_mmproj_cli(full_prompt, image_paths, audio_paths)

        data = resp.json()
        response = data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()

        # ä» usage è·å– token æ•°
        usage = data.get("usage", {})
        output_tokens = usage.get("completion_tokens", len(response) // 4)
        speed = output_tokens / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "tokens": output_tokens,
                "backend": "mmproj-server"
            }
        }
    except Exception as e:
        # æœåŠ¡å™¨å¯èƒ½æŒ‚äº†ï¼Œæ ‡è®°ä¸ºä¸å¯ç”¨å¹¶å›é€€åˆ° CLI
        print(f"[mmproj] Server è¯·æ±‚å¤±è´¥: {e}ï¼Œå›é€€åˆ° CLI æ¨¡å¼")
        llama_mmproj_server_ready = False
        return run_llama_mmproj_cli(full_prompt, image_paths, audio_paths)


def _build_mmproj_prompt(prompt, messages_history=None, session_id=None, has_media=False):
    """
    æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡å’Œåª’ä½“ç†è§£çš„å®Œæ•´ prompt

    ç­–ç•¥:
    - å¦‚æœæœ‰æ–°åª’ä½“: ä¸æ³¨å…¥å†å²ä¸Šä¸‹æ–‡ï¼Œè®©æ¨¡å‹ä¸“æ³¨äºå½“å‰åª’ä½“
    - å¦‚æœæ²¡æœ‰æ–°åª’ä½“: æ³¨å…¥å†å²å¯¹è¯ + åª’ä½“ç†è§£ (thought signature)
    """
    if has_media:
        # æœ‰æ–°åª’ä½“æ—¶ï¼Œç›´æ¥è¿”å›åŸå§‹ prompt
        print(f"[mmproj] æœ‰æ–°åª’ä½“ï¼Œè·³è¿‡å†å²ä¸Šä¸‹æ–‡æ³¨å…¥")
        return prompt

    context_parts = []

    # 1. è·å–åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†)
    if session_id:
        media_context = get_session_media_context(session_id)
        if media_context:
            context_parts.append(f"[Previous Media Understanding]\n{media_context}")
            print(f"[mmproj] æ³¨å…¥åª’ä½“ç†è§£ä¸Šä¸‹æ–‡: {len(media_context)} å­—ç¬¦")

    # 2. è·å–å†å²å¯¹è¯ä¸Šä¸‹æ–‡
    if messages_history:
        history_parts = []
        for msg in messages_history[-MAX_HISTORY_TURNS * 2:]:
            role = "User" if msg["role"] == "user" else "Assistant"
            history_parts.append(f"{role}: {msg['text']}")
        if history_parts:
            history_context = "\n".join(history_parts)
            context_parts.append(f"[Previous Conversation]\n{history_context}")
            print(f"[mmproj] æ³¨å…¥å†å²å¯¹è¯: {len(history_parts)} æ¡æ¶ˆæ¯")

    # 3. æ„å»ºå®Œæ•´ prompt
    if context_parts:
        context_str = "\n\n".join(context_parts)
        full_prompt = f"""{context_str}

[Current Message]: {prompt}

Please respond to the current message, taking into account the context above."""
        return full_prompt
    else:
        return prompt


def run_llama_mmproj_cli(prompt, image_paths=None, audio_paths=None):
    """
    ä½¿ç”¨ llama-mtmd-cli ç”Ÿæˆå›å¤ (æ”¯æŒå¤šå›¾ç‰‡/å¤šéŸ³é¢‘)

    æ³¨æ„: llama.cpp ç›®å‰ä¸æ”¯æŒåŒæ—¶åŠ è½½è§†è§‰å’ŒéŸ³é¢‘ projector (Metal bug)
    è§£å†³æ–¹æ¡ˆ: åˆ†ä¸¤æ¬¡è°ƒç”¨ï¼Œå…ˆå¤„ç†å›¾ç‰‡ï¼Œå†å¤„ç†éŸ³é¢‘

    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        audio_paths: éŸ³é¢‘è·¯å¾„åˆ—è¡¨
    """
    if not Path(LLAMA_MTMD_BIN).exists():
        return {"error": f"llama-mtmd-cli ä¸å­˜åœ¨: {LLAMA_MTMD_BIN}"}
    if not Path(LLAMA_MM_MODEL).exists():
        return {"error": f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_MM_MODEL}"}

    # æ ‡å‡†åŒ–ä¸ºåˆ—è¡¨
    if image_paths is None:
        image_paths = []
    elif isinstance(image_paths, str):
        image_paths = [image_paths]

    if audio_paths is None:
        audio_paths = []
    elif isinstance(audio_paths, str):
        audio_paths = [audio_paths]

    print(f"[DEBUG] mmproj CLI: {len(image_paths)} å¼ å›¾ç‰‡, {len(audio_paths)} ä¸ªéŸ³é¢‘")

    # llama.cpp ä¸æ”¯æŒåŒæ—¶åŠ è½½è§†è§‰å’ŒéŸ³é¢‘ projector (Metal bug)
    # è§£å†³æ–¹æ¡ˆ: åˆ†ä¸¤æ¬¡å¤„ç†
    if image_paths and audio_paths:
        print("[DEBUG] åŒæ—¶æœ‰å›¾ç‰‡å’ŒéŸ³é¢‘ï¼Œåˆ†ä¸¤æ¬¡å¤„ç†...")

        # ç¬¬ä¸€æ¬¡: å¤„ç†å›¾ç‰‡
        image_result = _run_mmproj_single(prompt + " (Focus on describing the images)", image_paths, None)
        if "error" in image_result:
            return image_result

        # ç¬¬äºŒæ¬¡: å¤„ç†éŸ³é¢‘
        audio_result = _run_mmproj_single("Transcribe the audio content", None, audio_paths)
        if "error" in audio_result:
            return audio_result

        # åˆå¹¶ç»“æœ
        combined_response = f"**å›¾åƒåˆ†æ:**\n{image_result['response']}\n\n**éŸ³é¢‘è½¬å½•:**\n{audio_result['response']}"
        total_time = image_result['metrics']['total_time'] + audio_result['metrics']['total_time']

        return {
            "response": combined_response,
            "metrics": {
                "total_time": round(total_time, 2),
                "speed": round((image_result['metrics'].get('speed', 0) + audio_result['metrics'].get('speed', 0)) / 2, 1),
                "backend": "mmproj-cli (split)",
                "images": len(image_paths),
                "audios": len(audio_paths)
            }
        }
    else:
        # åªæœ‰å›¾ç‰‡æˆ–åªæœ‰éŸ³é¢‘ï¼Œç›´æ¥å¤„ç†
        return _run_mmproj_single(prompt, image_paths, audio_paths)


def _run_mmproj_single(prompt, image_paths=None, audio_paths=None):
    """å•æ¬¡ mmproj CLI è°ƒç”¨ (åªå¤„ç†å›¾ç‰‡æˆ–åªå¤„ç†éŸ³é¢‘)"""
    image_paths = image_paths or []
    audio_paths = audio_paths or []

    # æ ¹æ®è¾“å…¥ç±»å‹åŠ¨æ€é€‰æ‹© mmproj
    mmproj_list = []
    if image_paths and LLAMA_MM_PROJ_IMAGE and Path(LLAMA_MM_PROJ_IMAGE).exists():
        mmproj_list.append(LLAMA_MM_PROJ_IMAGE)
    if audio_paths and LLAMA_MM_PROJ_AUDIO and Path(LLAMA_MM_PROJ_AUDIO).exists():
        mmproj_list.append(LLAMA_MM_PROJ_AUDIO)

    if not mmproj_list:
        if LLAMA_MM_PROJ and Path(LLAMA_MM_PROJ).exists():
            mmproj_list = [LLAMA_MM_PROJ]
        elif LLAMA_MM_PROJ_COMBINED:
            mmproj_list = [p.strip() for p in LLAMA_MM_PROJ_COMBINED.split(",") if p.strip() and Path(p.strip()).exists()]

    if not mmproj_list:
        return {"error": "æœªé…ç½®æœ‰æ•ˆçš„ mmproj è·¯å¾„"}

    mmproj_combined = ",".join(mmproj_list)
    print(f"[DEBUG] mmproj CLI ä½¿ç”¨: {mmproj_combined}")

    cmd = [
        LLAMA_MTMD_BIN,
        "--log-verbosity", "0",
        "--no-warmup",
        "-m", LLAMA_MM_MODEL,
        "--mmproj", mmproj_combined,
        "-p", prompt,
        "-n", str(LLAMA_MM_N_PREDICT),
        "--temp", "0.7",
    ]
    if image_paths:
        cmd += ["--image", ",".join(image_paths)]
    if audio_paths:
        cmd += ["--audio", ",".join(audio_paths)]

    start = time.time()
    try:
        env = os.environ.copy()
        bin_dir = str(Path(LLAMA_MTMD_BIN).parent)
        env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"
        out = subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=180, env=env)
        elapsed = time.time() - start
        lines = [ln for ln in out.stdout.splitlines() if ln.strip()]
        content_lines = [
            ln for ln in lines
            if not ln.startswith(("ggml", "AVX", "gguf", "llama", "clip", "Using", "model", "warmup", "load"))
        ]
        response = "\n".join(content_lines).strip() if content_lines else "\n".join(lines).strip()

        output_tokens = len(response) // 4
        speed = output_tokens / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "backend": "mmproj-cli",
                "images": len(image_paths),
                "audios": len(audio_paths)
            }
        }
    except subprocess.TimeoutExpired:
        return {"error": "llama-mtmd-cli è¶…æ—¶"}
    except subprocess.CalledProcessError as e:
        return {"error": f"llama-mtmd-cli å¤±è´¥: {e.stderr or e.stdout}"}


def run_llama_run(prompt, history_context=""):
    """
    ä½¿ç”¨ llama-run è¿›è¡Œçº¯æ–‡æœ¬æ¨ç†
    è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ llama.cpp åç«¯ï¼Œä¸éœ€è¦åŠ è½½ PyTorch æ¨¡å‹
    """
    if not Path(LLAMA_RUN_BIN).exists():
        return {"error": f"llama-run ä¸å­˜åœ¨: {LLAMA_RUN_BIN}"}
    if not LLAMA_RUN_MODEL or not Path(LLAMA_RUN_MODEL).exists():
        return {"error": f"GGUF æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_RUN_MODEL}"}

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«å†å²ä¸Šä¸‹æ–‡ï¼‰
    full_prompt = prompt
    if history_context:
        full_prompt = f"{history_context}\n\nUser: {prompt}\n\nAssistant:"

    start = time.time()
    try:
        env = os.environ.copy()
        bin_dir = str(Path(LLAMA_RUN_BIN).parent)
        env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"

        cmd = [
            LLAMA_RUN_BIN,
            "--ngl", "999",  # ä½¿ç”¨ GPU åŠ é€Ÿ
            "--temp", "0.7",
            "-t", "8",  # ä½¿ç”¨ 8 ä¸ªçº¿ç¨‹
            LLAMA_RUN_MODEL,
            full_prompt
        ]

        out = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120,
            env=env
        )

        elapsed = time.time() - start

        if out.returncode != 0:
            return {"error": f"llama-run å¤±è´¥: {out.stderr}"}

        # llama-run çš„è¾“å‡ºæ˜¯å¹²å‡€çš„ï¼Œç›´æ¥ä½¿ç”¨
        # è¿‡æ»¤æ‰å¯èƒ½çš„ ANSI è½¬ä¹‰ç 
        response = out.stdout.strip()
        # ç§»é™¤ ANSI æ§åˆ¶å­—ç¬¦
        import re
        response = re.sub(r'\x1b\[[0-9;]*m', '', response)
        response = response.strip()

        # ä¼°ç®— token æ•°ï¼ˆç®€å•ä¼°ç®—ï¼šå­—ç¬¦æ•°/4ï¼‰
        output_tokens = len(response) // 4
        speed = output_tokens / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "backend": "llama.cpp"
            }
        }
    except subprocess.TimeoutExpired:
        return {"error": "llama-run è¶…æ—¶"}
    except Exception as e:
        return {"error": f"llama-run é”™è¯¯: {str(e)}"}


def start_llama_server():
    """å¯åŠ¨ llama-server ä½œä¸ºæŒä¹…åŒ–æ¨ç†æœåŠ¡"""
    global llama_server_process, llama_server_ready

    if not Path(LLAMA_SERVER_BIN).exists():
        print(f"[llama-server] äºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_SERVER_BIN}")
        return False

    if not LLAMA_RUN_MODEL or not Path(LLAMA_RUN_MODEL).exists():
        print(f"[llama-server] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {LLAMA_RUN_MODEL}")
        return False

    # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿è¡Œ
    try:
        import requests
        resp = requests.get(f"http://127.0.0.1:{LLAMA_SERVER_PORT}/health", timeout=2)
        if resp.status_code == 200:
            print(f"[llama-server] å·²åœ¨ç«¯å£ {LLAMA_SERVER_PORT} è¿è¡Œ")
            llama_server_ready = True
            return True
    except:
        pass

    print(f"[llama-server] å¯åŠ¨ä¸­... ç«¯å£: {LLAMA_SERVER_PORT}")
    print(f"[llama-server] æ¨¡å‹: {LLAMA_RUN_MODEL}")

    env = os.environ.copy()
    bin_dir = str(Path(LLAMA_SERVER_BIN).parent)
    env["DYLD_LIBRARY_PATH"] = f"{bin_dir}:{env.get('DYLD_LIBRARY_PATH', '')}"

    cmd = [
        LLAMA_SERVER_BIN,
        "-m", LLAMA_RUN_MODEL,
        "--port", str(LLAMA_SERVER_PORT),
        "--host", "127.0.0.1",
        "-ngl", "999",
        "-t", "8",
        "--ctx-size", "4096",
        "--flash-attn", "on",
    ]

    llama_server_process = subprocess.Popen(
        cmd,
        env=env,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    import requests
    for _ in range(30):  # æœ€å¤šç­‰å¾… 30 ç§’
        try:
            resp = requests.get(f"http://127.0.0.1:{LLAMA_SERVER_PORT}/health", timeout=1)
            if resp.status_code == 200:
                print(f"[llama-server] å¯åŠ¨æˆåŠŸï¼")
                llama_server_ready = True
                return True
        except:
            pass
        time.sleep(1)

    print("[llama-server] å¯åŠ¨è¶…æ—¶")
    return False


def query_llama_server(prompt, history_context=""):
    """
    é€šè¿‡ llama-server API è¿›è¡Œæ¨ç†
    é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºæ¨¡å‹å·²ç»é¢„åŠ è½½
    """
    global llama_server_ready

    if not llama_server_ready:
        if not start_llama_server():
            # å›é€€åˆ° llama-run
            return run_llama_run(prompt, history_context)

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯
    full_prompt = prompt
    if history_context:
        full_prompt = f"{history_context}\n\nUser: {prompt}\n\nAssistant:"

    start = time.time()
    try:
        import requests
        resp = requests.post(
            f"http://127.0.0.1:{LLAMA_SERVER_PORT}/completion",
            json={
                "prompt": full_prompt,
                "n_predict": 256,
                "temperature": 0.7,
                "stop": ["</s>", "<eos>", "\n\nUser:", "\nUser:"],
                "stream": False,
            },
            timeout=60
        )

        elapsed = time.time() - start

        if resp.status_code != 200:
            return {"error": f"llama-server è¯·æ±‚å¤±è´¥: {resp.status_code}"}

        data = resp.json()
        response = data.get("content", "").strip()
        tokens_predicted = data.get("tokens_predicted", len(response) // 4)

        # ä» timings è·å–çœŸå®é€Ÿåº¦
        timings = data.get("timings", {})
        speed = timings.get("predicted_per_second", 0)
        if speed == 0:
            speed = tokens_predicted / elapsed if elapsed > 0 else 0

        return {
            "response": response,
            "metrics": {
                "total_time": round(elapsed, 2),
                "speed": round(speed, 1),
                "tokens": tokens_predicted,
                "backend": "llama-server"
            }
        }
    except Exception as e:
        # æœåŠ¡å™¨å¯èƒ½æŒ‚äº†ï¼Œå°è¯•é‡å¯
        llama_server_ready = False
        return {"error": f"llama-server é”™è¯¯: {str(e)}"}

def generate_response(messages_history, current_content, session_id=None, has_media=False, media_type=None):
    """
    ç”Ÿæˆå›å¤
    messages_history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "text": "..."}]
    current_content: å½“å‰æ¶ˆæ¯çš„contentåˆ—è¡¨
    session_id: ä¼šè¯ID (ç”¨äº thought signature)
    has_media: å½“å‰æ¶ˆæ¯æ˜¯å¦åŒ…å«åª’ä½“ (å›¾ç‰‡/éŸ³é¢‘)
    media_type: å½“å‰åª’ä½“ç±»å‹ ("image", "audio", None)

    æ³¨æ„: Gemma 3n å¤„ç†å™¨è¦æ±‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å›¾ç‰‡ï¼Œå¦åˆ™ä¼šæŠ¥æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´é”™è¯¯ã€‚
    è§£å†³æ–¹æ¡ˆ: å°†å†å²å¯¹è¯åˆå¹¶æˆä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œåªå‘é€ä¸€æ¡åŒ…å«å›¾ç‰‡çš„å½“å‰æ¶ˆæ¯ã€‚
    """
    global stats

    if not model_loaded:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}

    start_time = time.time()
    history_turns = len(messages_history) // 2  # è®°å½•å®é™…çš„å†å²è½®æ¬¡

    # è·å–å†å²åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†)
    # ç­–ç•¥ï¼š
    # - å¦‚æœå½“å‰æ¶ˆæ¯æœ‰æ–°åª’ä½“ï¼šä¸æ³¨å…¥å†å²åª’ä½“ç†è§£ï¼ˆé¿å…å¹²æ‰°ï¼‰
    # - å¦‚æœå½“å‰æ¶ˆæ¯æ²¡æœ‰æ–°åª’ä½“ï¼šæ³¨å…¥å†å²åª’ä½“ç†è§£ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå›ç­”å…³äºä¹‹å‰åª’ä½“çš„é—®é¢˜
    media_context = ""
    if session_id and not has_media:
        # åªåœ¨æ²¡æœ‰æ–°åª’ä½“æ—¶æ‰æ³¨å…¥å†å²åª’ä½“ç†è§£
        media_context = get_session_media_context(session_id)
        if media_context:
            print(f"[DEBUG] æ³¨å…¥åª’ä½“ç†è§£ä¸Šä¸‹æ–‡: {len(media_context)} å­—ç¬¦")
    elif has_media:
        print(f"[DEBUG] å½“å‰æœ‰æ–°{media_type or 'åª’ä½“'}ï¼Œè·³è¿‡å†å²åª’ä½“ç†è§£æ³¨å…¥")

    # æ–¹æ¡ˆ: å°†å†å²å¯¹è¯åˆå¹¶æˆä¸Šä¸‹æ–‡æ–‡æœ¬
    # è¿™æ ·å°±åªæœ‰ä¸€æ¡ user æ¶ˆæ¯ï¼Œé¿å…æ‰¹æ¬¡ä¸ä¸€è‡´é—®é¢˜
    # æ³¨æ„: å½“æœ‰æ–°åª’ä½“æ—¶ï¼Œä¸æ³¨å…¥å†å²å¯¹è¯ï¼Œé¿å…å¹²æ‰°æ¨¡å‹å¯¹å½“å‰åª’ä½“çš„ç†è§£
    history_context = ""
    if messages_history and not has_media:
        history_parts = []
        for msg in messages_history[-MAX_HISTORY_TURNS * 2:]:
            role = "User" if msg["role"] == "user" else "Assistant"
            history_parts.append(f"{role}: {msg['text']}")
        history_context = "\n".join(history_parts)
    elif has_media:
        print(f"[DEBUG] å½“å‰æœ‰æ–°åª’ä½“ï¼Œè·³è¿‡å†å²å¯¹è¯æ³¨å…¥")

    # æ„å»ºå•æ¡æ¶ˆæ¯ (åŒ…å«å†å²ä¸Šä¸‹æ–‡ + å½“å‰è¾“å…¥)
    messages = []

    # ä¿®æ”¹å½“å‰æ¶ˆæ¯å†…å®¹ï¼Œæ·»åŠ å†å²ä¸Šä¸‹æ–‡
    modified_content = []
    current_text = ""

    for item in current_content:
        if item.get("type") == "text":
            current_text = item.get("text", "")
        else:
            modified_content.append(item)

    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡æç¤º
    # åŒ…å«: åª’ä½“ç†è§£ + å†å²å¯¹è¯ + å½“å‰æ¶ˆæ¯
    full_context_parts = []

    # 1. åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†) - åªåœ¨æ— æ–°åª’ä½“æ—¶æ³¨å…¥
    if media_context:
        full_context_parts.append(f"[Previous Media Understanding]\n{media_context}")

    # 2. å†å²å¯¹è¯ä¸Šä¸‹æ–‡
    if history_context:
        full_context_parts.append(f"[Previous Conversation]\n{history_context}")

    # 3. å½“å‰æ¶ˆæ¯
    if full_context_parts:
        # æœ‰ä¸Šä¸‹æ–‡ï¼Œæ„å»ºå®Œæ•´æç¤º
        context_str = "\n\n".join(full_context_parts)
        context_prompt = f"""{context_str}

[Current Message]: {current_text}

Please respond to the current message, taking into account the context above."""
        modified_content.append({"type": "text", "text": context_prompt})
    else:
        # æ— ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½¿ç”¨å½“å‰æ–‡æœ¬
        modified_content.append({"type": "text", "text": current_text})

    messages.append({"role": "user", "content": modified_content})

    print(f"[DEBUG] æ¶ˆæ¯æ•°é‡: {len(messages)}, å†å²è½®æ¬¡: {history_turns}, æœ‰æ–°åª’ä½“: {has_media}")

    # å¤„ç†è¾“å…¥
    tokenize_start = time.time()
    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    )
    tokenize_time = time.time() - tokenize_start

    # å‡†å¤‡ç”Ÿæˆå‚æ•°
    input_ids = inputs["input_ids"].to(model.device)
    attention_mask = inputs["attention_mask"].to(model.device)
    input_tokens = input_ids.shape[1]

    print(f"[DEBUG] input_tokens: {input_tokens}")

    generate_kwargs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "max_new_tokens": 512,
        "do_sample": False,
    }

    # å¤„ç†å›¾åƒ
    if "pixel_values" in inputs and inputs["pixel_values"] is not None:
        generate_kwargs["pixel_values"] = inputs["pixel_values"].to(model.device, dtype=model.dtype)

    # å¤„ç†éŸ³é¢‘
    if "input_features" in inputs and inputs["input_features"] is not None:
        generate_kwargs["input_features"] = inputs["input_features"].to(model.device, dtype=model.dtype)
        generate_kwargs["input_features_mask"] = inputs["input_features_mask"].to(model.device)

    # ç”Ÿæˆ
    generate_start = time.time()
    with torch.inference_mode():
        outputs = model.generate(**generate_kwargs)
    generate_time = time.time() - generate_start

    # è§£ç 
    output_tokens = len(outputs[0]) - input_tokens
    response = processor.tokenizer.decode(
        outputs[0][input_ids.shape[1]:],
        skip_special_tokens=True
    )

    total_time = time.time() - start_time
    speed = output_tokens / generate_time if generate_time > 0 else 0

    # æ›´æ–°ç»Ÿè®¡
    stats["total_requests"] += 1
    stats["total_tokens"] += output_tokens
    stats["total_time"] += total_time
    stats["avg_speed"] = stats["total_tokens"] / stats["total_time"] if stats["total_time"] > 0 else 0

    return {
        "response": response,
        "metrics": {
            "total_time": round(total_time, 2),
            "tokenize_time": round(tokenize_time, 3),
            "generate_time": round(generate_time, 2),
            "input_tokens": int(input_tokens),
            "output_tokens": int(output_tokens),
            "speed": round(speed, 1),
            "history_turns": history_turns  # ä½¿ç”¨å®é™…çš„å†å²è½®æ¬¡
        }
    }

@app.route("/")
def index():
    return send_from_directory("static", "index.html")

@app.route("/chat")
def chat_page():
    return send_from_directory("static", "chat.html")

@app.route("/docs")
def docs_page():
    return send_from_directory("static", "docs.html")

@app.route("/api/status")
def status():
    hw_stats = get_hardware_stats() if model_loaded else {}
    mmproj_paths = [p.strip() for part in LLAMA_MM_PROJ_COMBINED.split(",") for p in part.split(";") if p.strip()]
    mmproj_files = {
        "bin": Path(LLAMA_MTMD_BIN).exists(),
        "model": Path(LLAMA_MM_MODEL).exists(),
        "proj": all(Path(p).exists() for p in mmproj_paths),
    }
    if LLAMA_MM_PROJ_IMAGE:
        mmproj_files["proj_image"] = Path(LLAMA_MM_PROJ_IMAGE).exists()
    if LLAMA_MM_PROJ_AUDIO:
        mmproj_files["proj_audio"] = Path(LLAMA_MM_PROJ_AUDIO).exists()

    # llama.cpp åç«¯çŠ¶æ€
    llama_cpp_ready = Path(LLAMA_RUN_BIN).exists() and LLAMA_RUN_MODEL and Path(LLAMA_RUN_MODEL).exists()
    llama_cpp_files = {
        "bin": Path(LLAMA_RUN_BIN).exists(),
        "model": bool(LLAMA_RUN_MODEL and Path(LLAMA_RUN_MODEL).exists()),
        "model_path": LLAMA_RUN_MODEL or "æœªæ‰¾åˆ°"
    }

    return jsonify({
        "loaded": model_loaded,
        "stats": stats,
        "memory_gb": get_memory_usage() if model_loaded else 0,
        "hardware": hw_stats,
        "active_sessions": len(sessions),
        "default_backend": DEFAULT_BACKEND,
        "mmproj_ready": all(mmproj_files.values()),
        "mmproj_files": mmproj_files,
        "llama_cpp_ready": llama_cpp_ready,
        "llama_cpp_files": llama_cpp_files,
    })

@app.route("/api/session/new", methods=["POST"])
def new_session():
    """åˆ›å»ºæ–°ä¼šè¯"""
    session_id = str(uuid.uuid4())[:8]
    session_data = {
        "messages": [],
        "created_at": time.time(),
        "title": "æ–°å¯¹è¯"
    }
    sessions[session_id] = session_data
    # ç«‹å³ä¿å­˜åˆ°ç£ç›˜
    save_session_to_disk(session_id, session_data)
    cleanup_old_sessions()
    return jsonify({"session_id": session_id})

@app.route("/api/session/list", methods=["GET"])
def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
    session_list = list_all_sessions()
    return jsonify({"sessions": session_list})

@app.route("/api/session/<session_id>/load", methods=["GET"])
def load_session(session_id):
    """åŠ è½½æŒ‡å®šä¼šè¯"""
    # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
    if session_id in sessions:
        session = sessions[session_id]
        return jsonify({
            "session_id": session_id,
            "messages": session["messages"],
            "title": session.get("title", "æ–°å¯¹è¯")
        })

    # ä»ç£ç›˜æŸ¥æ‰¾
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        if session_id in jsonl_file.name:
            session_data = load_session_from_disk(jsonl_file)
            if session_data:
                sessions[session_id] = session_data
                return jsonify({
                    "session_id": session_id,
                    "messages": session_data["messages"],
                    "title": session_data.get("title", "æ–°å¯¹è¯")
                })

    return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

@app.route("/api/session/<session_id>/delete", methods=["POST"])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    # ä»å†…å­˜ç§»é™¤
    if session_id in sessions:
        file_path = sessions[session_id].get("file_path")
        del sessions[session_id]
        # åˆ é™¤ç£ç›˜æ–‡ä»¶
        if file_path and Path(file_path).exists():
            Path(file_path).unlink()
            return jsonify({"success": True})

    # ä»ç£ç›˜æŸ¥æ‰¾å¹¶åˆ é™¤
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        if session_id in jsonl_file.name:
            jsonl_file.unlink()
            return jsonify({"success": True})

    return jsonify({"success": True})

@app.route("/api/session/<session_id>/clear", methods=["POST"])
def clear_session(session_id):
    """æ¸…ç©ºä¼šè¯å†å²"""
    if session_id in sessions:
        sessions[session_id]["messages"] = []
        sessions[session_id]["title"] = "æ–°å¯¹è¯"
        # ä¿å­˜åˆ°ç£ç›˜
        save_session_to_disk(session_id, sessions[session_id])
    return jsonify({"success": True})

@app.route("/api/session/<session_id>/history", methods=["GET"])
def get_history(session_id):
    """è·å–ä¼šè¯å†å²"""
    if session_id not in sessions:
        return jsonify({"messages": []})
    return jsonify({"messages": sessions[session_id]["messages"]})

@app.route("/api/thought/state/<session_id>", methods=["GET"])
def get_thought_state(session_id):
    """è·å–ä¼šè¯çš„ thought signature çŠ¶æ€"""
    state = thought_states.get(session_id, {"turn_index": 0, "media_refs": []})
    media_understandings = []

    for media_ref in state.get("media_refs", []):
        cached = media_understanding_cache.get(media_ref)
        if cached:
            media_understandings.append({
                "media_ref": media_ref,
                "turn_index": cached.get("turn_index", 0),
                "understanding_preview": cached.get("understanding", "")[:100] + "...",
                "created_at": cached.get("created_at", 0)
            })

    return jsonify({
        "session_id": session_id,
        "turn_index": state.get("turn_index", 0),
        "media_count": len(state.get("media_refs", [])),
        "media_understandings": media_understandings
    })

@app.route("/api/thought/stats", methods=["GET"])
def thought_stats():
    """è·å– thought signature å…¨å±€ç»Ÿè®¡"""
    return jsonify({
        "total_sessions": len(thought_states),
        "total_media_cached": len(media_understanding_cache),
        "sessions": [
            {
                "session_id": sid,
                "turn_index": state.get("turn_index", 0),
                "media_count": len(state.get("media_refs", []))
            }
            for sid, state in thought_states.items()
        ]
    })

@app.route("/api/chat", methods=["POST"])
def chat():
    try:
        data = request.json
        text = data.get("text", "")
        # æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªå›¾ç‰‡/éŸ³é¢‘ (å…¼å®¹æ—§API)
        image_data = data.get("image")  # å•ä¸ªå›¾ç‰‡ (å‘åå…¼å®¹)
        images_data = data.get("images", [])  # å¤šä¸ªå›¾ç‰‡ (æ–°API)
        audio_data = data.get("audio")  # å•ä¸ªéŸ³é¢‘ (å‘åå…¼å®¹)
        audios_data = data.get("audios", [])  # å¤šä¸ªéŸ³é¢‘ (æ–°API)
        session_id = data.get("session_id")
        backend = data.get("backend") or DEFAULT_BACKEND

        # åˆå¹¶å•ä¸ªå’Œå¤šä¸ªæ–‡ä»¶
        if image_data and image_data not in images_data:
            images_data = [image_data] + images_data
        if audio_data and audio_data not in audios_data:
            audios_data = [audio_data] + audios_data

        # é™åˆ¶æœ€å¤š14å¼ å›¾ç‰‡ï¼Œ10ä¸ªéŸ³é¢‘ (ä¸MPSæ¨¡å¼ä¿æŒä¸€è‡´)
        MAX_IMAGES = 14
        MAX_AUDIOS = 10
        if len(images_data) > MAX_IMAGES:
            return jsonify({"error": f"æœ€å¤šæ”¯æŒ {MAX_IMAGES} å¼ å›¾ç‰‡ï¼Œå½“å‰ {len(images_data)} å¼ "}), 400
        if len(audios_data) > MAX_AUDIOS:
            return jsonify({"error": f"æœ€å¤šæ”¯æŒ {MAX_AUDIOS} ä¸ªéŸ³é¢‘ï¼Œå½“å‰ {len(audios_data)} ä¸ª"}), 400

        import sys
        print(f"[DEBUG /api/chat] backend={backend}, images={len(images_data)}, audios={len(audios_data)}, text={repr(text[:50] if text else '')}", flush=True)
        sys.stdout.flush()

        # è·å–æˆ–åˆ›å»ºä¼šè¯
        if not session_id or session_id not in sessions:
            # å°è¯•ä»ç£ç›˜åŠ è½½
            loaded = False
            if session_id:
                for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
                    if session_id in jsonl_file.name:
                        session_data = load_session_from_disk(jsonl_file)
                        if session_data:
                            sessions[session_id] = session_data
                            loaded = True
                            break
            if not loaded:
                session_id = str(uuid.uuid4())[:8]
                sessions[session_id] = {"messages": [], "created_at": time.time(), "title": "æ–°å¯¹è¯"}

        session = sessions[session_id]
        images = []  # PIL Image å¯¹è±¡åˆ—è¡¨ (MPSæ¨¡å¼ç”¨)
        audios = []  # (audio_array, sr) å…ƒç»„åˆ—è¡¨ (MPSæ¨¡å¼ç”¨)
        image_paths = []  # å›¾ç‰‡è·¯å¾„åˆ—è¡¨ (mmprojæ¨¡å¼ç”¨)
        audio_paths = []  # éŸ³é¢‘è·¯å¾„åˆ—è¡¨ (mmprojæ¨¡å¼ç”¨)

        # å¤„ç†å¤šä¸ªå›¾ç‰‡
        for idx, img_data in enumerate(images_data):
            if "," in img_data:
                img_data = img_data.split(",")[1]
            image_bytes = base64.b64decode(img_data)
            img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
            images.append(img)
            # ä¿å­˜ä¸´æ—¶å›¾ç‰‡ä¾› mmproj ä½¿ç”¨
            img_path = f"/tmp/image_{session_id}_{idx}.png"
            img.save(img_path)
            image_paths.append(img_path)

        # å¤„ç†å¤šä¸ªéŸ³é¢‘
        for idx, aud_data in enumerate(audios_data):
            mime_part = ""
            if "," in aud_data:
                mime_part = aud_data.split(",")[0]
                aud_data = aud_data.split(",")[1]
            audio_bytes = base64.b64decode(aud_data)

            if "wav" in mime_part:
                ext = ".wav"
            elif "webm" in mime_part:
                ext = ".webm"
            elif "ogg" in mime_part:
                ext = ".ogg"
            elif "mp3" in mime_part or "mpeg" in mime_part:
                ext = ".mp3"
            elif "flac" in mime_part:
                ext = ".flac"
            else:
                ext = ".wav"

            temp_path = f"/tmp/audio_{session_id}_{idx}{ext}"
            with open(temp_path, "wb") as f:
                f.write(audio_bytes)

            audio_array, sr = librosa.load(temp_path, sr=16000)
            audios.append((audio_array, sr))
            audio_paths.append(temp_path)
            print(f"[DEBUG] éŸ³é¢‘ {idx}: {len(audio_array)/sr:.2f}ç§’")

        # å‘åå…¼å®¹: å•ä¸ªå˜é‡
        image = images[0] if images else None
        audio = audios[0] if audios else None
        image_path = image_paths[0] if image_paths else None
        audio_path = audio_paths[0] if audio_paths else None

        # æ„å»ºå½“å‰æ¶ˆæ¯å†…å®¹
        content = []
        has_media = (image is not None or audio is not None)
        display_text = text

        if backend == "mps":
            # MPS æ¨¡å¼éœ€è¦ PyTorch
            if not PYTORCH_AVAILABLE:
                return jsonify({
                    "error": "MPS åç«¯éœ€è¦å®‰è£… PyTorch (è¿›é˜¶åŠŸèƒ½)",
                    "hint": "è¯·ä½¿ç”¨ mmproj åç«¯ï¼Œæˆ–å®‰è£…: pip install torch transformers librosa"
                }), 400

            if not has_media:
                # çº¯æ–‡æœ¬æ¶ˆæ¯ï¼šæ·»åŠ  dummy_image
                content.append({"type": "image", "image": dummy_image})
                display_text = text
                text = "Ignore the blank image. " + text

            # Gemma 3n è¦æ±‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å›¾ç‰‡
            # å¦‚æœåªæœ‰éŸ³é¢‘æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿéœ€è¦æ·»åŠ  dummy_image
            if image is not None:
                content.append({"type": "image", "image": image})
            elif audio is not None:
                # åªæœ‰éŸ³é¢‘ï¼Œæ·»åŠ  dummy_image
                content.append({"type": "image", "image": dummy_image})
                text = "Ignore the blank image. " + text

            if audio is not None:
                content.append({"type": "audio", "audio": audio[0], "sample_rate": audio[1]})

            content.append({"type": "text", "text": text})

        # è®¡ç®—å½“å‰è½®æ¬¡
        turn_index = len(session["messages"]) // 2 + 1

        # ç”Ÿæˆå›å¤ (ä¼ å…¥ session_id å’Œ has_media)
        # ç¡®å®šå½“å‰åª’ä½“ç±»å‹
        current_media_type = None
        if image is not None:
            current_media_type = "image"
        elif audio is not None:
            current_media_type = "audio"

        # ç”Ÿæˆå›å¤ (ä¼ å…¥ session_id, has_media, media_type)
        if backend == "mps":
            result = generate_response(
                session["messages"],
                content,
                session_id=session_id,
                has_media=has_media,
                media_type=current_media_type
            )
        elif backend == "llama.cpp":
            # llama.cpp çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆä½¿ç”¨ llama-server æŒä¹…åŒ–æœåŠ¡ï¼‰
            # æ³¨æ„ï¼šåªæ”¯æŒæ–‡æœ¬ï¼Œä¸æ”¯æŒå›¾ç‰‡/éŸ³é¢‘
            if has_media:
                result = {"error": "llama.cpp åç«¯æš‚ä¸æ”¯æŒå›¾ç‰‡/éŸ³é¢‘ï¼Œè¯·åˆ‡æ¢åˆ° MPS æˆ– mmproj åç«¯"}
            else:
                # æ„å»ºå†å²ä¸Šä¸‹æ–‡
                history_context = ""
                if session["messages"]:
                    history_parts = []
                    for msg in session["messages"][-MAX_HISTORY_TURNS * 2:]:
                        role = "User" if msg["role"] == "user" else "Assistant"
                        history_parts.append(f"{role}: {msg['text']}")
                    history_context = "\n".join(history_parts)

                # ä½¿ç”¨ llama-server æŒä¹…åŒ–æœåŠ¡ï¼ˆæ›´å¿«ï¼‰
                result = query_llama_server(text, history_context)
        else:
            # llama.cpp/mmproj æ¨¡å¼
            # ä¼ å…¥å†å²æ¶ˆæ¯å’Œ session_idï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œ thought signature
            mm_prompt = text or "Please describe what you see/hear."
            # ä½¿ç”¨å¤šæ–‡ä»¶è·¯å¾„ (å¦‚æœ‰)ï¼Œå¦åˆ™å›é€€åˆ°å•æ–‡ä»¶
            mm_image_paths = image_paths if image_paths else None
            mm_audio_paths = audio_paths if audio_paths else None
            print(f"[DEBUG mmproj] backend={backend}, text={repr(text)}, images={len(image_paths)}, audios={len(audio_paths)}")

            result = run_llama_mmproj(
                mm_prompt,
                image_path=mm_image_paths,  # æ”¯æŒå•ä¸ªè·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
                audio_path=mm_audio_paths,  # æ”¯æŒå•ä¸ªè·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
                messages_history=session["messages"],
                session_id=session_id,
                has_media=has_media
            )

        if "error" not in result:
            # å¦‚æœæœ‰åª’ä½“è¾“å…¥ï¼Œä»æ¨¡å‹å›å¤ä¸­æå–ç†è§£å¹¶å­˜å‚¨åˆ° thought signature
            if has_media:
                # æ¨¡å‹çš„å›å¤å°±æ˜¯å¯¹åª’ä½“çš„ç†è§£
                # å°†å…¶å­˜å‚¨ä¸º "å‹ç¼©è®°å¿†"
                understanding = result["response"]

                # ç”Ÿæˆåª’ä½“ç†è§£ç­¾å
                media_ref = generate_media_signature(
                    session_id=session_id,
                    turn_index=turn_index,
                    understanding=understanding[:500]  # é™åˆ¶é•¿åº¦ï¼Œåªä¿å­˜æ‘˜è¦
                )
                print(f"[Thought Signature] å­˜å‚¨ {current_media_type} ç†è§£: {media_ref}")

            # ä¿å­˜åˆ°å†å²ï¼ˆåªä¿å­˜æ–‡æœ¬æ‘˜è¦ï¼‰
            user_summary = display_text
            if len(images) > 0:
                user_summary = f"[{len(images)}å¼ å›¾ç‰‡] " + user_summary
            if len(audios) > 0:
                user_summary = f"[{len(audios)}ä¸ªéŸ³é¢‘] " + user_summary

            session["messages"].append({
                "role": "user",
                "text": user_summary,
                "has_image": len(images) > 0,
                "has_audio": len(audios) > 0,
                "image_count": len(images),
                "audio_count": len(audios),
                "timestamp": time.time()
            })
            session["messages"].append({
                "role": "assistant",
                "text": result["response"],
                "timestamp": time.time()
            })

            # é™åˆ¶å†å²é•¿åº¦
            if len(session["messages"]) > MAX_HISTORY_TURNS * 2:
                session["messages"] = session["messages"][-MAX_HISTORY_TURNS * 2:]

            # æ›´æ–°æ ‡é¢˜ (ç”¨ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯)
            if session.get("title") == "æ–°å¯¹è¯" and len(session["messages"]) >= 1:
                first_user_msg = next((m for m in session["messages"] if m["role"] == "user"), None)
                if first_user_msg:
                    session["title"] = first_user_msg["text"][:30] + ("..." if len(first_user_msg["text"]) > 30 else "")

            # ä¿å­˜åˆ°ç£ç›˜
            save_session_to_disk(session_id, session)
            # è¿½åŠ åˆ°å…¨å±€å†å²
            append_to_history(session_id, display_text)

        result["session_id"] = session_id
        return jsonify(result)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    init_storage()

    # macOS: è¯·æ±‚ sudo æƒé™ç”¨äºç¡¬ä»¶ç›‘æ§ (å¯é€‰)
    if platform.system() == "Darwin" and DEFAULT_BACKEND == "mps":
        request_sudo_permission()

    load_model()
    print("\n" + "=" * 60)
    print("ğŸ‰ çµç©º AI å¤šæ¨¡æ€èŠå¤©æœåŠ¡å™¨")
    print("=" * 60)
    print(f"  åœ°å€: http://localhost:5000")
    print(f"  åç«¯: {DEFAULT_BACKEND}")
    if DEFAULT_BACKEND == "mmproj":
        print(f"  æ¨¡å‹: {LLAMA_MM_MODEL}")
        print(f"  è§†è§‰: {LLAMA_MM_PROJ_IMAGE}")
        print(f"  éŸ³é¢‘: {LLAMA_MM_PROJ_AUDIO}")
    print(f"  å­˜å‚¨: {GEMMA3N_HOME}")
    print("")
    if DEFAULT_BACKEND == "mmproj":
        print("  æç¤º: ä½¿ç”¨ llama.cpp å¤šæ¨¡æ€åç«¯ï¼Œæ— éœ€ PyTorch")
        print("  è¿›é˜¶: export GEMMA3N_BACKEND=mps (éœ€è¦ PyTorch)")
    if sudo_authorized:
        print("  GPU æ¸©åº¦ç›‘æ§: âœ… å·²å¯ç”¨")
    print("=" * 60)
    port = int(os.environ.get("WEBUI_PORT", 5001))
    app.run(host="0.0.0.0", port=port, debug=False, threaded=True)
