"""
æ™ºèƒ½è§†é¢‘å¤„ç†å™¨ - å®Œæ•´çš„éŸ³è§†é¢‘åŒæ­¥ + æ„å›¾è¯†åˆ« + Prompt ä¼˜åŒ–

æ ¸å¿ƒåŠŸèƒ½:
1. éŸ³è§†é¢‘æ—¶é—´åŒæ­¥
2. ç”¨æˆ·æ„å›¾åˆ†ç±»
3. æ™ºèƒ½èµ„æºåˆ†é…
4. åŠ¨æ€ Prompt ç”Ÿæˆ
5. ç»“æ„åŒ–è¾“å‡º
"""

import os
import sys
import time
import json
import torch
import numpy as np
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Union, Any
from dataclasses import dataclass, field
from enum import Enum
from PIL import Image

sys.path.insert(0, str(Path(__file__).parent.parent))


class UserIntent(Enum):
    """ç”¨æˆ·æ„å›¾ç±»å‹"""
    DESCRIBE = "describe"       # æè¿°è§†é¢‘å†…å®¹
    SUMMARIZE = "summarize"     # æ€»ç»“è§†é¢‘
    LOCATE = "locate"           # å®šä½ç‰¹å®šå†…å®¹
    COMPARE = "compare"         # å¯¹æ¯”åˆ†æ
    COUNT = "count"             # è®¡æ•°
    EXPLAIN = "explain"         # è§£é‡Šè¯´æ˜
    AUDIO_FOCUS = "audio_focus" # å…³æ³¨éŸ³é¢‘
    TRANSCRIBE = "transcribe"   # è½¬å½•
    GENERAL = "general"         # é€šç”¨é—®ç­”


@dataclass
class SyncedSegment:
    """åŒæ­¥çš„éŸ³è§†é¢‘ç‰‡æ®µ"""
    segment_id: int
    start_time: float
    end_time: float
    keyframe: np.ndarray
    keyframe_time: float
    audio_segment: Optional[np.ndarray] = None
    change_score: float = 0.0
    is_speech: bool = False
    audio_energy: float = 0.0


@dataclass
class ProcessingConfig:
    """å¤„ç†é…ç½®"""
    max_keyframes: int = 5
    audio_segments: int = 3
    include_full_audio: bool = False
    prompt_style: str = "structured"
    output_format: str = "narrative"


@dataclass
class SmartVideoResult:
    """æ™ºèƒ½å¤„ç†ç»“æœ"""
    # åŸºç¡€ä¿¡æ¯
    video_path: str
    duration: float
    activity_level: str

    # åŒæ­¥ç‰‡æ®µ
    synced_segments: List[SyncedSegment]

    # æ„å›¾åˆ†æ
    detected_intent: UserIntent
    intent_confidence: float

    # æ—¶åºä¿¡æ¯
    timeline: str
    semantic_summary: str

    # ç”Ÿæˆç»“æœ
    response: Optional[str] = None
    structured_output: Optional[Dict] = None

    # å…ƒä¿¡æ¯
    processing_time: float = 0.0
    config_used: Optional[ProcessingConfig] = None


class IntentClassifier:
    """ç”¨æˆ·æ„å›¾åˆ†ç±»å™¨"""

    INTENT_KEYWORDS = {
        UserIntent.DESCRIBE: ["æè¿°", "è¯´æ˜", "è®²è§£", "çœ‹åˆ°ä»€ä¹ˆ", "å‘ç”Ÿä»€ä¹ˆ", "describe", "what", "show"],
        UserIntent.SUMMARIZE: ["æ€»ç»“", "æ¦‚æ‹¬", "æ‘˜è¦", "ç®€è¿°", "summarize", "summary", "brief"],
        UserIntent.LOCATE: ["ä»€ä¹ˆæ—¶å€™", "å“ªé‡Œ", "æ‰¾åˆ°", "å®šä½", "å‡ºç°", "when", "where", "find", "locate"],
        UserIntent.COMPARE: ["å¯¹æ¯”", "æ¯”è¾ƒ", "åŒºåˆ«", "å˜åŒ–", "å‰å", "compare", "difference", "change"],
        UserIntent.COUNT: ["å¤šå°‘", "å‡ ä¸ª", "æ•°é‡", "ç»Ÿè®¡", "count", "how many", "number"],
        UserIntent.EXPLAIN: ["ä¸ºä»€ä¹ˆ", "åŸå› ", "è§£é‡Š", "explain", "why", "reason"],
        UserIntent.AUDIO_FOCUS: ["è¯´äº†ä»€ä¹ˆ", "éŸ³é¢‘", "å£°éŸ³", "å¯¹è¯", "audio", "sound", "speech", "say"],
        UserIntent.TRANSCRIBE: ["è½¬å½•", "å­—å¹•", "æ–‡å­—", "transcribe", "transcript", "subtitle"],
    }

    @classmethod
    def classify(cls, query: str) -> Tuple[UserIntent, float]:
        """
        åˆ†ç±»ç”¨æˆ·æ„å›¾

        Returns:
            intent: æ„å›¾ç±»å‹
            confidence: ç½®ä¿¡åº¦ (0-1)
        """
        query_lower = query.lower()
        scores = {}

        for intent, keywords in cls.INTENT_KEYWORDS.items():
            score = sum(1 for kw in keywords if kw in query_lower)
            if score > 0:
                scores[intent] = score

        if not scores:
            return UserIntent.GENERAL, 0.5

        best_intent = max(scores, key=scores.get)
        max_score = scores[best_intent]
        confidence = min(1.0, max_score / 3)  # 3ä¸ªå…³é”®è¯åŒ¹é…è¾¾åˆ°æ»¡åˆ†

        return best_intent, confidence


class ResourceAllocator:
    """æ™ºèƒ½èµ„æºåˆ†é…å™¨"""

    @staticmethod
    def allocate(
        intent: UserIntent,
        video_duration: float,
        activity_level: str,
        has_audio: bool
    ) -> ProcessingConfig:
        """æ ¹æ®æ„å›¾å’Œè§†é¢‘ç‰¹å¾åˆ†é…èµ„æº"""

        # åŸºç¡€é…ç½®
        configs = {
            UserIntent.DESCRIBE: ProcessingConfig(
                max_keyframes=min(8, max(3, int(video_duration / 2))),
                audio_segments=3,
                prompt_style="detailed",
                output_format="narrative"
            ),
            UserIntent.SUMMARIZE: ProcessingConfig(
                max_keyframes=3,
                audio_segments=1,
                prompt_style="concise",
                output_format="bullet"
            ),
            UserIntent.LOCATE: ProcessingConfig(
                max_keyframes=min(10, max(5, int(video_duration))),
                audio_segments=5,
                prompt_style="search",
                output_format="timestamp"
            ),
            UserIntent.COMPARE: ProcessingConfig(
                max_keyframes=4,  # ä¸»è¦å…³æ³¨é¦–å°¾
                audio_segments=2,
                prompt_style="comparative",
                output_format="structured"
            ),
            UserIntent.COUNT: ProcessingConfig(
                max_keyframes=min(8, max(4, int(video_duration / 1.5))),
                audio_segments=1,
                prompt_style="analytical",
                output_format="count"
            ),
            UserIntent.EXPLAIN: ProcessingConfig(
                max_keyframes=5,
                audio_segments=3,
                prompt_style="reasoning",
                output_format="narrative"
            ),
            UserIntent.AUDIO_FOCUS: ProcessingConfig(
                max_keyframes=3,
                audio_segments=1,
                include_full_audio=True,
                prompt_style="audio_centric",
                output_format="transcript"
            ),
            UserIntent.TRANSCRIBE: ProcessingConfig(
                max_keyframes=2,
                audio_segments=1,
                include_full_audio=True,
                prompt_style="transcription",
                output_format="transcript"
            ),
            UserIntent.GENERAL: ProcessingConfig(
                max_keyframes=5,
                audio_segments=2,
                prompt_style="balanced",
                output_format="narrative"
            ),
        }

        config = configs.get(intent, configs[UserIntent.GENERAL])

        # æ ¹æ®æ´»åŠ¨çº§åˆ«è°ƒæ•´
        if activity_level == "high":
            config.max_keyframes = min(14, int(config.max_keyframes * 1.5))
        elif activity_level == "static":
            config.max_keyframes = max(2, config.max_keyframes // 2)

        # æ ¹æ®æ—¶é•¿è°ƒæ•´
        if video_duration > 120:  # è¶…è¿‡2åˆ†é’Ÿ
            config.max_keyframes = min(config.max_keyframes, 10)
        elif video_duration < 10:  # çŸ­äº10ç§’
            config.max_keyframes = min(config.max_keyframes, 5)

        # æ— éŸ³é¢‘æ—¶çš„è°ƒæ•´
        if not has_audio:
            config.audio_segments = 0
            config.include_full_audio = False

        return config


class PromptBuilder:
    """åŠ¨æ€ Prompt æ„å»ºå™¨"""

    SYSTEM_CONTEXTS = {
        "detailed": "You are a precise video analyst. Describe visual and audio content with timestamps.",
        "concise": "You are a video summarizer. Provide brief, essential information only.",
        "search": "You are a video search assistant. Help locate specific moments in videos.",
        "comparative": "You are a change analyst. Focus on differences between video segments.",
        "analytical": "You are a visual counter. Carefully count and enumerate objects/people.",
        "reasoning": "You are a video interpreter. Explain the context and reasons behind what you see.",
        "audio_centric": "You are an audio analyst. Focus primarily on speech, sounds, and audio content.",
        "transcription": "You are a transcription assistant. Convert speech to text accurately.",
        "balanced": "You are a multimodal video analyst. Consider both visual and audio equally.",
    }

    TASK_TEMPLATES = {
        UserIntent.DESCRIBE: """
Based on the video keyframes and audio content:
1. Describe what appears in each scene
2. Note any actions or movements
3. Describe the audio content (speech, music, sounds)
4. Explain how scenes connect temporally

Use timestamps (MM:SS format) when referencing specific moments.
""",
        UserIntent.SUMMARIZE: """
Provide a concise summary (2-3 sentences) covering:
- Main subject/topic
- Key action or event
- Overall purpose or message
""",
        UserIntent.LOCATE: """
Help find the specific content mentioned in the query.
- Identify the most relevant timestamp
- Describe what happens at that moment
- Provide context (what happens before/after)
""",
        UserIntent.COMPARE: """
Compare the beginning and end of the video:
- What changed visually?
- What changed in the audio?
- What remained the same?
""",
        UserIntent.COUNT: """
Carefully count the requested items:
- List each instance you can identify
- Provide timestamps for each occurrence
- Give a final count with confidence level
""",
        UserIntent.EXPLAIN: """
Explain the context and reasoning:
- What is happening and why?
- What led to this situation?
- What might the purpose or goal be?
""",
        UserIntent.AUDIO_FOCUS: """
Focus on the audio content:
- Transcribe any speech (with speaker labels if possible)
- Describe background sounds and music
- Note how audio relates to visuals
""",
        UserIntent.TRANSCRIBE: """
Transcribe the audio content:
- [Speaker]: "dialogue"
- Include non-verbal sounds in [brackets]
- Note pauses and timing
""",
        UserIntent.GENERAL: """
Analyze the video based on the user's question:
- Consider both visual and audio content
- Provide relevant details
- Use timestamps where helpful
""",
    }

    @classmethod
    def build(
        cls,
        intent: UserIntent,
        config: ProcessingConfig,
        timeline: str,
        semantic_summary: str,
        user_query: str,
        video_duration: float,
        activity_level: str,
        has_audio: bool
    ) -> str:
        """æ„å»ºå®Œæ•´çš„ Prompt"""

        parts = []

        # 1. ç³»ç»Ÿä¸Šä¸‹æ–‡
        system_ctx = cls.SYSTEM_CONTEXTS.get(config.prompt_style, cls.SYSTEM_CONTEXTS["balanced"])
        parts.append(f"## Role\n{system_ctx}")

        # 2. è§†é¢‘å…ƒä¿¡æ¯
        parts.append(f"""
## Video Information
- Duration: {video_duration:.1f} seconds
- Activity Level: {activity_level}
- Audio: {'Present' if has_audio else 'None'}
""")

        # 3. æ—¶åºä¸Šä¸‹æ–‡
        parts.append(f"## Timeline\n{timeline}")

        # 4. è¯­ä¹‰æ‘˜è¦ (æ¥è‡ª V-JEPA2)
        if semantic_summary:
            parts.append(f"## AI Pre-analysis\n{semantic_summary}")

        # 5. ä»»åŠ¡æ¨¡æ¿
        task = cls.TASK_TEMPLATES.get(intent, cls.TASK_TEMPLATES[UserIntent.GENERAL])
        parts.append(f"## Task\n{task}")

        # 6. ç”¨æˆ·æŸ¥è¯¢
        parts.append(f"## User Query\n{user_query}")

        # 7. è¾“å‡ºæ ¼å¼æŒ‡å¯¼
        format_guide = cls._get_format_guide(config.output_format)
        if format_guide:
            parts.append(f"## Output Format\n{format_guide}")

        return "\n".join(parts)

    @staticmethod
    def _get_format_guide(output_format: str) -> str:
        """è·å–è¾“å‡ºæ ¼å¼æŒ‡å¯¼"""
        guides = {
            "narrative": "Respond in natural paragraphs with clear temporal flow.",
            "bullet": "Use bullet points for clarity. Each point should be self-contained.",
            "timestamp": "Structure response around timestamps: [MM:SS] description",
            "structured": """Respond in JSON format:
{
    "main_finding": "...",
    "details": [...],
    "timestamps": [...]
}""",
            "count": "Provide enumerated list followed by total count.",
            "transcript": "Format as dialogue transcript with timestamps and speakers.",
        }
        return guides.get(output_format, "")


class AudioVideoSynchronizer:
    """éŸ³è§†é¢‘åŒæ­¥å™¨"""

    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate

    def sync_segments(
        self,
        keyframe_times: List[float],
        keyframe_scores: List[float],
        keyframes: List[np.ndarray],
        audio_array: Optional[np.ndarray],
        video_duration: float
    ) -> List[SyncedSegment]:
        """
        åˆ›å»ºåŒæ­¥çš„éŸ³è§†é¢‘ç‰‡æ®µ

        ç­–ç•¥: ä»¥å…³é”®å¸§ä¸ºä¸­å¿ƒï¼Œå…³è”ç›¸é‚»æ—¶é—´æ®µçš„éŸ³é¢‘
        """
        segments = []

        for i, (kf_time, score, frame) in enumerate(zip(keyframe_times, keyframe_scores, keyframes)):
            # è®¡ç®—æ—¶é—´èŒƒå›´
            if i == 0:
                start_time = 0
            else:
                start_time = (keyframe_times[i-1] + kf_time) / 2

            if i == len(keyframe_times) - 1:
                end_time = video_duration
            else:
                end_time = (kf_time + keyframe_times[i+1]) / 2

            # æå–éŸ³é¢‘ç‰‡æ®µ
            audio_seg = None
            is_speech = False
            audio_energy = 0.0

            if audio_array is not None:
                start_sample = int(start_time * self.sample_rate)
                end_sample = int(end_time * self.sample_rate)
                audio_seg = audio_array[start_sample:end_sample]

                if len(audio_seg) > 0:
                    audio_energy = float(np.sqrt(np.mean(audio_seg ** 2)))
                    # ç®€å•çš„è¯­éŸ³æ£€æµ‹ (èƒ½é‡ > é˜ˆå€¼)
                    is_speech = audio_energy > 0.01

            segment = SyncedSegment(
                segment_id=i,
                start_time=start_time,
                end_time=end_time,
                keyframe=frame,
                keyframe_time=kf_time,
                audio_segment=audio_seg,
                change_score=score,
                is_speech=is_speech,
                audio_energy=audio_energy
            )
            segments.append(segment)

        return segments


class SmartVideoProcessor:
    """
    æ™ºèƒ½è§†é¢‘å¤„ç†å™¨

    æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œæä¾›ç«¯åˆ°ç«¯çš„è§†é¢‘ç†è§£æœåŠ¡
    """

    def __init__(
        self,
        vjepa_model_size: str = "L",
        gemma_model_name: str = "google/gemma-3n-E2B-it",
        load_gemma: bool = False,
        device: Optional[str] = None
    ):
        print("=" * 60)
        print("åˆå§‹åŒ–æ™ºèƒ½è§†é¢‘å¤„ç†å™¨")
        print("=" * 60)

        # å¯¼å…¥å¢å¼ºç‰ˆç®¡é“
        from vjepa2.enhanced_pipeline import EnhancedVideoPipeline

        self.pipeline = EnhancedVideoPipeline(
            vjepa_model_size=vjepa_model_size,
            target_keyframes=10,  # åˆå§‹å€¼ï¼Œåé¢ä¼šåŠ¨æ€è°ƒæ•´
            load_gemma=load_gemma
        )

        self.synchronizer = AudioVideoSynchronizer()
        self.gemma_loaded = load_gemma

        print("æ™ºèƒ½è§†é¢‘å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ!")

    def process(
        self,
        video_path: str,
        user_query: str,
        audio_path: Optional[str] = None,
        sample_fps: float = 5.0
    ) -> SmartVideoResult:
        """
        å¤„ç†è§†é¢‘å¹¶å“åº”ç”¨æˆ·æŸ¥è¯¢

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            user_query: ç”¨æˆ·æŸ¥è¯¢
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šè‡ªåŠ¨æå–)
            sample_fps: é‡‡æ ·å¸§ç‡

        Returns:
            SmartVideoResult
        """
        start_time = time.time()

        print(f"\n{'='*60}")
        print(f"å¤„ç†è§†é¢‘: {video_path}")
        print(f"ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        print(f"{'='*60}")

        # 1. æ„å›¾åˆ†ç±»
        print("\n[Step 1] ç”¨æˆ·æ„å›¾åˆ†ç±»...")
        intent, confidence = IntentClassifier.classify(user_query)
        print(f"    æ„å›¾: {intent.value} (ç½®ä¿¡åº¦: {confidence:.2f})")

        # 2. è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
        print("\n[Step 2] åˆ†æè§†é¢‘...")
        # ä½¿ç”¨å¢å¼ºç®¡é“è¿›è¡Œåˆæ­¥åˆ†æ
        initial_result = self.pipeline.analyze_video(
            video_path=video_path,
            sample_fps=sample_fps,
            generate_response=False
        )

        video_duration = initial_result.stats.get("video_duration", 0)
        activity_level = initial_result.activity_level
        has_audio = audio_path is not None or self._check_audio(video_path)

        print(f"    æ—¶é•¿: {video_duration:.1f}s")
        print(f"    æ´»åŠ¨çº§åˆ«: {activity_level}")
        print(f"    éŸ³é¢‘: {'æœ‰' if has_audio else 'æ— '}")

        # 3. èµ„æºåˆ†é…
        print("\n[Step 3] æ™ºèƒ½èµ„æºåˆ†é…...")
        config = ResourceAllocator.allocate(intent, video_duration, activity_level, has_audio)
        print(f"    å…³é”®å¸§æ•°: {config.max_keyframes}")
        print(f"    éŸ³é¢‘ç‰‡æ®µ: {config.audio_segments}")
        print(f"    Prompt é£æ ¼: {config.prompt_style}")

        # 4. æ ¹æ®é…ç½®é‡æ–°æå–å…³é”®å¸§
        print("\n[Step 4] æå–å…³é”®å¸§å’ŒéŸ³é¢‘...")
        self.pipeline.target_keyframes = config.max_keyframes

        result = self.pipeline.analyze_video(
            video_path=video_path,
            sample_fps=sample_fps,
            generate_response=False
        )

        keyframes = result.keyframes[:config.max_keyframes]
        keyframe_times = result.keyframe_timestamps[:config.max_keyframes]
        keyframe_scores = result.keyframe_scores[:config.max_keyframes]

        # 5. åŠ è½½éŸ³é¢‘
        audio_array = None
        if has_audio:
            audio_array = self._load_audio(video_path, audio_path)

        # 6. éŸ³è§†é¢‘åŒæ­¥
        print("\n[Step 5] éŸ³è§†é¢‘åŒæ­¥...")
        synced_segments = self.synchronizer.sync_segments(
            keyframe_times, keyframe_scores, keyframes,
            audio_array, video_duration
        )
        print(f"    åŒæ­¥ç‰‡æ®µæ•°: {len(synced_segments)}")

        # 7. æ„å»ºæ—¶é—´çº¿
        timeline = self._build_timeline(synced_segments)

        # 8. æ„å»º Prompt
        print("\n[Step 6] æ„å»º Prompt...")
        prompt = PromptBuilder.build(
            intent=intent,
            config=config,
            timeline=timeline,
            semantic_summary=result.semantic_summary,
            user_query=user_query,
            video_duration=video_duration,
            activity_level=activity_level,
            has_audio=has_audio
        )

        # 9. ç”Ÿæˆå“åº”
        response = None
        if self.gemma_loaded:
            print("\n[Step 7] Gemma 3n ç”Ÿæˆå“åº”...")
            response = self._generate_response(
                synced_segments, prompt, audio_array, config
            )

        processing_time = time.time() - start_time

        print(f"\n{'='*60}")
        print(f"å¤„ç†å®Œæˆ! è€—æ—¶: {processing_time:.2f}s")
        print(f"{'='*60}")

        return SmartVideoResult(
            video_path=video_path,
            duration=video_duration,
            activity_level=activity_level,
            synced_segments=synced_segments,
            detected_intent=intent,
            intent_confidence=confidence,
            timeline=timeline,
            semantic_summary=result.semantic_summary,
            response=response,
            processing_time=processing_time,
            config_used=config
        )

    def _check_audio(self, video_path: str) -> bool:
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰éŸ³é¢‘è½¨é“"""
        import subprocess
        try:
            result = subprocess.run(
                ['ffprobe', '-v', 'quiet', '-select_streams', 'a',
                 '-show_entries', 'stream=codec_type', '-of', 'csv=p=0', video_path],
                capture_output=True, text=True
            )
            return 'audio' in result.stdout
        except:
            return False

    def _load_audio(self, video_path: str, audio_path: Optional[str]) -> Optional[np.ndarray]:
        """åŠ è½½éŸ³é¢‘"""
        try:
            import librosa

            if audio_path and os.path.exists(audio_path):
                audio, _ = librosa.load(audio_path, sr=16000)
                return audio
            else:
                # ä»è§†é¢‘æå–
                import subprocess
                import tempfile

                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
                    temp_path = f.name

                subprocess.run([
                    'ffmpeg', '-y', '-i', video_path,
                    '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1',
                    temp_path
                ], capture_output=True)

                if os.path.exists(temp_path):
                    audio, _ = librosa.load(temp_path, sr=16000)
                    os.unlink(temp_path)
                    return audio

        except Exception as e:
            print(f"    éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
        return None

    def _build_timeline(self, segments: List[SyncedSegment]) -> str:
        """æ„å»ºæ—¶é—´çº¿æè¿°"""
        lines = []
        for seg in segments:
            mm = int(seg.keyframe_time // 60)
            ss = int(seg.keyframe_time % 60)

            # å˜åŒ–æŒ‡ç¤ºç¬¦
            if seg.change_score > 0.1:
                change_icon = "â–²"
            elif seg.change_score > 0.05:
                change_icon = "â†’"
            else:
                change_icon = "â”€"

            # éŸ³é¢‘æŒ‡ç¤ºç¬¦
            audio_icon = "ğŸ”Š" if seg.is_speech else "ğŸ”‡" if seg.audio_segment is not None else ""

            lines.append(f"[{mm:02d}:{ss:02d}] {change_icon} Frame {seg.segment_id + 1} {audio_icon}")

        return "\n".join(lines)

    def _generate_response(
        self,
        segments: List[SyncedSegment],
        prompt: str,
        audio_array: Optional[np.ndarray],
        config: ProcessingConfig
    ) -> Optional[str]:
        """ä½¿ç”¨ Gemma 3n ç”Ÿæˆå“åº”"""
        if not self.pipeline.gemma_loaded:
            self.pipeline._load_gemma()

        if not self.pipeline.gemma_loaded:
            return None

        # æ„å»ºæ¶ˆæ¯
        content = []

        # æ·»åŠ å›¾åƒ (æœ€å¤š5å¼ )
        for seg in segments[:5]:
            img = Image.fromarray(seg.keyframe)
            content.append({"type": "image", "image": img})

        # æ·»åŠ éŸ³é¢‘
        if audio_array is not None and config.include_full_audio:
            content.append({
                "type": "audio",
                "audio": audio_array,
                "sample_rate": 16000
            })

        content.append({"type": "text", "text": prompt})

        messages = [{"role": "user", "content": content}]

        try:
            inputs = self.pipeline.gemma_processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"
            )

            input_ids = inputs["input_ids"].to(self.pipeline.gemma_model.device)
            attention_mask = inputs["attention_mask"].to(self.pipeline.gemma_model.device)

            generate_kwargs = {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "max_new_tokens": 768,
                "do_sample": False,
            }

            if "pixel_values" in inputs and inputs["pixel_values"] is not None:
                generate_kwargs["pixel_values"] = inputs["pixel_values"].to(
                    self.pipeline.gemma_model.device,
                    dtype=self.pipeline.gemma_model.dtype
                )

            if "input_features" in inputs and inputs["input_features"] is not None:
                generate_kwargs["input_features"] = inputs["input_features"].to(
                    self.pipeline.gemma_model.device,
                    dtype=self.pipeline.gemma_model.dtype
                )
                generate_kwargs["input_features_mask"] = inputs["input_features_mask"].to(
                    self.pipeline.gemma_model.device
                )

            with torch.inference_mode():
                outputs = self.pipeline.gemma_model.generate(**generate_kwargs)

            response = self.pipeline.gemma_processor.tokenizer.decode(
                outputs[0][input_ids.shape[1]:],
                skip_special_tokens=True
            )

            return response

        except Exception as e:
            print(f"    å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


# ============================================================
# æµ‹è¯•
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("æ™ºèƒ½è§†é¢‘å¤„ç†å™¨æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯•æ„å›¾åˆ†ç±»
    print("\næµ‹è¯•æ„å›¾åˆ†ç±»:")
    test_queries = [
        "æè¿°ä¸€ä¸‹è¿™ä¸ªè§†é¢‘çš„å†…å®¹",
        "æ€»ç»“è§†é¢‘ä¸»é¢˜",
        "ä»€ä¹ˆæ—¶å€™å‡ºç°äº†äººè„¸",
        "è§†é¢‘å¼€å§‹å’Œç»“æŸæœ‰ä»€ä¹ˆå˜åŒ–",
        "æœ‰å¤šå°‘ä¸ªäºº",
        "ä¸ºä»€ä¹ˆä¼šè¿™æ ·",
        "è§†é¢‘é‡Œè¯´äº†ä»€ä¹ˆ",
        "å¸®æˆ‘è½¬å½•å¯¹è¯",
        "è¿™ä¸ªè§†é¢‘å¥½çœ‹å—",
    ]

    for query in test_queries:
        intent, conf = IntentClassifier.classify(query)
        print(f"  '{query}' â†’ {intent.value} ({conf:.2f})")

    # æµ‹è¯•èµ„æºåˆ†é…
    print("\næµ‹è¯•èµ„æºåˆ†é…:")
    for intent in UserIntent:
        config = ResourceAllocator.allocate(intent, 30.0, "medium", True)
        print(f"  {intent.value}: keyframes={config.max_keyframes}, audio={config.audio_segments}")

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ!")
    print("=" * 60)
