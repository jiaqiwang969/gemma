"""
AI å¤šæ¨¡æ€èŠå¤©æœåŠ¡å™¨
æ”¯æŒ: æ–‡æœ¬ + å›¾ç‰‡ + éŸ³é¢‘ + å¤šè½®å¯¹è¯å†å²
å­˜å‚¨: ~/.gemma3n/ (å‚è€ƒ Codex æ¶æ„)
"""
import os
import io
import base64
import torch
import numpy as np
import uuid
import json
from pathlib import Path
from datetime import datetime
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from transformers import AutoProcessor, Gemma3nForConditionalGeneration
from PIL import Image
import librosa
import warnings
import time
import psutil
import subprocess
import platform
import threading

warnings.filterwarnings("ignore")
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

app = Flask(__name__, static_folder="static")
CORS(app)

# å…¨å±€å˜é‡
model = None
processor = None
model_loaded = False
model_info = {}
dummy_image = None

# å­˜å‚¨è·¯å¾„ (~/.gemma3n/)
GEMMA3N_HOME = Path.home() / ".gemma3n"
SESSIONS_DIR = GEMMA3N_HOME / "sessions"
HISTORY_FILE = GEMMA3N_HOME / "history.jsonl"

# ä¼šè¯ç®¡ç† (å†…å­˜ç¼“å­˜)
sessions = {}  # session_id -> {"messages": [...], "created_at": timestamp, "file_path": ...}
MAX_SESSIONS = 100
MAX_HISTORY_TURNS = 10  # ä¿ç•™æœ€è¿‘Nè½®å¯¹è¯

# sudo æƒé™çŠ¶æ€
sudo_authorized = False
sudo_refresh_thread = None

# ========== Thought Signature ç³»ç»Ÿ (å‹ç¼©è®°å¿†) ==========
# æ ¸å¿ƒæ€æƒ³: å›¾ç‰‡/éŸ³é¢‘ â†’ æ¨¡å‹ç†è§£ â†’ å­˜å…¥ signature â†’ åç»­è½®æ¬¡æ¢å¤
import hashlib
import hmac

THOUGHT_SIGNATURE_SECRET = "gemma3n-thought-signature-key"
media_understanding_cache = {}  # media_ref -> {"understanding": "...", "session_id": "..."}
thought_states = {}  # session_id -> {"turn_index": 0, "media_refs": [...]}


def generate_media_signature(session_id: str, turn_index: int, understanding: str) -> str:
    """
    ä¸ºåª’ä½“ç†è§£ç”Ÿæˆç­¾åå¼•ç”¨

    Returns: media_ref (ç”¨äºåç»­æ¢å¤)
    """
    import base64
    timestamp = int(time.time())
    media_ref = hashlib.md5(f"{session_id}:{turn_index}:{timestamp}".encode()).hexdigest()[:12]

    # å­˜å‚¨ç†è§£åˆ°ç¼“å­˜
    media_understanding_cache[media_ref] = {
        "session_id": session_id,
        "turn_index": turn_index,
        "understanding": understanding,
        "created_at": timestamp
    }

    # æ›´æ–°ä¼šè¯çš„æ€ç»´çŠ¶æ€
    if session_id not in thought_states:
        thought_states[session_id] = {"turn_index": 0, "media_refs": []}
    thought_states[session_id]["media_refs"].append(media_ref)
    thought_states[session_id]["turn_index"] = turn_index

    return media_ref


def get_session_media_context(session_id: str) -> str:
    """
    è·å–ä¼šè¯ä¸­æ‰€æœ‰åª’ä½“ç†è§£çš„ä¸Šä¸‹æ–‡

    è¿™æ˜¯ thought signature ä½œä¸º"å‹ç¼©è®°å¿†"çš„æ ¸å¿ƒï¼š
    - ç¬¬1è½®: ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ â†’ æ¨¡å‹ç”Ÿæˆç†è§£ â†’ å­˜å…¥ signature
    - ç¬¬2è½®: ä» signature æ¢å¤ç†è§£ â†’ æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­
    """
    context_parts = []

    state = thought_states.get(session_id, {})
    media_refs = state.get("media_refs", [])

    for media_ref in media_refs:
        cached = media_understanding_cache.get(media_ref)
        if cached:
            turn = cached.get("turn_index", 0)
            understanding = cached.get("understanding", "")
            if understanding:
                context_parts.append(f"[Turn {turn} - Media Understanding]: {understanding}")

    return "\n".join(context_parts) if context_parts else ""

stats = {
    "total_requests": 0,
    "total_tokens": 0,
    "total_time": 0,
    "avg_speed": 0
}

def init_storage():
    """åˆå§‹åŒ–å­˜å‚¨ç›®å½•ç»“æ„"""
    GEMMA3N_HOME.mkdir(exist_ok=True)
    SESSIONS_DIR.mkdir(exist_ok=True)
    if not HISTORY_FILE.exists():
        HISTORY_FILE.touch()
    print(f"[Storage] åˆå§‹åŒ–å®Œæˆ: {GEMMA3N_HOME}")

def get_session_dir():
    """è·å–å½“å¤©çš„ä¼šè¯ç›®å½• sessions/YYYY/MM/DD/"""
    now = datetime.now()
    day_dir = SESSIONS_DIR / str(now.year) / f"{now.month:02d}" / f"{now.day:02d}"
    day_dir.mkdir(parents=True, exist_ok=True)
    return day_dir

def save_session_to_disk(session_id, session_data):
    """å°†ä¼šè¯ä¿å­˜åˆ° JSONL æ–‡ä»¶"""
    if "file_path" not in session_data:
        # åˆ›å»ºæ–°æ–‡ä»¶
        now = datetime.now()
        timestamp = now.strftime("%H%M%S")
        filename = f"session-{timestamp}-{session_id}.jsonl"
        session_data["file_path"] = str(get_session_dir() / filename)

    file_path = Path(session_data["file_path"])

    # å†™å…¥ä¼šè¯æ•°æ® (å®Œæ•´è¦†ç›–)
    with open(file_path, "w", encoding="utf-8") as f:
        # ç¬¬ä¸€è¡Œ: ä¼šè¯å…ƒæ•°æ®
        meta = {
            "type": "meta",
            "session_id": session_id,
            "created_at": session_data["created_at"],
            "title": session_data.get("title", "æ–°å¯¹è¯"),
            "updated_at": time.time()
        }
        f.write(json.dumps(meta, ensure_ascii=False) + "\n")

        # åç»­è¡Œ: æ¶ˆæ¯
        for msg in session_data["messages"]:
            item = {"type": "message", **msg}
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

def load_session_from_disk(file_path):
    """ä» JSONL æ–‡ä»¶åŠ è½½ä¼šè¯"""
    file_path = Path(file_path)
    if not file_path.exists():
        return None

    session_data = {"messages": [], "file_path": str(file_path)}

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            item = json.loads(line)
            if item.get("type") == "meta":
                session_data["created_at"] = item.get("created_at", time.time())
                session_data["title"] = item.get("title", "æ–°å¯¹è¯")
                session_data["session_id"] = item.get("session_id")
            elif item.get("type") == "message":
                del item["type"]
                session_data["messages"].append(item)

    return session_data

def append_to_history(session_id, text):
    """è¿½åŠ åˆ°å…¨å±€å†å²è®°å½•"""
    entry = {
        "session_id": session_id,
        "ts": int(time.time()),
        "text": text[:100]  # åªä¿å­˜å‰100å­—ç¬¦ä½œä¸ºæ‘˜è¦
    }
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def list_all_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯æ–‡ä»¶"""
    session_list = []
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        try:
            with open(jsonl_file, "r", encoding="utf-8") as f:
                first_line = f.readline().strip()
                if first_line:
                    meta = json.loads(first_line)
                    if meta.get("type") == "meta":
                        session_list.append({
                            "session_id": meta.get("session_id"),
                            "title": meta.get("title", "æ–°å¯¹è¯"),
                            "created_at": meta.get("created_at", 0),
                            "updated_at": meta.get("updated_at", 0),
                            "file_path": str(jsonl_file)
                        })
        except Exception as e:
            print(f"[Warning] æ— æ³•è¯»å–ä¼šè¯æ–‡ä»¶ {jsonl_file}: {e}")

    # æŒ‰æ›´æ–°æ—¶é—´å€’åº
    session_list.sort(key=lambda x: x.get("updated_at", 0), reverse=True)
    return session_list[:50]  # æœ€å¤šè¿”å›50ä¸ª

def get_memory_usage():
    process = psutil.Process(os.getpid())
    mem_gb = process.memory_info().rss / 1024 / 1024 / 1024
    return round(mem_gb, 2)


def request_sudo_permission():
    """
    å¯åŠ¨æ—¶è¯·æ±‚ sudo æƒé™ï¼ˆç”¨æˆ·è¾“å…¥ä¸€æ¬¡å¯†ç ï¼‰
    å¹¶å¯åŠ¨åå°çº¿ç¨‹å®šæœŸåˆ·æ–°å‡­è¯
    """
    global sudo_authorized, sudo_refresh_thread

    if platform.system() != "Darwin":
        return False

    print("\n" + "=" * 60)
    print("ğŸ” è¯·æ±‚ç¡¬ä»¶ç›‘æ§æƒé™ (ç”¨äºè·å– GPU æ¸©åº¦)")
    print("   è¯·è¾“å…¥æ‚¨çš„ macOS å¯†ç ï¼ˆå¯é€‰ï¼ŒæŒ‰ Ctrl+C è·³è¿‡ï¼‰")
    print("=" * 60)

    try:
        # è¯·æ±‚ sudo æƒé™
        result = subprocess.run(
            ["sudo", "-v"],
            timeout=60  # ç»™ç”¨æˆ·60ç§’è¾“å…¥å¯†ç 
        )
        if result.returncode == 0:
            sudo_authorized = True
            print("âœ… æƒé™æˆæƒæˆåŠŸï¼GPU æ¸©åº¦ç›‘æ§å·²å¯ç”¨")

            # å¯åŠ¨åå°çº¿ç¨‹å®šæœŸåˆ·æ–° sudo å‡­è¯
            def refresh_sudo():
                while sudo_authorized:
                    time.sleep(240)  # æ¯4åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡ï¼ˆsudo é»˜è®¤5åˆ†é’Ÿè¶…æ—¶ï¼‰
                    try:
                        subprocess.run(["sudo", "-v"], capture_output=True, timeout=5)
                    except:
                        pass

            sudo_refresh_thread = threading.Thread(target=refresh_sudo, daemon=True)
            sudo_refresh_thread.start()
            return True
        else:
            print("âš ï¸  æƒé™æœªæˆæƒï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
            return False
    except subprocess.TimeoutExpired:
        print("âš ï¸  æˆæƒè¶…æ—¶ï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
        return False
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²è·³è¿‡æƒé™æˆæƒï¼ŒGPU æ¸©åº¦ç›‘æ§å°†ä¸å¯ç”¨")
        return False
    except Exception as e:
        print(f"âš ï¸  æˆæƒå¤±è´¥: {e}")
        return False


def get_hardware_stats():
    """è·å–ç¡¬ä»¶ç›‘æ§ä¿¡æ¯ (GPUä½¿ç”¨ç‡ã€æ˜¾å­˜ã€æ¸©åº¦ç­‰)"""
    hw_stats = {
        "gpu_usage": "-",
        "vram_usage": "-",
        "gpu_temp": "-",
        "memory_usage": "-",
        "cpu_usage": "-"
    }

    try:
        # CPU ä½¿ç”¨ç‡ (ä¸ä½¿ç”¨ interval é¿å…é˜»å¡)
        hw_stats["cpu_usage"] = f"{psutil.cpu_percent():.1f}%"

        # å†…å­˜ä½¿ç”¨
        mem = psutil.virtual_memory()
        hw_stats["memory_usage"] = f"{mem.used / 1024**3:.1f} GB / {mem.total / 1024**3:.1f} GB"

        system = platform.system()

        if system == "Darwin":  # macOS
            try:
                # æ£€æµ‹ MPS çŠ¶æ€
                if torch.backends.mps.is_available():
                    hw_stats["gpu_usage"] = "MPS è¿è¡Œä¸­"
                else:
                    hw_stats["gpu_usage"] = "CPU æ¨¡å¼"

                # macOS ç»Ÿä¸€å†…å­˜æ¶æ„ï¼Œæ˜¾å­˜å’Œå†…å­˜å…±äº«
                hw_stats["vram_usage"] = "ç»Ÿä¸€å†…å­˜"

                # å°è¯•é€šè¿‡ powermetrics è·å– GPU æ¸©åº¦ (éœ€è¦ sudo æƒé™)
                if sudo_authorized:
                    try:
                        result = subprocess.run(
                            ["sudo", "powermetrics", "--samplers", "smc", "-i", "1", "-n", "1"],
                            capture_output=True, text=True, timeout=3
                        )
                        if result.returncode == 0:
                            output = result.stdout
                            # æŸ¥æ‰¾ GPU æ¸©åº¦ç›¸å…³è¡Œ
                            for line in output.split("\n"):
                                if "GPU" in line and "die" in line.lower():
                                    parts = line.split(":")
                                    if len(parts) >= 2:
                                        temp_str = parts[1].strip().replace("C", "").strip()
                                        try:
                                            temp = float(temp_str)
                                            hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                            break
                                        except:
                                            pass
                                elif "GPU" in line and "temp" in line.lower():
                                    parts = line.split(":")
                                    if len(parts) >= 2:
                                        temp_str = parts[1].strip().replace("C", "").strip()
                                        try:
                                            temp = float(temp_str)
                                            hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                            break
                                        except:
                                            pass

                            # å¦‚æœæ²¡æ‰¾åˆ° GPU æ¸©åº¦ï¼Œå°è¯•æ‰¾ SOC æ¸©åº¦
                            if hw_stats["gpu_temp"] == "-":
                                for line in output.split("\n"):
                                    if "SOC" in line and "temp" in line.lower():
                                        parts = line.split(":")
                                        if len(parts) >= 2:
                                            temp_str = parts[1].strip().replace("C", "").strip()
                                            try:
                                                temp = float(temp_str)
                                                hw_stats["gpu_temp"] = f"{temp:.0f}Â°C"
                                                break
                                            except:
                                                pass
                    except subprocess.TimeoutExpired:
                        hw_stats["gpu_temp"] = "è¶…æ—¶"
                    except Exception:
                        hw_stats["gpu_temp"] = "è·å–å¤±è´¥"
                else:
                    hw_stats["gpu_temp"] = "æœªæˆæƒ"

            except Exception as e:
                print(f"[DEBUG] macOS GPU info error: {e}")

        elif system == "Linux":
            # NVIDIA GPU (ä½¿ç”¨ nvidia-smi)
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")
                    if lines:
                        # å¯èƒ½æœ‰å¤šä¸ª GPUï¼Œå–ç¬¬ä¸€ä¸ª
                        parts = lines[0].split(",")
                        if len(parts) >= 4:
                            gpu_util = parts[0].strip()
                            mem_used = float(parts[1].strip())
                            mem_total = float(parts[2].strip())
                            temp = parts[3].strip()

                            hw_stats["gpu_usage"] = f"{gpu_util}%"
                            hw_stats["vram_usage"] = f"{mem_used/1024:.1f} GB / {mem_total/1024:.1f} GB"
                            hw_stats["gpu_temp"] = f"{temp}Â°C"
            except FileNotFoundError:
                # nvidia-smi ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ AMD æˆ–æ—  GPU
                try:
                    # å°è¯• AMD GPU (rocm-smi)
                    result = subprocess.run(
                        ["rocm-smi", "--showuse", "--showtemp", "--showmeminfo", "vram"],
                        capture_output=True, text=True, timeout=5
                    )
                    if result.returncode == 0:
                        hw_stats["gpu_usage"] = "AMD GPU"
                except:
                    pass
            except Exception as e:
                print(f"[DEBUG] Linux GPU info error: {e}")

        elif system == "Windows":
            # Windows NVIDIA GPU
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5, shell=True
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")
                    if lines:
                        parts = lines[0].split(",")
                        if len(parts) >= 4:
                            gpu_util = parts[0].strip()
                            mem_used = float(parts[1].strip())
                            mem_total = float(parts[2].strip())
                            temp = parts[3].strip()

                            hw_stats["gpu_usage"] = f"{gpu_util}%"
                            hw_stats["vram_usage"] = f"{mem_used/1024:.1f} GB / {mem_total/1024:.1f} GB"
                            hw_stats["gpu_temp"] = f"{temp}Â°C"
            except Exception as e:
                print(f"[DEBUG] Windows GPU info error: {e}")

    except Exception as e:
        print(f"[DEBUG] Hardware stats error: {e}")

    return hw_stats

def cleanup_old_sessions():
    """æ¸…ç†å†…å­˜ä¸­çš„æ—§ä¼šè¯ç¼“å­˜ (ç£ç›˜æ–‡ä»¶ä¿ç•™)"""
    if len(sessions) > MAX_SESSIONS:
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä»å†…å­˜ä¸­ç§»é™¤æœ€è€çš„
        sorted_sessions = sorted(sessions.items(), key=lambda x: x[1]["created_at"])
        for sid, _ in sorted_sessions[:len(sessions) - MAX_SESSIONS]:
            del sessions[sid]

def load_model():
    global model, processor, model_loaded, model_info, dummy_image

    if model_loaded:
        return True

    print("=" * 60)
    print("åŠ è½½ AI å¤šæ¨¡æ€æ¨¡å‹...")
    print("=" * 60)

    model_name = "google/gemma-3n-E2B-it"
    load_start = time.time()

    print("[1/2] åŠ è½½å¤„ç†å™¨...")
    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

    print("[2/2] åŠ è½½æ¨¡å‹...")
    model = Gemma3nForConditionalGeneration.from_pretrained(
        model_name,
        device_map="auto",
        max_memory={"mps": "64GiB", "cpu": "64GiB"},
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model.eval()

    dummy_image = Image.new('RGB', (64, 64), color='white')
    load_time = time.time() - load_start
    total_params = sum(p.numel() for p in model.parameters())

    model_info = {
        "name": "AI Multimodal",
        "params": f"{total_params / 1e9:.2f}B",
        "dtype": "bfloat16",
        "device": str(model.device),
        "load_time": round(load_time, 2),
        "memory_gb": get_memory_usage(),
        "capabilities": ["æ–‡æœ¬å¯¹è¯", "å›¾åƒç†è§£", "éŸ³é¢‘è½¬å½•", "å¤šè½®å¯¹è¯"],
        "max_tokens": 8192,
    }

    model_loaded = True
    print("=" * 60)
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶ {load_time:.2f}s")
    print(f"å†…å­˜å ç”¨: {model_info['memory_gb']} GB")
    print("=" * 60)
    return True

def generate_response(messages_history, current_content, session_id=None, has_media=False, media_type=None):
    """
    ç”Ÿæˆå›å¤
    messages_history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "text": "..."}]
    current_content: å½“å‰æ¶ˆæ¯çš„contentåˆ—è¡¨
    session_id: ä¼šè¯ID (ç”¨äº thought signature)
    has_media: å½“å‰æ¶ˆæ¯æ˜¯å¦åŒ…å«åª’ä½“ (å›¾ç‰‡/éŸ³é¢‘)
    media_type: å½“å‰åª’ä½“ç±»å‹ ("image", "audio", None)

    æ³¨æ„: Gemma 3n å¤„ç†å™¨è¦æ±‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å›¾ç‰‡ï¼Œå¦åˆ™ä¼šæŠ¥æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´é”™è¯¯ã€‚
    è§£å†³æ–¹æ¡ˆ: å°†å†å²å¯¹è¯åˆå¹¶æˆä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œåªå‘é€ä¸€æ¡åŒ…å«å›¾ç‰‡çš„å½“å‰æ¶ˆæ¯ã€‚
    """
    global stats

    if not model_loaded:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}

    start_time = time.time()
    history_turns = len(messages_history) // 2  # è®°å½•å®é™…çš„å†å²è½®æ¬¡

    # è·å–å†å²åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†)
    # ç­–ç•¥ï¼š
    # - å¦‚æœå½“å‰æ¶ˆæ¯æœ‰æ–°åª’ä½“ï¼šä¸æ³¨å…¥å†å²åª’ä½“ç†è§£ï¼ˆé¿å…å¹²æ‰°ï¼‰
    # - å¦‚æœå½“å‰æ¶ˆæ¯æ²¡æœ‰æ–°åª’ä½“ï¼šæ³¨å…¥å†å²åª’ä½“ç†è§£ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå›ç­”å…³äºä¹‹å‰åª’ä½“çš„é—®é¢˜
    media_context = ""
    if session_id and not has_media:
        # åªåœ¨æ²¡æœ‰æ–°åª’ä½“æ—¶æ‰æ³¨å…¥å†å²åª’ä½“ç†è§£
        media_context = get_session_media_context(session_id)
        if media_context:
            print(f"[DEBUG] æ³¨å…¥åª’ä½“ç†è§£ä¸Šä¸‹æ–‡: {len(media_context)} å­—ç¬¦")
    elif has_media:
        print(f"[DEBUG] å½“å‰æœ‰æ–°{media_type or 'åª’ä½“'}ï¼Œè·³è¿‡å†å²åª’ä½“ç†è§£æ³¨å…¥")

    # æ–¹æ¡ˆ: å°†å†å²å¯¹è¯åˆå¹¶æˆä¸Šä¸‹æ–‡æ–‡æœ¬
    # è¿™æ ·å°±åªæœ‰ä¸€æ¡ user æ¶ˆæ¯ï¼Œé¿å…æ‰¹æ¬¡ä¸ä¸€è‡´é—®é¢˜
    # æ³¨æ„: å½“æœ‰æ–°åª’ä½“æ—¶ï¼Œä¸æ³¨å…¥å†å²å¯¹è¯ï¼Œé¿å…å¹²æ‰°æ¨¡å‹å¯¹å½“å‰åª’ä½“çš„ç†è§£
    history_context = ""
    if messages_history and not has_media:
        history_parts = []
        for msg in messages_history[-MAX_HISTORY_TURNS * 2:]:
            role = "User" if msg["role"] == "user" else "Assistant"
            history_parts.append(f"{role}: {msg['text']}")
        history_context = "\n".join(history_parts)
    elif has_media:
        print(f"[DEBUG] å½“å‰æœ‰æ–°åª’ä½“ï¼Œè·³è¿‡å†å²å¯¹è¯æ³¨å…¥")

    # æ„å»ºå•æ¡æ¶ˆæ¯ (åŒ…å«å†å²ä¸Šä¸‹æ–‡ + å½“å‰è¾“å…¥)
    messages = []

    # ä¿®æ”¹å½“å‰æ¶ˆæ¯å†…å®¹ï¼Œæ·»åŠ å†å²ä¸Šä¸‹æ–‡
    modified_content = []
    current_text = ""

    for item in current_content:
        if item.get("type") == "text":
            current_text = item.get("text", "")
        else:
            modified_content.append(item)

    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡æç¤º
    # åŒ…å«: åª’ä½“ç†è§£ + å†å²å¯¹è¯ + å½“å‰æ¶ˆæ¯
    full_context_parts = []

    # 1. åª’ä½“ç†è§£ä¸Šä¸‹æ–‡ (thought signature å‹ç¼©è®°å¿†) - åªåœ¨æ— æ–°åª’ä½“æ—¶æ³¨å…¥
    if media_context:
        full_context_parts.append(f"[Previous Media Understanding]\n{media_context}")

    # 2. å†å²å¯¹è¯ä¸Šä¸‹æ–‡
    if history_context:
        full_context_parts.append(f"[Previous Conversation]\n{history_context}")

    # 3. å½“å‰æ¶ˆæ¯
    if full_context_parts:
        # æœ‰ä¸Šä¸‹æ–‡ï¼Œæ„å»ºå®Œæ•´æç¤º
        context_str = "\n\n".join(full_context_parts)
        context_prompt = f"""{context_str}

[Current Message]: {current_text}

Please respond to the current message, taking into account the context above."""
        modified_content.append({"type": "text", "text": context_prompt})
    else:
        # æ— ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½¿ç”¨å½“å‰æ–‡æœ¬
        modified_content.append({"type": "text", "text": current_text})

    messages.append({"role": "user", "content": modified_content})

    print(f"[DEBUG] æ¶ˆæ¯æ•°é‡: {len(messages)}, å†å²è½®æ¬¡: {history_turns}, æœ‰æ–°åª’ä½“: {has_media}")

    # å¤„ç†è¾“å…¥
    tokenize_start = time.time()
    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    )
    tokenize_time = time.time() - tokenize_start

    # å‡†å¤‡ç”Ÿæˆå‚æ•°
    input_ids = inputs["input_ids"].to(model.device)
    attention_mask = inputs["attention_mask"].to(model.device)
    input_tokens = input_ids.shape[1]

    print(f"[DEBUG] input_tokens: {input_tokens}")

    generate_kwargs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "max_new_tokens": 512,
        "do_sample": False,
    }

    # å¤„ç†å›¾åƒ
    if "pixel_values" in inputs and inputs["pixel_values"] is not None:
        generate_kwargs["pixel_values"] = inputs["pixel_values"].to(model.device, dtype=model.dtype)

    # å¤„ç†éŸ³é¢‘
    if "input_features" in inputs and inputs["input_features"] is not None:
        generate_kwargs["input_features"] = inputs["input_features"].to(model.device, dtype=model.dtype)
        generate_kwargs["input_features_mask"] = inputs["input_features_mask"].to(model.device)

    # ç”Ÿæˆ
    generate_start = time.time()
    with torch.inference_mode():
        outputs = model.generate(**generate_kwargs)
    generate_time = time.time() - generate_start

    # è§£ç 
    output_tokens = len(outputs[0]) - input_tokens
    response = processor.tokenizer.decode(
        outputs[0][input_ids.shape[1]:],
        skip_special_tokens=True
    )

    total_time = time.time() - start_time
    speed = output_tokens / generate_time if generate_time > 0 else 0

    # æ›´æ–°ç»Ÿè®¡
    stats["total_requests"] += 1
    stats["total_tokens"] += output_tokens
    stats["total_time"] += total_time
    stats["avg_speed"] = stats["total_tokens"] / stats["total_time"] if stats["total_time"] > 0 else 0

    return {
        "response": response,
        "metrics": {
            "total_time": round(total_time, 2),
            "tokenize_time": round(tokenize_time, 3),
            "generate_time": round(generate_time, 2),
            "input_tokens": int(input_tokens),
            "output_tokens": int(output_tokens),
            "speed": round(speed, 1),
            "history_turns": history_turns  # ä½¿ç”¨å®é™…çš„å†å²è½®æ¬¡
        }
    }

@app.route("/")
def index():
    return send_from_directory("static", "index.html")

@app.route("/api/status")
def status():
    hw_stats = get_hardware_stats() if model_loaded else {}
    return jsonify({
        "loaded": model_loaded,
        "stats": stats,
        "memory_gb": get_memory_usage() if model_loaded else 0,
        "hardware": hw_stats,
        "active_sessions": len(sessions)
    })

@app.route("/api/session/new", methods=["POST"])
def new_session():
    """åˆ›å»ºæ–°ä¼šè¯"""
    session_id = str(uuid.uuid4())[:8]
    session_data = {
        "messages": [],
        "created_at": time.time(),
        "title": "æ–°å¯¹è¯"
    }
    sessions[session_id] = session_data
    # ç«‹å³ä¿å­˜åˆ°ç£ç›˜
    save_session_to_disk(session_id, session_data)
    cleanup_old_sessions()
    return jsonify({"session_id": session_id})

@app.route("/api/session/list", methods=["GET"])
def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
    session_list = list_all_sessions()
    return jsonify({"sessions": session_list})

@app.route("/api/session/<session_id>/load", methods=["GET"])
def load_session(session_id):
    """åŠ è½½æŒ‡å®šä¼šè¯"""
    # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
    if session_id in sessions:
        session = sessions[session_id]
        return jsonify({
            "session_id": session_id,
            "messages": session["messages"],
            "title": session.get("title", "æ–°å¯¹è¯")
        })

    # ä»ç£ç›˜æŸ¥æ‰¾
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        if session_id in jsonl_file.name:
            session_data = load_session_from_disk(jsonl_file)
            if session_data:
                sessions[session_id] = session_data
                return jsonify({
                    "session_id": session_id,
                    "messages": session_data["messages"],
                    "title": session_data.get("title", "æ–°å¯¹è¯")
                })

    return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

@app.route("/api/session/<session_id>/delete", methods=["POST"])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    # ä»å†…å­˜ç§»é™¤
    if session_id in sessions:
        file_path = sessions[session_id].get("file_path")
        del sessions[session_id]
        # åˆ é™¤ç£ç›˜æ–‡ä»¶
        if file_path and Path(file_path).exists():
            Path(file_path).unlink()
            return jsonify({"success": True})

    # ä»ç£ç›˜æŸ¥æ‰¾å¹¶åˆ é™¤
    for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
        if session_id in jsonl_file.name:
            jsonl_file.unlink()
            return jsonify({"success": True})

    return jsonify({"success": True})

@app.route("/api/session/<session_id>/clear", methods=["POST"])
def clear_session(session_id):
    """æ¸…ç©ºä¼šè¯å†å²"""
    if session_id in sessions:
        sessions[session_id]["messages"] = []
        sessions[session_id]["title"] = "æ–°å¯¹è¯"
        # ä¿å­˜åˆ°ç£ç›˜
        save_session_to_disk(session_id, sessions[session_id])
    return jsonify({"success": True})

@app.route("/api/session/<session_id>/history", methods=["GET"])
def get_history(session_id):
    """è·å–ä¼šè¯å†å²"""
    if session_id not in sessions:
        return jsonify({"messages": []})
    return jsonify({"messages": sessions[session_id]["messages"]})

@app.route("/api/thought/state/<session_id>", methods=["GET"])
def get_thought_state(session_id):
    """è·å–ä¼šè¯çš„ thought signature çŠ¶æ€"""
    state = thought_states.get(session_id, {"turn_index": 0, "media_refs": []})
    media_understandings = []

    for media_ref in state.get("media_refs", []):
        cached = media_understanding_cache.get(media_ref)
        if cached:
            media_understandings.append({
                "media_ref": media_ref,
                "turn_index": cached.get("turn_index", 0),
                "understanding_preview": cached.get("understanding", "")[:100] + "...",
                "created_at": cached.get("created_at", 0)
            })

    return jsonify({
        "session_id": session_id,
        "turn_index": state.get("turn_index", 0),
        "media_count": len(state.get("media_refs", [])),
        "media_understandings": media_understandings
    })

@app.route("/api/thought/stats", methods=["GET"])
def thought_stats():
    """è·å– thought signature å…¨å±€ç»Ÿè®¡"""
    return jsonify({
        "total_sessions": len(thought_states),
        "total_media_cached": len(media_understanding_cache),
        "sessions": [
            {
                "session_id": sid,
                "turn_index": state.get("turn_index", 0),
                "media_count": len(state.get("media_refs", []))
            }
            for sid, state in thought_states.items()
        ]
    })

@app.route("/api/chat", methods=["POST"])
def chat():
    try:
        data = request.json
        text = data.get("text", "")
        image_data = data.get("image")
        audio_data = data.get("audio")
        session_id = data.get("session_id")

        # è·å–æˆ–åˆ›å»ºä¼šè¯
        if not session_id or session_id not in sessions:
            # å°è¯•ä»ç£ç›˜åŠ è½½
            loaded = False
            if session_id:
                for jsonl_file in SESSIONS_DIR.rglob("*.jsonl"):
                    if session_id in jsonl_file.name:
                        session_data = load_session_from_disk(jsonl_file)
                        if session_data:
                            sessions[session_id] = session_data
                            loaded = True
                            break
            if not loaded:
                session_id = str(uuid.uuid4())[:8]
                sessions[session_id] = {"messages": [], "created_at": time.time(), "title": "æ–°å¯¹è¯"}

        session = sessions[session_id]
        image = None
        audio = None

        # å¤„ç†å›¾ç‰‡
        if image_data:
            if "," in image_data:
                image_data = image_data.split(",")[1]
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        # å¤„ç†éŸ³é¢‘
        if audio_data:
            mime_part = ""
            if "," in audio_data:
                mime_part = audio_data.split(",")[0]
                audio_data = audio_data.split(",")[1]
            audio_bytes = base64.b64decode(audio_data)

            if "wav" in mime_part:
                ext = ".wav"
            elif "webm" in mime_part:
                ext = ".webm"
            elif "ogg" in mime_part:
                ext = ".ogg"
            elif "mp3" in mime_part or "mpeg" in mime_part:
                ext = ".mp3"
            elif "flac" in mime_part:
                ext = ".flac"
            else:
                ext = ".wav"

            temp_path = f"/tmp/audio_{session_id}{ext}"
            with open(temp_path, "wb") as f:
                f.write(audio_bytes)

            audio_array, sr = librosa.load(temp_path, sr=16000)
            audio = (audio_array, sr)
            print(f"[DEBUG] éŸ³é¢‘: {len(audio_array)/sr:.2f}ç§’")

        # æ„å»ºå½“å‰æ¶ˆæ¯å†…å®¹
        content = []
        has_media = (image is not None or audio is not None)

        if not has_media:
            # çº¯æ–‡æœ¬æ¶ˆæ¯ï¼šæ·»åŠ  dummy_image
            content.append({"type": "image", "image": dummy_image})
            display_text = text
            text = "Ignore the blank image. " + text
        else:
            display_text = text

        # Gemma 3n è¦æ±‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å›¾ç‰‡
        # å¦‚æœåªæœ‰éŸ³é¢‘æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿéœ€è¦æ·»åŠ  dummy_image
        if image is not None:
            content.append({"type": "image", "image": image})
        elif audio is not None:
            # åªæœ‰éŸ³é¢‘ï¼Œæ·»åŠ  dummy_image
            content.append({"type": "image", "image": dummy_image})
            text = "Ignore the blank image. " + text

        if audio is not None:
            content.append({"type": "audio", "audio": audio[0], "sample_rate": audio[1]})

        content.append({"type": "text", "text": text})

        # è®¡ç®—å½“å‰è½®æ¬¡
        turn_index = len(session["messages"]) // 2 + 1

        # ç”Ÿæˆå›å¤ (ä¼ å…¥ session_id å’Œ has_media)
        # ç¡®å®šå½“å‰åª’ä½“ç±»å‹
        current_media_type = None
        if image is not None:
            current_media_type = "image"
        elif audio is not None:
            current_media_type = "audio"

        # ç”Ÿæˆå›å¤ (ä¼ å…¥ session_id, has_media, media_type)
        result = generate_response(
            session["messages"],
            content,
            session_id=session_id,
            has_media=has_media,
            media_type=current_media_type
        )

        if "error" not in result:
            # å¦‚æœæœ‰åª’ä½“è¾“å…¥ï¼Œä»æ¨¡å‹å›å¤ä¸­æå–ç†è§£å¹¶å­˜å‚¨åˆ° thought signature
            if has_media:
                # æ¨¡å‹çš„å›å¤å°±æ˜¯å¯¹åª’ä½“çš„ç†è§£
                # å°†å…¶å­˜å‚¨ä¸º "å‹ç¼©è®°å¿†"
                understanding = result["response"]

                # ç”Ÿæˆåª’ä½“ç†è§£ç­¾å
                media_ref = generate_media_signature(
                    session_id=session_id,
                    turn_index=turn_index,
                    understanding=understanding[:500]  # é™åˆ¶é•¿åº¦ï¼Œåªä¿å­˜æ‘˜è¦
                )
                print(f"[Thought Signature] å­˜å‚¨ {current_media_type} ç†è§£: {media_ref}")

            # ä¿å­˜åˆ°å†å²ï¼ˆåªä¿å­˜æ–‡æœ¬æ‘˜è¦ï¼‰
            user_summary = display_text
            if image is not None:
                user_summary = "[å›¾ç‰‡] " + user_summary
            if audio is not None:
                user_summary = "[éŸ³é¢‘] " + user_summary

            session["messages"].append({
                "role": "user",
                "text": user_summary,
                "has_image": image is not None,
                "has_audio": audio is not None,
                "timestamp": time.time()
            })
            session["messages"].append({
                "role": "assistant",
                "text": result["response"],
                "timestamp": time.time()
            })

            # é™åˆ¶å†å²é•¿åº¦
            if len(session["messages"]) > MAX_HISTORY_TURNS * 2:
                session["messages"] = session["messages"][-MAX_HISTORY_TURNS * 2:]

            # æ›´æ–°æ ‡é¢˜ (ç”¨ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯)
            if session.get("title") == "æ–°å¯¹è¯" and len(session["messages"]) >= 1:
                first_user_msg = next((m for m in session["messages"] if m["role"] == "user"), None)
                if first_user_msg:
                    session["title"] = first_user_msg["text"][:30] + ("..." if len(first_user_msg["text"]) > 30 else "")

            # ä¿å­˜åˆ°ç£ç›˜
            save_session_to_disk(session_id, session)
            # è¿½åŠ åˆ°å…¨å±€å†å²
            append_to_history(session_id, display_text)

        result["session_id"] = session_id
        return jsonify(result)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    init_storage()

    # macOS: è¯·æ±‚ sudo æƒé™ç”¨äºç¡¬ä»¶ç›‘æ§
    if platform.system() == "Darwin":
        request_sudo_permission()

    load_model()
    print("\n" + "=" * 60)
    print("AI å¤šæ¨¡æ€èŠå¤©æœåŠ¡å™¨å¯åŠ¨: http://localhost:5000")
    print(f"ä¼šè¯å­˜å‚¨: {GEMMA3N_HOME}")
    print("æ”¯æŒå¤šè½®å¯¹è¯å†å²è®°å¿†")
    if sudo_authorized:
        print("GPU æ¸©åº¦ç›‘æ§: âœ… å·²å¯ç”¨")
    else:
        print("GPU æ¸©åº¦ç›‘æ§: âŒ æœªå¯ç”¨ (å¯é‡å¯æœåŠ¡å¹¶æˆæƒ)")
    print("=" * 60)
    app.run(host="0.0.0.0", port=5000, debug=False, threaded=True)
